{"meta":{"title":"Severin的博客","subtitle":"","description":"","author":"Severin","url":"https://www.severin.xyz","root":"/"},"pages":[{"title":"404 Not Found","date":"2020-01-28T06:37:46.274Z","updated":"2020-01-28T06:37:46.274Z","comments":true,"path":"404.html","permalink":"https://www.severin.xyz/404.html","excerpt":"","text":"404 Not Found 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"所有分类","date":"2020-01-28T03:15:02.282Z","updated":"2020-01-28T03:15:02.282Z","comments":true,"path":"categories/index.html","permalink":"https://www.severin.xyz/categories/index.html","excerpt":"","text":""},{"title":"关于","date":"2020-01-30T12:54:44.506Z","updated":"2020-01-30T12:54:44.506Z","comments":true,"path":"about/index.html","permalink":"https://www.severin.xyz/about/index.html","excerpt":"","text":"有什么想对我说的，可以在评论里告诉我(￣▽￣)~*"},{"title":"我的朋友们","date":"2020-01-28T03:26:54.831Z","updated":"2020-01-28T03:26:54.831Z","comments":true,"path":"friends/index.html","permalink":"https://www.severin.xyz/friends/index.html","excerpt":"","text":"你可以在评论中留下你的博客名、头像链接和博客链接"},{"title":"","date":"2020-01-28T03:16:40.195Z","updated":"2020-01-28T03:16:40.195Z","comments":true,"path":"mylist/index.html","permalink":"https://www.severin.xyz/mylist/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2020-01-28T03:16:27.915Z","updated":"2020-01-28T03:16:27.915Z","comments":true,"path":"tags/index.html","permalink":"https://www.severin.xyz/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"操作系统总结之内存篇","slug":"操作系统总结之内存篇","date":"2020-02-27T12:43:10.006Z","updated":"2020-02-27T12:54:44.194Z","comments":true,"path":"2020/02/27/操作系统总结之内存篇/","link":"","permalink":"https://www.severin.xyz/2020/02/27/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%80%BB%E7%BB%93%E4%B9%8B%E5%86%85%E5%AD%98%E7%AF%87/","excerpt":"","text":"1. 内存管理概念内存是一个稀缺资源，很难将所有需要执行的进程都放入主存中，所以操作系统必须将内存空间进行合理地划分和有效地动态分派。操作系统对内存的划分和动态分配就是内存管理的概念。 内存管理的功能有： 内存空间的分配与回收 由操作系统完成主存储器空间的分配和管理，使程序员摆脱存储分配的麻烦，提高编程效率。 地址转换 在多道程序环境下， 程序中的逻辑地址与内存中的物理地址不可能一致，因此存储管理必须提供地址变换功能，把逻辑地址转换成相应的物理地址。 内存空间的扩充 利用虚拟存储技术或自动覆盖技术，从逻辑上扩充内存。 存储保护 保证各道作业在各自的存储空间内运行，互不干扰 程序装入和链接 创建进程首先要将程序和数据装入内存。将用户源程序变为可在内存中执行的程序，通常需要以下几个步骤： 编译：由编译程序将用户源代码编译成若干个目标模块。 链接： 由链接程序将编译后形成的一组目标模块，以及所需库函数链接在一起，形成一个完整的装入模块。 装入： 由装入程序将装入模块装入内存运行。 链接方式： 静态链接 在程序运行之前，先将各目标模块及它们所需的库函数链接成一个完整的可执行程序，以后不再拆开 转入时动态链接 将用户源程序编译后所得到的一组目标模块，在装入内存时，采用边装入边链接的链接方式。 运行时动态链接 对某些目标模块的链接，是在程序执行中需要该目标模块时，才对它进行的链接。其优点是便于修改和更新，便于实现对目标模块的共享 装入方式： 绝对装入 在编译时，如果知道程序将驻留在内存的某个位置，编译程序将产生绝对地址的目标代码。绝对装入程序按照装入模块中的地址，将程序和数据装入内存． 由于程序中的逻辑地址与实际内存地址完全相同，故不需对程序和数据的地址进行修改。 可重定位装入 在多道程序环境下，多个目标模块的起始地址通常都是从0 开始， 程序中的其他地址都是相对于起始地址的，此时应采用可重定位装入方式。根据内存的当前情况，将装入模块装入到内存的适当位置。 动态运行时装入 装入程序在把装入模块装入内存后， 并不立即把装入模块中的相对地址转换为绝对地址，而是把这种地址转换推迟到程序真正要执行时才进行 覆盖和交换 覆盖：由于程序运行时并非任何时候都要访问程序及数据的各个部分（尤其是大程序〉，因此可以把用户壁间分成一个固定区和若干个覆盖区。将经常活跃的部分放在固定区，其余部分按调用关系分段。首先将那些即将要访问的段放入覆盖区，其他段放在外存中，在需要调用前，系统再将其调入覆盖区，替换覆盖区中原有的段。 交换（对换）的基本思想是，把处于等待状态（或在CPU 调度原则下被剥夺运行权利）的程序从内存移到辅存，把内存空间腾出来，这一过程又叫换出：把准备好竞争CPU 运行的程序从辅存移到内存，这一过程又称为换入。第2 章介绍的中级调度就是采用交换技术。 2. 连续分配管理方式 单一连续分配 交换（对换）的基本思想是，把处于等待状态（或在CPU 调度原则下被剥夺运行权利）的程序从内存移到辅存，把内存空间腾出来，这一过程又叫换出：把准备好竞争CPU 运行的程序从辅存移到内存，这一过程又称为换入。第2 章介绍的中级调度就是采用交换技术。 这种方式的优点是简单、无外部碎片，可以采用覆盖技术，不需要额外的技术支持。缺点是只能用于单用户、单任务的操作系统中，有内部碎片，存储器的利用率极低。 固定分区分配 固定分区分配是最简单的一种多道程序存储管理方式，它将用户内存空间划分为若干个固定大小的区域，每个分区只装入一道作业。当有空闲分区时，便可以再从外存的后备作业队列中，选择适当大小的作业装入该分区，如此循环。 动态分区分配","categories":[],"tags":[]},{"title":"操作系统总结之进程篇","slug":"操作系统总结之进程篇","date":"2020-02-26T12:45:24.760Z","updated":"2020-03-03T11:37:22.344Z","comments":true,"path":"2020/02/26/操作系统总结之进程篇/","link":"","permalink":"https://www.severin.xyz/2020/02/26/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%80%BB%E7%BB%93%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AF%87/","excerpt":"","text":"1. 进程与线程的基本概念什么是进程 进程是具有独立功能的程序在一个数据集合上运行的过程，它是系统进行资源分配和调度的一个独立单位。 进程的运行状态 新建 进程正在被创建，尚未转到就绪状态 就绪 进程己处于准备运行的状态， 即进程获得了除处理机之外的一切所需资源，一旦得到处理机即可运行 运行 进程正在处理机上运行 阻塞 进程正在等待某一事件而暂停运行 终止 进程正从系统中消失，这可能是进程正常结束或其他原因中断退出运行 进程控制 创建进程 申请一个唯一的进程标识符，然后分配一个空白的PCB，如果PCB分配失败则进程创建失败 为进程分配资源，为进程的程序和数据以及用户栈分配内存空间 初始化PCB，主要包括初始化PCB的标志信息、进程状态和进程优先级等 如果就绪队列可以容纳新的进程，将当前进程放到就绪队列中等待调度 进程终止 根据被终止进程的标识符，检索PCB ，从中读出该进程的状态。 若被终止进程处于执行状态，立即终止该进程的执行，将处理机资源分配给其他进程。 若该进程还有子进程，则应将其所有子进程终止。 将该进程所拥有的全部资源，或归还给其父进程或归还给操作系统。 将该PCB 从所在队列（链表） 中删除。 进程阻塞 找到将要被阻塞进程的标识号对应的PCB 若该进程为运行状态，则保护其现场，将其状态转为阻塞状态，停止运行。 把该PCB 插入到相应事件的等待队列中去 进程唤醒 在该事件的等待队列中找到相应进程的PCB 将其从等待队列中移出，并置其状态为就绪状态 把该PCB 插入就绪队列中，等待调度程序调度 进程切换 保存处理机上下文，包括程序计数器和其他寄存器 更新PCB 信息 把进程的PCB 移入相应的队列，如就绪、在某事件阻塞等队列 选择另一个进程执行， 并更新其PCB 更新内存管理的数据结构 恢复处理机上下文 进程的组成 PCB 是进程实体的一部分，是进程存在的唯一标准。包括进程描述信息，进程控制和管理信息、资源分配清单和处理机相关信息 程序段 程序段就是能被进程调度程序调度到CPU 执行的程序代码段 数据段 一个进程的数据段，可以是进程对应的程序加工处理的原始数据，也可以是程序执行时产生时的中间或最终结果 进程通信 管道 管道是一种半双工的通信方式，可以看做是一种特殊的文件，但是这个文件只存在于内存中。管道只能用于举有亲缘关系的进程之间的通信，如父子进程/兄弟进程等。 FIFO FIFO也是以一种特殊的文件存放在内存中，但是FIFO文件有一个路径名，所以可以在无关的进程之间交换数据。 消息队列 消息队列，是消息的链接表，存放在内核中。一个消息队列由一个标识符（即队列ID）来标识。 消息队列是面向记录的，其中的消息具有特定的格式以及特定的优先级。 消息队列独立于发送与接收进程。进程终止时，消息队列及其内容并不会被删除。 息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取。 信号量 信号量用于进程间同步，若要在进程间传递数据需要结合共享内存。 共享内存 共享内存（Shared Memory），指两个或多个进程共享一个给定的存储区，信号量+共享内存通常结合在一起使用，信号量用来同步对共享内存的访问。 Socket 通过网络或者是本地socket 线程概念 线程是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位 线程的实现方式 用户级线程 有关线程管理的所有工作都由应用程序完成，内核意识不到线程的存在。 内核级线程 线程管理的所有工作由内核完成，应用程序没有进行线程管理的代码，只有一个到内核级线程的编程接口 多线程模型 多对一 将多个用户级线程映射到一个内核级线程，线程管理在用户空间完成。此模式中，用户级线程对操作系统不可见。 优点：线程管理是在用户空间进行的，因而效率比较高。缺点： 当一个线程在使用内核服务时被阻塞，那么整个进程都会被阻塞：多个线程不能并行地运行在多处理机上。 一对一 将每个用户级线程映射到一个内核级线程优点：当一个线程被阻塞后，允许另一个线程继续执行，所以并发能力较强。缺点：每创建一个用户级线程都需要创建一个内核级线程与其对应，这样创建线程的开销比较大，会影响到应用程序的性能。 一对多 将n 个用户级线程映射到m 个内核级线程上 在多对一模型和一对一模型中取了个折中，克服了多对一模型的并发度不高的缺点，又克服了一对一模型的一个用户进程占用太多内核级线程，开销太大的缺点。又拥有多对一模型和一对一模型各自的优点，可谓集两者之所长。 2. 进程调度什么是调度 处理机的数量小于进程的数量，就存在进程争用处理机的情况， 处理机调度是对处理机进行分配，就是从就绪队列中，按照一定的算法（公平、高效）选择一个进程并将处理机分配给它运行， 以实现进程并发地执行。 调度层次 作业调度 按一定的原则从外存上处于后备状态的作业中挑选作业，并分配内存、输入／输出设备等必要的资源，并建立相应的进程，以使其 获得竞争处理机的权利。 中级调度 引入中级调度是为了提高内存利用率和系统吞吐量。为此，应使那些暂时不能运行的进程，调至外存等待，把此时的进程状态称为挂起状态。当它们己具备运行条件且内存又稍有空闲时，由中级调度来决定，把外存上的那些己具备运行条件的就绪进程，再重新调入内存，并修改其状态为就绪状态，挂在就绪队列上等待。 进程调度 按照某种方法和策略从就绪队列中选取一个进程，将处理机分配给它 不能进行进程调度的情况 处理中断过程 进程在操作系统内核临界区中 原子操作 调度的基本准则 CPU利用率 系统吞吐量 表示单位时间内CPU 完成作业的数量 周转时间 是指从作业提交到作业完成所经历的时间，包括作业等待、在就绪队列中排队、在处理机上运行以及进行输入／输出操作所花费时间的总和 等待时间 是指进程处于等处理机状态时间之和 响应时间 用户提交请求到系统首次产生响应所用的时间 调度算法 先来先服务调度算法 每次从就绪队列中选择最先进入该队列的进程，将处理机分配给它，使之投入运行，直到完成或因某种原因而阻塞时才释放处理机。 最短进程优先算法 从就绪队列中选择一个估计运行时间最短的进程，将处理机分配给它，使之立即执行，直到完成或发生某事件而阻塞时，才释放处理机。 优先级调度算法 优先级调度算法每次从就绪队列中选择优先级最高的进程，将处理机分配给它，使之投入运行 高响应比优先调度算法 高响应比优先调度算法主要用于作业调度，该算法是对FCFS 调度算法和SJF 调度算法的一种综合平衡，同时考虑每个作业的等待时间和估计的运行时间。在每次进行作业调度时，先计算后备作业队列中每个作业的响应比，从中选出响应比最高的作业投入运行。 时间片轮转调度算法 时间片轮转调度算法主要适用于分时系统。在这种算法中，系统将所有就绪进程按到达时间的先后次序排成一个队列，进程调度程序总是选择就绪队列中第一个进程执行，即先来先服务的原则，但仅能运行一个时间片，如100ms 。在使用完一个时间片后，即使进程并未完成其运行，它也必须释放出（被剥夺〉处理机给下一个就绪的进程，而被剥夺的进程返回到就绪队列的末尾重新排队，等候再次运行。 多级反馈队列调度算法 设置多个就绪队列，每一级就绪队列的优先级不一样，比如第一级队列的优先级最高，第二级次之，然后依次类推，每一级队列的时间片也不一样，优先级越高队列时间片越短。刚进入系统的进程放到第一级队列的末尾，运行该进程后如果没有在指定的时间片完成就放入下一级队列，同样在下一级队列中也是如此操作，但是每次只要上一级队列中有进程就会先调度上面的队列。 3. 进程同步进程同步的基本概念 临界资源 多个进程可以共享系统中的各种资源，但其中许多资源一次只能为一个进程所使用，我们把一次仅允许一个进程使用的资源称为临界资源。访问临界资源的代码称为临界区。 同步 同步亦称直接制约关系，它是指为完成某种任务而建立的两个或多个进程，这些进程因为需要在某些位置上协调它们的工作次序而等待、传递信息所产生的制约关系。 互斥 互斥亦称间接制约关系。当一个进程进入临界区使用临界资源时， 另一个进程必须等待，当占用临界资源的进程退出临界区后，另一进程才允许去访问此临界资源。 实现临界区的方法 软件实现 Peterson算法 硬件实现 中断屏蔽方法 当一个进程正在使用处理机执行它的临界区代码时，要防止其他进程再进入其临界区访问的最简单方法是禁止一切中断发生，或称之为屏蔽中断、关中断。因为CPU 只在发生中断时引起进程切换，这样屏蔽中断就能保证当前运行进程将临界区代码顺利地执行完，从而保证了互斥的正确实现，然后再执行开中断。 硬件指令方法 TestAndSet指令 swap指令 信号量 信号量是个整数变量，除了初始化外，它只能通过两个标准原子操作：wait()和signal()来访问。这些操作原来被称为P和V操作。wait会判断信号量是否&lt;=0，如果是的话就会阻塞，否则会将信号量减1，而single操作可以将信号量加1。信号量有两种分别是二进制信号量和计数信号量，二进制信号量的值只能取0和1，计数信号量的值可以是任何整数。 管程 经典的同步问题 生产者-消费者问题 读者-写者问题 哲学家进程问题 吸烟者问题 4. 死锁死锁发生的条件 互斥条件 进程要求对所分配的资源（如打印机〉进行排他性控制，即在一段时间内某资源仅为一个进程所占有。此时若有其他进程请求该资源，则请求进程只能等待。 不可剥夺条件 进程所获得的资源在未使用完毕之前，不能被其他进程强行夺走，即只能由获得该资源的进程自己来释放（只能是主动释放）。 请求和保持条件 进程己经保持了至少一个资源，但又提出了新的资源请求，而该资源己被其他进程占有， 此时请求进程被阻塞，但对自己己获得的资源保持不放。 循环等待条件 存在一种进程资源的循环等待链，链中每一个进程己获得的资源同时被链中下一个进程所请求 死锁的处理 死锁预防 设置某些限制条件，破坏产生死锁的四个必要条件中的一个或几个，以防止发生死锁。 死锁避免 在资源的动态分配过程中，用某种方法防止系统进入不安全状态，从而避免死锁。如银行家算法。 死锁检测和解除 无需采取任何限制性措施，允许进程在运行过程中发生死锁。通过系统的检测机构及时地检测出死锁的发生，然后采取某种措施解除死锁。 如何解除死锁 资源剥夺法 挂起某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程。但应防止被挂起的进程长时间得不到资源，而处于资源匮乏的状态 撤销进程法 强制撤销部分、甚至全部死锁进程并剥夺这些进程的资源。撤销的原则可以按进程优先级和撤销进程代价的高低进行。 5. 常见面试题","categories":[],"tags":[]},{"title":"Spring探究之mvc","slug":"Spring探究之mvc","date":"2020-02-26T01:58:01.209Z","updated":"2020-02-26T06:14:48.140Z","comments":true,"path":"2020/02/26/Spring探究之mvc/","link":"","permalink":"https://www.severin.xyz/2020/02/26/Spring%E6%8E%A2%E7%A9%B6%E4%B9%8Bmvc/","excerpt":"","text":"1. MVC的理解2. Spring MVC的实现以下面的代码启动一个web项目： @SpringBootApplication public class SpringBootMvcApplication { public static void main(String[] args) { new SpringApplicationBuilder(SpringBootMvcApplication.class) .web(WebApplicationType.SERVLET).run(); } } @RestController public class HelloController { @GetMapping(&quot;/hello&quot;) public String hello(){ return &quot;hello&quot;; } } Spring MVC的核心类叫做DispatcherServlet。首先来分析一下DispatcherServlet的初始化过程。 Spring MVC初始化过程 DispatcherServlet是Spring MVC的核心类。其继承关系如下： 通过上图可以看出DispatcherServlet是Servlet的实现类。我们从最顶层的实现类GenericServlet开始分析，根据我们对Servlet生命周期的了解，可以知道初始化会触发GenericServlet的init方法，我们以此为起点跟踪初始化过程。 // 类: GenericServlet public void init(ServletConfig config) throws ServletException { this.config = config; this.init(); } 上面代码调用子类的init方法 // 类：HttpServletBean public final void init() throws ServletException { // 前面代码省略 // 让子类去初始化，继续 initServletBean(); } // 类：FrameworkServlet protected final void initServletBean() throws ServletException { try { this.webApplicationContext = initWebApplicationContext();// 继续 initFrameworkServlet(); } // 后面代码省略 } protected WebApplicationContext initWebApplicationContext() { // 前面代码省略 if (!this.refreshEventReceived) { // Either the context is not a ConfigurableApplicationContext with refresh // support or the context injected at construction time had already been // refreshed -&gt; trigger initial onRefresh manually here. synchronized (this.onRefreshMonitor) { onRefresh(wac);// 进入下一步分析 } } return wac; } 终于到了 // DispatcherServlet protected void onRefresh(ApplicationContext context) { initStrategies(context); } protected void initStrategies(ApplicationContext context) { // 下面这九行代码的调用对应着Spring MVC九大组件的初始化 initMultipartResolver(context);// MultipartResolver用于处理上传请求 initLocaleResolver(context);// LocaleResolver用于实现国际化 initThemeResolver(context);// ThemeResolver用于解析主题 initHandlerMappings(context);// 初始化HandlerMapping initHandlerAdapters(context);// 初始化HandlerAdapter initHandlerExceptionResolvers(context);// Handler异常解析器 initRequestToViewNameTranslator(context);// ViewName是根据ViewName查找View initViewResolvers(context);// 视图解析器 initFlashMapManager(context);// FlashMapManager } 上面的代码时对Spring MVC的九大组件进行了初始化，后面处理Http请求的时候就是靠DispatcherServlet这些组件完成的。可以看到这些组件的初始化都需要一个ApplicationContext容器，这个容器代表着web容器。 Spring MVC处理请求 开门见山，先看一张总所周知的图。 接下来用源码验证一遍,``DispatcherServlet时一个Servlet，所以请求肯定会经过其父类的service`方法到达该类的某个方法。 protected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception { logRequest(request); // 前面代码省略 try { // 这一行把请求和响应交给doDispatch方法处理 doDispatch(request, response); } // 后面省略 } protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try { ModelAndView mv = null; Exception dispatchException = null; try { processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // 获取处理当前请求的handler mappedHandler = getHandler(processedRequest); if (mappedHandler == null) { noHandlerFound(processedRequest, response); return; } // 获取handler的适配器 HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // 这里是执行缓存策略 String method = request.getMethod(); boolean isGet = &quot;GET&quot;.equals(method); if (isGet || &quot;HEAD&quot;.equals(method)) { long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (new ServletWebRequest (request, response).checkNotModified(lastModified) &amp;&amp; isGet) { return; } } // 拦截器：在handler方法执行前拦截 if (!mappedHandler.applyPreHandle(processedRequest, response)) { return; } // 调用handlerAdapter方法，得到ModelAndView，这一步就执行了我们实现的handler方法 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) { return; } applyDefaultViewName(processedRequest, mv); // 拦截器：在handler方法执行后拦截 mappedHandler.applyPostHandle(processedRequest, response, mv); } catch (Exception ex) { dispatchException = ex; } catch (Throwable err) { dispatchException = new NestedServletException(&quot;Handler dispatch failed&quot;, err); } // 处理视图和异常 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); } catch (Exception ex) { triggerAfterCompletion(processedRequest, response, mappedHandler, ex); } catch (Throwable err) { triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(&quot;Handler processing failed&quot;, err)); } finally { if (asyncManager.isConcurrentHandlingStarted()) { // Instead of postHandle and afterCompletion if (mappedHandler != null) { mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); } } else { // Clean up any resources used by a multipart request. if (multipartRequestParsed) { cleanupMultipart(processedRequest); } } } } 这个方法基本证明了图中描述的处理过程是对的。 3， Spring MVC容器分析4. 其他问题filter与intercepter的区别 filter是servlet中的一个接口，用来过滤http请求中的某些内容，在执行handler方法前执行，并且可以针对特定的servlet进行过滤。而intercepter是Spring框架提供的概念，在handler执行前后都可以调用相应的方法，不能配置对某个url进行拦截。","categories":[],"tags":[]},{"title":"spring探究之IoC","slug":"spring探究之IoC","date":"2020-02-26T01:46:18.074Z","updated":"2020-02-26T01:57:47.305Z","comments":true,"path":"2020/02/26/spring探究之IoC/","link":"","permalink":"https://www.severin.xyz/2020/02/26/spring%E6%8E%A2%E7%A9%B6%E4%B9%8BIoC/","excerpt":"","text":"1. 什么是IoC首先解释一下什么是IoC，IoC的意思是控制反转，控制反转是一种思想，指的是将类管理自身成员变量的权利交给第三方容器，也就是说在没有使用IoC容器的时候，一个对象所依赖的成员变量是需要自己管理、实例化的，但是如果一个项目涉及的类太多并且对象与对象之间的关系非常复杂，那么对象的依赖关系很难维护，为了解决这个问题就有了IoC，IoC是一个容器，里面存放了各种各样的对象实例，程序员只要通过配置信息描述对象与对象之间的关系，然后交给ioc容器，容器会自动帮我们实例化对象并且配置好类与类之间的关系，这样在我们写代码的时候如果需要用到某个类的实例，就只需要通过容器拿到这个实例对象就可以了，而不需要我们手动创建和设置。 2. IoC容器的初始化在Spring中实现控制反转的方式叫做DI，也就是依赖注入。首先我们使用Spring创建IOC容器是通过ApplicationContex这个类创建的，而这个类有一个顶层的类，叫做BeanFactory，BeanFactory这个类不是由用户直接使用的，而是Spring内部的一个很重要的类，它实现了ioc的基本功能。ApplicationContex也有几个子类，如 ClassPathXmlApplicationContex FileSystemXmlApplicationContex AnnotationConfigApplicationContex 这些子类的区别在于配置信息的位置和类型不同，比如ClassPathXmlApplicationContex的配置信息是XML文件，并且会在ClassPath下找，而FileSystemXmlApplicationContex的配置文件是xml文件，但是需要提供一个全路径名的xml文件，AnnotationConfigApplicationContex它的配置信息是java类和一些注解。 下面以最简单的ClassPathXmlApplicationContex为例说明IoC容器的初始化过程：","categories":[],"tags":[]},{"title":"java IO与Reactor和Proactor模式","slug":"java IO与Reactor和Proactor模式","date":"2020-02-22T05:10:05.423Z","updated":"2020-02-22T12:55:16.985Z","comments":true,"path":"2020/02/22/java IO与Reactor和Proactor模式/","link":"","permalink":"https://www.severin.xyz/2020/02/22/java%20IO%E4%B8%8EReactor%E5%92%8CProactor%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"1. IO模型的演变BIO是java最原始的IO模型，使用BIO实现的服务器最简单的方式如下： public class Demo01 { public static void main(String[] args) throws IOException { ServerSocket serverSocket=new ServerSocket(); serverSocket.bind(new InetSocketAddress(8088)); while (true){ Socket socket = serverSocket.accept(); handler(socket); } } private static void handler(Socket socket){ System.out.println(&quot;处理当前socket&quot;+socket); } } 上面这个代码有非常明显的确定，socket是阻塞的，如果服务端接收到一个连接需要将这个连接处理完成后才能接收下一个连接，可见上面这种模式是不支持并发的。 为了使得服务器支持多个并发的连接，可以考虑使用线程来处理handler方法，代码如下 public class Demo01 { public static void main(String[] args) throws IOException { ServerSocket serverSocket=new ServerSocket(); serverSocket.bind(new InetSocketAddress(8088)); while (true){ Socket socket = serverSocket.accept(); new Thread(()-&gt;{ handler(socket); }).start(); } } private static void handler(Socket socket){ System.out.println(&quot;处理当前socket&quot;+socket); } } 上面的方法目前支持处理并发的请求，但是代价是有多少个连接就需要开辟多个线程，而线程创建和销毁有一定的开销，显然这种方式依然无法面对高并发。 为了解决这种局面，java引入了非阻塞的NIO，NIO对应着linux的IO多路复用模型，为了使得编程方便，又总结了不同的编程模式，如Reactor模式和Proactor模式 2. Reactor模式单Reactor单线程 示意图如下： 基本工作原理是Reactor负责循环监听IO事件，当有事件发生则会进行处理，整个过程都在一个线程中实现，他的特点是简单，没有线程通信和竞争的问题，但是不能完全发挥多核CPU的功能，如果线程崩溃服务则不可用，并且如果处理请求的时间太长，会导致后面的请求阻塞。这种模式适用于业务处理速度快的应用，如redis服务器。 实例代码如下： public class Demo01 { public static void main(String[] args) throws IOException { Selector selector=Selector.open(); ServerSocketChannel serverSocketChannel=ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.bind(new InetSocketAddress(8088)); serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (true){ selector.select(); // 处理事件：连接事件/读事件 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator(); while (iterator.hasNext()){ SelectionKey key = iterator.next(); if (key.isAcceptable()){ handlerAcceptable(serverSocketChannel,selector); }else if (key.isReadable()){ handlerReadable((SocketChannel) key.channel()); } iterator.remove(); } } } private static void handlerAcceptable(ServerSocketChannel channel,Selector selector) throws IOException { SocketChannel socketChannel = channel.accept(); socketChannel.configureBlocking(false); socketChannel.register(selector,SelectionKey.OP_READ); } private static void handlerReadable(SocketChannel socketChannel) throws IOException { ByteBuffer byteBuffer=ByteBuffer.allocate(1024); socketChannel.read(byteBuffer); System.out.println(new String(byteBuffer.array())); } } 单Reactor多线程 示意图如下： 基本工作原理是Reactor只负责监听事件和处理连接事件，其他的事件会交给工作线程组去完成。优点是可以利用多核CPU的处理能力，缺点是数据共享会带来线程安全问题，并且Reactor要处理所有事件的分发，假如事件太多容易出现性能瓶颈。 示例代码如下： public class Demo01 { public static void main(String[] args) throws IOException { Selector selector=Selector.open(); ServerSocketChannel serverSocketChannel=ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.bind(new InetSocketAddress(8088)); serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); ExecutorService executorService = Executors.newCachedThreadPool(); while (true){ selector.select(); // 处理事件：连接事件/读事件 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator(); while (iterator.hasNext()){ SelectionKey key = iterator.next(); if (key.isAcceptable()){ handlerAcceptable(serverSocketChannel,selector); }else if (key.isReadable()){ // 交给线程组处理 executorService.submit(()-&gt;{ handlerReadable((SocketChannel) key.channel()); }); } iterator.remove(); } } } private static void handlerAcceptable(ServerSocketChannel channel,Selector selector) throws IOException { SocketChannel socketChannel = channel.accept(); socketChannel.configureBlocking(false); socketChannel.register(selector,SelectionKey.OP_READ); } private static void handlerReadable(SocketChannel socketChannel){ ByteBuffer byteBuffer=ByteBuffer.allocate(1024); try { socketChannel.read(byteBuffer); System.out.println(new String(byteBuffer.array())); } catch (IOException e) { e.printStackTrace(); } } } 主从Reactor多线程 示意图如下： 基本工作原理是Reactor的功能分离，主Ractor只负责处理连接事件，其他的事件交给子Reactor去处理，子Reactor进行事件分发，将事件交给工作线程去处理。优点是Reactor的功能实现了分离。 3. Proactor模式Proactor模式也是一种基于IO多路复用的模式，只不过它基于异步IO，所以必须OS支持异步IO。 应用程序启动，调用异步操作处理器提供的异步操作接口函数，调用之后应用程序和异步操作处理就独立运行；应用程序可以调用新的异步操作，而其它操作可以并发进行； 应用程序启动Proactor主动器，进行无限的事件循环，等待完成事件到来； 异步操作处理器执行异步操作，完成后将结果放入到完成事件队列； 主动器从完成事件队列中取出结果，分发到相应的完成事件回调函数处理逻辑中；","categories":[],"tags":[]},{"title":"Redis底层数据结构实现原理与源码分析","slug":"Redis底层数据结构实现原理与源码分析","date":"2020-02-09T16:00:00.000Z","updated":"2020-03-05T05:26:38.423Z","comments":true,"path":"2020/02/10/Redis底层数据结构实现原理与源码分析/","link":"","permalink":"https://www.severin.xyz/2020/02/10/Redis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E4%B8%8E%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"一. 字符串1. 介绍Redis中的字符串Redis是由C语言实现的，而C语言中使用char数组表示字符串，并以\\0表示字符串的结尾。但是redis并没有使用C语言中的这种形式的字符串，Redis在设计与实现过程中充分考虑了性能问题，比如在C语言中实现获取字符长度的时间复杂度为O(n)，某些操作C语言字符串也很难完成，比如字符串动态扩容。因此Redis自己实现了字符串，称为简单动态字符串，简称SDS。 2. SDS的结构struct __attribute__ ((__packed__)) sdshdr64 { uint64_t len; /* used */ uint64_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; sds使用结构体来表示，实际上一共有4个类似的结构体，分别是sdshdr8，sdshdr16，sdshdr32,sdshdr64。 结构体的成员包含一个char数组，实际字符串长度len和数组长度alloc。char数组用来存放实际的字符串数据，以\\0结尾，len表示字符串的长度，这样在计算字符串长度时就可以直接获取而不需要去遍历数组。alloc用来表示当前结构体可以容纳多长的字符串，如果字符串在执行appen之类的操作导致字符串长度动态增长，如果新的字符串长度大于alloc，那么就会引发扩容。 3. 字符串扩容sds sdscatlen(sds s, const void *t, size_t len) { size_t curlen = sdslen(s); s = sdsMakeRoomFor(s,len); if (s == NULL) return NULL; memcpy(s+curlen, t, len); sdssetlen(s, curlen+len); s[curlen+len] = &#39;\\0&#39;; return s; } 这个函数的作用是将一个长度为len的字符串附加在字符串s后面。sdsMakeRoomFor用来扩大sds的缓存空间 sds sdsMakeRoomFor(sds s, size_t addlen) { void *sh, *newsh; // 1. 获取sds空闲空间的长度 size_t avail = sdsavail(s); size_t len, newlen; char type, oldtype = s[-1] &amp; SDS_TYPE_MASK; int hdrlen; // 2. 如果sds空闲空间够用，就不需要进行扩容直接返回 if (avail &gt;= addlen) return s; // 3. 获取sds的字符串长度 len = sdslen(s); sh = (char*)s-sdsHdrSize(oldtype); newlen = (len+addlen); // SDS_MAX_PREALLOC=1024x1024 // 下面的代码说明，如果新字符串的长度小于1m，则sds的数组长度为新字符串的长度的2倍，否则加上1MB if (newlen &lt; SDS_MAX_PREALLOC) newlen *= 2; else newlen += SDS_MAX_PREALLOC; // 判断新字符串使用哪个类型的sds头部 type = sdsReqType(newlen); /* Don&#39;t use type 5: the user is appending to the string and type 5 is * not able to remember empty space, so sdsMakeRoomFor() must be called * at every appending operation. */ if (type == SDS_TYPE_5) type = SDS_TYPE_8; // 计算sds头部，用于后面分配内存 hdrlen = sdsHdrSize(type); if (oldtype==type) { // 重新分配内存 newsh = s_realloc(sh, hdrlen+newlen+1); if (newsh == NULL) return NULL; s = (char*)newsh+hdrlen; } else { /* Since the header size changes, need to move the string forward, * and can&#39;t use realloc */ newsh = s_malloc(hdrlen+newlen+1); if (newsh == NULL) return NULL; memcpy((char*)newsh+hdrlen, s, len+1); s_free(sh); s = (char*)newsh+hdrlen; s[-1] = type; sdssetlen(s, len); } sdssetalloc(s, newlen); return s; } 通过上面的函数可以得到扩容机制：首先计算出新字符串的长度，如果该长度小于1MB，则翻倍否则增加1MB，然后判断新字符串属于哪种sds类型，最后重新分配内存得到新的sds返回。 4. Redis对象的表示Redis的对象使用一个叫做redisObj的结构体表示 typedef struct redisObject { unsigned type:4; unsigned encoding:4; unsigned lru:LRU_BITS; int refcount; void *ptr; } robj; type表示对象类型，encodeing表示对象编码，lru表示lru时间，refcount表示该对象的计数器，用来实现内存自动回收，ptr是一个void指针，可以指向任何类型的变量，指向内存实际的空间。 看完这个结构体可以发现，Redis的开发者虽然使用C语言实现的Redis，C语言是一门面向过程的语言，但是redis的开发者试图构造出一个面向对象的体系。 5. 字符串编码Redis中的字符串对象有三种编码：int、embstr，raw。 robj *createStringObject(const char *ptr, size_t len) { // OBJ_ENCODING_EMBSTR_SIZE_LIMIT=44 if (len &lt;= OBJ_ENCODING_EMBSTR_SIZE_LIMIT) // 使用emb编码 return createEmbeddedStringObject(ptr,len); else // 使用raw编码 return createRawStringObject(ptr,len); } 可以看出分配字符串对象时，如果字符串长度小于或等于44时使用embstr编码，否则使用raw编码。 embstr与raw出来字符串长度不同还有什么区别 embstr编码 /* 使用编码OBJ_ENCODING_EMBSTR创建一个string对象，在这个对象中，sds字符串实际上是与对象本身在同一块中分配的不可修改的字符串。 */ robj *createEmbeddedStringObject(const char *ptr, size_t len) { robj *o = zmalloc(sizeof(robj)+sizeof(struct sdshdr8)+len+1); struct sdshdr8 *sh = (void*)(o+1); o-&gt;type = OBJ_STRING; o-&gt;encoding = OBJ_ENCODING_EMBSTR; o-&gt;ptr = sh+1; o-&gt;refcount = 1; if (server.maxmemory_policy &amp; MAXMEMORY_FLAG_LFU) { o-&gt;lru = (LFUGetTimeInMinutes()&lt;&lt;8) | LFU_INIT_VAL; } else { o-&gt;lru = LRU_CLOCK(); } sh-&gt;len = len; sh-&gt;alloc = len; sh-&gt;flags = SDS_TYPE_8; if (ptr) { memcpy(sh-&gt;buf,ptr,len); sh-&gt;buf[len] = &#39;\\0&#39;; } else { memset(sh-&gt;buf,0,len+1); } return o; } raw编码 /* 用编码OBJ_ENCODING_RAW创建一个字符串对象，这是一个普通的字符串对象，其中o-&gt;ptr指向一个适当的sds字符串 */ robj *createRawStringObject(const char *ptr, size_t len) { // sdsnewlen实际上内部调用malloc分配了一个sds的内存空间 // createObject继续分配redisObject return createObject(OBJ_STRING, sdsnewlen(ptr,len)); } /* ===================== Creation and parsing of objects ==================== */ robj *createObject(int type, void *ptr) { // 分配redisObject robj *o = zmalloc(sizeof(*o)); o-&gt;type = type; o-&gt;encoding = OBJ_ENCODING_RAW; o-&gt;ptr = ptr; o-&gt;refcount = 1; /* Set the LRU to the current lruclock (minutes resolution), or * alternatively the LFU counter. */ if (server.maxmemory_policy &amp; MAXMEMORY_FLAG_LFU) { o-&gt;lru = (LFUGetTimeInMinutes()&lt;&lt;8) | LFU_INIT_VAL; } else { o-&gt;lru = LRU_CLOCK(); } return o; } embstr 存储形式将 RedisObject 对象头和 SDS 对象连续存在一起，使用 malloc 方法一次分配。而 raw 存储形式不一样，它需要两次 malloc，两个对象头在内存地址上一般是不连续的。如下图所示: 6. sds的优点 获取字符串长度的复杂度为O(1) API是安全的，不会造成缓冲区溢出 修改字符串长度N次最多需要执行N次内存重分配，预分配和惰性释放 可以保存文本或者二进制数据，可以使用len属性判断是否结束 可以使用C库中的函数 7. 疑问 为什么以44作为两种编码的分界点 内存分配器一次能分配64字节，如果超过64字节就会转化为raw格式。字符串对象由对象头和sds头以及字符串组成。对象头占了16字节，sds头占了（sdshdr8）占了3字节。64-16-3=45字节，但是字符串末尾需要使用\\0，所以还剩下44字节。 字符串的应用 作为计数器 存储简单的键值对 将对象json序列化后存储，获取时反序列化 二. 链表1. 介绍Redis中的list2. linkedlist的结构list节点 typedef struct listNode { struct listNode *prev; struct listNode *next; void *value; } listNode; list节点包含前后指针以及一个void类型的指针，说明该链表节点可以放任何类型的数据（有点泛型的意思）。前后指针说明这个链表是双向链表 list底层数据结构 typedef struct list { listNode *head; listNode *tail; void *(*dup)(void *ptr); void (*free)(void *ptr); int (*match)(void *ptr, void *key); unsigned long len; } list; 使用结构体list表示链表，list结构体中包含头尾指针和链表长度。 list相关的常用方法 list *listCreate(void);// 创建链表 void listRelease(list *list);// 释放链表 void listEmpty(list *list);// 清空链表 list *listAddNodeHead(list *list, void *value);// 在链表头添加元素 list *listAddNodeTail(list *list, void *value);// 在链表尾添加元素 list *listInsertNode(list *list, listNode *old_node, void *value, int after); void listDelNode(list *list, listNode *node); listIter *listGetIterator(list *list, int direction); listNode *listNext(listIter *iter); void listReleaseIterator(listIter *iter); list *listDup(list *orig); listNode *listSearchKey(list *list, void *key); listNode *listIndex(list *list, long index); void listRewind(list *list, listIter *li); void listRewindTail(list *list, listIter *li); void listRotate(list *list); void listJoin(list *l, list *o); 上面说明redis实现了对list的各种操作，list结构非常简单。 三. 压缩列表1. 介绍压缩列表当一个列表只包含少量表项，并且每个列表项要么是小整数，要么是较短的字符串 ，那么redis就会使用压缩列表来作为列表的底层实现。 2. ziplist的结构unsigned char *ziplistNew(void) { unsigned int bytes = ZIPLIST_HEADER_SIZE+1;// ZIPLIST_HEADER_SIZE=10字节 unsigned char *zl = zmalloc(bytes); ZIPLIST_BYTES(zl) = intrev32ifbe(bytes);// bytes为4字节 ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(ZIPLIST_HEADER_SIZE);// tail_offset=4字节 ZIPLIST_LENGTH(zl) = 0;// len=2字节 zl[bytes-1] = ZIP_END;// end=1字节 return zl; } 一般来说使用C语言定义数据结构一般会使用结构体，但是这里是直接分配内存。通过上面的方法可以看出ziplist的结构如下图。 bytes是一个4字节无符号整数，用来存储整个ziplist占用的字节数 tail是一个4字节无符号整数，用来存储ziplist最后一个节点的相对于ziplist首地址偏移量 len是一个2字节无符号整数，存储ziplist中节点的数目，最大值为(2^16 - 2)，当zllen大于最大值时，需要遍历整个ziplist才能获取ziplist节点的数目 entry是数据节点 ziplist占用的是一段连续的内存空间。 3. ziplist节点typedef struct zlentry { unsigned int prevrawlensize; /* 记录prevrawlen长度数值所需要的字节数*/ unsigned int prevrawlen; /* 上一个节点的长度 */ unsigned int lensize; /* 表示当前长度表示所需的字节数*/ unsigned int len; /* 表示当前数据节点的长度 */ unsigned int headersize; /* 数据结点的头部信息长度的字节数 */ unsigned char encoding; /* 编码方式 */ unsigned char *p; /* Pointer to the very start of the entry, that is, this points to prev-entry-len field. */ } zlentry; /* Return a struct with all information about an entry. */ void zipEntry(unsigned char *p, zlentry *e) { ZIP_DECODE_PREVLEN(p, e-&gt;prevrawlensize, e-&gt;prevrawlen); ZIP_DECODE_LENGTH(p + e-&gt;prevrawlensize, e-&gt;encoding, e-&gt;lensize, e-&gt;len); e-&gt;headersize = e-&gt;prevrawlensize + e-&gt;lensize; e-&gt;p = p;// 节点实际内容 } ziplist节点如图所示 previous_entry_length 表示前一个节点的长度，已知当前节点首地址可以计算出上一个节点的首地址，用来实现从后往前遍历压缩列表，占1个字节或者5个字节。当上一个节点的长度小于254字节时，占一个字节。当上一个节点的长度大于等于254时，占5个字节，其中第一个字节值为254，后面4个字节用来表示长度 encoding 记录了节点的content属性所保存的数据的类型和长度，使用前两位表示类型，类型有字节数组和整数两种。其他的位表示长度。 字节数组 编码 编码长度 conten属性保存的值 00 1字节 长度小于2^6-1的字节数组 01 2字节 长度小于2^14-1的字节数组 10 5字节 长度小于2^38-1的字节数组 整数 编码 编码长度 content属性保存的值 11000000 1字节 int16类型的整数 11010000 1字节 int32类型的整数 11100000 1字节 int64类型的整数 11110000 1字节 24位有符号整数 11111110 1字节 8位有符号整数 1111xxxx 1字节 没有相应的content，encoding属性表示0到12的数字 content 保存节点的值 4. 压缩列表的连锁更新previous_entry_length：当前一个节点长度小于254字节时占用一个字节；大于或等于254字节时占用5个字节，所以当在某个元素之前插入一个较大的节点，可能导致后面的节点中的previous_entry_length属性由1个字节变成5个字节从而导致后面那个节点也变大，后面的那个节点也可能会超过254字节，这种更新可能会持续下去，称为连锁更新，连锁更新发生后需要重新分配内存。 四. 哈希表1. 介绍哈希表2. 哈希表实现typedef struct dict { dictEntry **table; dictType *type; unsigned long size; unsigned long sizemask; unsigned long used; void *privdata; } dict; 上面的结构体表示一个哈希表结构 typedef struct dictEntry { void *key; void *val; struct dictEntry *next; } dictEntry; 上面的结构体表示一个哈希表Entry节点，包括key，value和一个指针指向下一个节点。 3. 冲突解决添加元素 /* Add an element to the target hash table */ static int dictAdd(dict *ht, void *key, void *val) { int index; dictEntry *entry; /*获取节点插入的位置，如果为-1证明已经存在一个key相等的结点 */ if ((index = _dictKeyIndex(ht, key)) == -1) return DICT_ERR; /* 为节点分配内存，并使用头插法插入元素 */ entry = malloc(sizeof(*entry)); entry-&gt;next = ht-&gt;table[index]; ht-&gt;table[index] = entry; /* Set the hash entry fields. */ dictSetHashKey(ht, entry, key); dictSetHashVal(ht, entry, val); ht-&gt;used++; return DICT_OK; } 通过上面的方法可以知道Redis解决哈希冲突使用的是拉链法 4. rehash随着操作的不断进行，哈希表保存的键值对会逐渐的增多，为了让哈希表的负载因子维持在一个合理的范围之内，当哈希表保存的键值对太多或者太少了，会对哈希表进行扩展或者收缩。这个步骤就是rehash的过程。 int dictRehash(dict *d, int n) { int empty_visits = n*10; /* 最多可以遇到10*n个空桶 */ if (!dictIsRehashing(d)) return 0; while(n-- &amp;&amp; d-&gt;ht[0].used != 0) { dictEntry *de, *nextde; /* 保证rehasIdx不会超过旧桶的大小 */ assert(d-&gt;ht[0].size &gt; (unsigned long)d-&gt;rehashidx); // 如果遇到某个桶是空的，empt_visits要减去1，如果等于0就结束这次rehash操作 while(d-&gt;ht[0].table[d-&gt;rehashidx] == NULL) { d-&gt;rehashidx++; if (--empty_visits == 0) return 1; } // 获取到要rehash的桶 de = d-&gt;ht[0].table[d-&gt;rehashidx]; /* 将旧桶中的所有key移动到新的哈希表中 */ while(de) { uint64_t h; nextde = de-&gt;next; /* Get the index in the new hash table */ h = dictHashKey(d, de-&gt;key) &amp; d-&gt;ht[1].sizemask; de-&gt;next = d-&gt;ht[1].table[h]; d-&gt;ht[1].table[h] = de; d-&gt;ht[0].used--; d-&gt;ht[1].used++; de = nextde; } // 清空旧桶中的元素 d-&gt;ht[0].table[d-&gt;rehashidx] = NULL; // 继续进行下一步 d-&gt;rehashidx++; } /* 如果旧的哈希表中没有元素了，说明rehash完成，把新的哈希表赋给旧的哈希表，然后返回0 */ if (d-&gt;ht[0].used == 0) { zfree(d-&gt;ht[0].table); d-&gt;ht[0] = d-&gt;ht[1]; _dictReset(&amp;d-&gt;ht[1]); d-&gt;rehashidx = -1; return 0; } /* rehash过程没有完全完成，返回1*/ return 1; } 从上面的方法可以看出redis的rehash过程不是一步完成的，这个过程称为渐进式rehash，哈希表的结构如下： 渐进式rehash过程 为table[1]分配内存空间，让哈希表同时持有两个空间 在哈希表维持一个索引计数器变量rehashidx，并将它设置成0，表示rehash工作正式开始 在rehash进行期间，对哈希表的添加删除查找和更新操作，程序除了执行指定操作外，还会顺带将table[0]在rehashidx位置rehash到table[1],当rehash完成，rehashidx的值会增加 经过多次rehash操作，最终会将所有元素都rehash至table[1]，完成后将rehashidx设为-1，表示rehash结束 渐进式rehash过程的其他操作 对于查找、删除、更新操作会在两个table中进行。 渐进式rehash操作的优点 如果不采取rehash的操作，当一个哈希表的元素太多的时候，需要很长的时间才能完成rehash操作，而redis是单线程处理命令的，所以会导致其他命令阻塞。渐进式rehash可以把rehash的时间分摊到多次操作中。 五. 跳跃表1. 介绍跳跃表跳跃表是一种有序的支持快速查找的数据结构，其效率与平衡树差不多，但是实现比平衡树简单。在redis中跳跃表是有序集合的底层结构。 2. 跳跃表的实现package datastruct; import java.util.*; public class SkipList&lt;K,V&gt;{ private Comparator&lt;K&gt; comparator; // 最大高度 private int maxLevel; private int level; private Entry header; // 概率参数 private double p; public SkipList(){ this(5,0.5); } public SkipList(int maxLevel,double p){ this.maxLevel=maxLevel; this.p=p; header=new Entry(null,null,maxLevel); } public SkipList(Comparator&lt;K&gt; comparator){ this(); this.comparator=comparator; } public V search(K key){ Entry entry = searchElement(key); return entry==null?null:(V)entry.value; } private Entry searchElement(K key){ Entry current=header; for (int i=level-1;i&gt;=0;i--){ while (current.next[i]!=null&amp;&amp;this.compare(current.next[i].key,key)&lt;0){ current=current.next[i]; } } current=current.next[0]; if (current!=null&amp;&amp;compare(current.key,key)==0){ return current; } return null; } public void insert(K key,V value){ Entry current=header; Entry[] update=new Entry[maxLevel]; for (int i=level-1;i&gt;=0;i--){ while (current.next[i]!=null &amp;&amp;compare(current.next[i].key,key)&lt;0){ current=current.next[i]; } update[i]=current; } current=current.next[0]; if (current==null||compare(current.key,key)!=0){ int randomLevel= randomLevel(); if (randomLevel&gt;level){ for (int i=level;i&lt;randomLevel;i++){ update[i]=header; } level=randomLevel; } Entry entry=new Entry(key,value,randomLevel); for (int i=0;i&lt;randomLevel;i++){ entry.next[i]=update[i].next[i]; update[i].next[i]=entry; } } } public void remove(K key){ Entry[] update=new Entry[level]; Entry current=header; for(int i=level-1;i&gt;=0;i--){ while (current.next[i]!=null&amp;&amp;compare(current.next[i].key,key)&lt;0){ current=current.next[i]; } update[i]=current; } for (int i=level-1;i&gt;=0;i--){ if (update[i].next!=null&amp;&amp;update[i].next[i].key == key){ update[i].next[i]=update[i].next[i].next[i]; } } } /** * 获取随机的高度 * @return 高度 */ private int randomLevel(){ int level=1; double temp=Math.random(); while (level&lt;maxLevel&amp;&amp;temp&lt;p){ temp=Math.random(); level++; } return level; } public List&lt;V&gt; valueList(){ List&lt;V&gt; list=new ArrayList&lt;&gt;(); Entry current=header.next[0]; while (current!=null){ list.add((V)current.value); current=current.next[0]; } return list; } @SuppressWarnings(&quot;unchecked&quot;) private int compare(Object k1,Object k2){ return comparator!=null?comparator.compare((K)k1,(K)k2): ((Comparable&lt;? super K&gt;)k1).compareTo((K)k2); } static final class Entry{ Object key; Object value; Entry[] next; int level; Entry(Object key,Object value,int level){ this.key=key; this.value=value; this.level=level; this.next=new Entry[level]; } } } 上面是自己实现的一个跳跃表 3. 跳跃表特点分析跳表思想 跳跃表是一种支持快速查找删除插入的有序的列表结构，首先对于列表我们常见的由数组和链表，数组的特点是内存空间连续，对于排序的数组可以通过二分法进行快速查找，但是对于增加元素和删除元素操作，数组需要调整大量元素的位置，并且可能需要重新分配内存空间完成扩容操作。链表的优点是对于修改开销小，但是由于每个元素不是连续的，所以查找需要从第一个元素向后遍历才能找到目标元素。而跳跃表就是在链表的基础上解决了查找的问题，它的思路是普通的单链表查找时每次只能移动一个元素，根据二分查找的思想，如果可以直接找到中间的元素就可以少移动一半的元素，但是链表的元素不是内存连续的，所以无法计算出了，而跳跃表的实现是每个节点不在像单链表一样只有一个后继指针指向下一个结点，而是有多个指针组成的数组。利用这些指针数组就可以实现类似二分查找的操作，理想情况下每隔两个元素连接起来，然后是每隔4个，8个以此类推。这样在查找元素时每次都可以淘汰一半的元素不用考虑。当然理想情况很难实现，为此跳跃表的实现使用概率的方式去决定每隔元素有多少个指针指向下一个元素。 时间复杂度 查找，删除，插入的时间复杂度是O(logn) 4. 为什么使用跳跃表为什么redis使用跳跃表来实现有序集合而不是平衡树如B+树。 Redis的作者是这样说的 1) They are not very memory intensive. It’s up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees. 2) A sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the cache locality of skip lists is at least as good as with other kind of balanced trees. 3) They are simpler to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code. 大概意思是 跳跃表比平衡树占用更少的内存（所有平衡树一般用于基于磁盘的查找，跳跃表用于基于内存的查找） 跳跃表方便进行范围查找 跳跃表的实现比平衡树简单，但是性能差不多 六. 整数集合1. 整数集合介绍整数集合是集合键的底层实现之一，当一个集合只包含整数值元素，并且这个集合的元素数量不多时，Redis就会使用整数集合作为集合键的底层实现。 2. 整数集合的实现typedef struct intset { uint32_t encoding; uint32_t length; int8_t contents[]; } intset; 整数集合的结构体如上所示，包括一个uint32_t的encoding成员代表编码对象，length表示集合元素个数，int8_t数组content用来存放实际的数据，数据的解码按照编码encoding的值来确定。下面时对整数集合的一些操作。 intset *intsetNew(void);// 创建一个新的整数集合 intset *intsetAdd(intset *is, int64_t value, uint8_t *success);// 插入元素 intset *intsetRemove(intset *is, int64_t value, int *success);// 删除元素 uint8_t intsetFind(intset *is, int64_t value);// 查找元素，使用的是二分查找 int64_t intsetRandom(intset *is);// 随机返回一个元素 uint8_t intsetGet(intset *is, uint32_t pos, int64_t *value);// 取出底层数组指定索引中的数 uint32_t intsetLen(const intset *is);// 返回集合大小 size_t intsetBlobLen(intset *is);// 返回整数集合占用的内存数 整数集合的结构如图所示 插入元素 intset *intsetAdd(intset *is, int64_t value, uint8_t *success) { uint8_t valenc = _intsetValueEncoding(value); uint32_t pos; if (success) *success = 1; /* 升级操作 */ if (valenc &gt; intrev32ifbe(is-&gt;encoding)) { return intsetUpgradeAndAdd(is,value); } else { /* 查找待插入的值，如果找到了就进入if语句，否则插入位置赋给pos */ if (intsetSearch(is,value,&amp;pos)) { if (success) *success = 0; return is; } // 扩容并将pos位置的值往后移动 is = intsetResize(is,intrev32ifbe(is-&gt;length)+1); if (pos &lt; intrev32ifbe(is-&gt;length)) intsetMoveTail(is,pos,pos+1); } // 将值插入到pos位置 _intsetSet(is,pos,value); // 更新长度 is-&gt;length = intrev32ifbe(intrev32ifbe(is-&gt;length)+1); return is; } 插入总能保证数组有序，时间复杂度位O(n) 查找操作 uint8_t intsetFind(intset *is, int64_t value) { uint8_t valenc = _intsetValueEncoding(value); return valenc &lt;= intrev32ifbe(is-&gt;encoding) &amp;&amp; intsetSearch(is,value,NULL); } intsetSearch是一个二分查找，具体代码不列出，所以查找的时间复杂度为O(logN) 升级操作 整数集合对应不同的编码，编码包括：INTSET_ENC_INT16,INTSET_ENC_INT32,INTSET_ENC_INT64,分别表示集合元素为16位，32位，64位整数，当插入一个较大的数就可能导致原来的编码无法容纳当前的元素，需要扩大编码，如从16位变成32位。 /* 升级整数集合的编码，然后插入value */ static intset *intsetUpgradeAndAdd(intset *is, int64_t value) { uint8_t curenc = intrev32ifbe(is-&gt;encoding);// 获取当前编码 uint8_t newenc = _intsetValueEncoding(value);// 根据当前待插入的值计算出其编码 int length = intrev32ifbe(is-&gt;length); int prepend = value &lt; 0 ? 1 : 0; /* 设置新的编码并扩容*/ is-&gt;encoding = intrev32ifbe(newenc); is = intsetResize(is,intrev32ifbe(is-&gt;length)+1); //从后往前升级，这样就不会覆盖值。注意，“prepend”变量用于确保intset的开头或结尾都有一个空格。 while(length--) _intsetSet(is,length+prepend,_intsetGetEncoded(is,length,curenc)); // 设置当前值 if (prepend) _intsetSet(is,0,value);// 设置再第0位 else _intsetSet(is,intrev32ifbe(is-&gt;length),value);// 设置在最后一位 is-&gt;length = intrev32ifbe(intrev32ifbe(is-&gt;length)+1);// 修改数组长度 return is; } 从上面的方法可以总结出升级过程： 根据新元素的类型确定编码并且将数组扩容。 根据新的编码从后往前将原来的元素转化成新的编码并放置到合适的位置 设置当前元素（引发当前数组扩容的新元素一定比所有的旧元素大或者小，因此放到第0位或者最后一位 降级操作 没有降级操作 3. 整数集合的优点采用编码和升级的方式保证数组内部的元素类型相同，并且可以节约内存。 4. 整数集合总结整数集合是集合对象的底层实现之一，用来保存不重复的整数集合。底层是一个有序数组，使用二分法进行查找，插入元素是需要维护数组的有序性，使用升级操作带来了灵活性和节约了内存。 七. 对象1. Redis对象介绍上面的实现是Redis对象底层数据结构，Redis并没有直接使用这些结构去实现键值对数据库，而是基于这些结构创建了一个对象系统，这个对象系统有多态的概念也有内存回收机制，并且对象还可以共享，极大的提高了Redis的灵活性和节约了内存。Redis对象用一个redisObject结构体表示。 typedef struct redisObject { unsigned type:4; unsigned encoding:4; unsigned lru:LRU_BITS; /* LRU time (relative to global lru_clock) or * LFU data (least significant 8 bits frequency * and most significant 16 bits access time). */ int refcount; void *ptr; } robj; Redis对象如图所示： type 表示对象类型，如字符串对象，列表对象，集合对象，有序集合对象，哈希对象等 encoding 编码，表示的是对象的底层数据结构 lru 当内存紧张时，通过lru算法淘汰对象 refcount 引用计数 ptr 指向对象的底层数据结构 2. 字符串对象字符串对象有三种编码，分别是int,embstr和raw。 如果一个字符串对象的值可以转化成数字，并且数字的范围不会超过long，则会使用int编码，ptr指针直接指向一个数字，对于int对象，只要执行某个命令使得值不在是一个long型的数字，编码都会发生变化。如果对象保存的是字符串值并且长度小于44，会使用embstr编码，当字符串长度大于44时会转换成raw编码。embstr和raw编码都会对应一个sds对象。embstr编码是一种内存优化的编码技术，它通过一次内存分配，redisObject与sds是连续的。而raw则需要通过两次内存分配，redisObject与sds不是连续的。 3. 列表对象列表对象的编码是压缩列表和双端链表。但在后序版本中使用quiklist来代替传统的压缩列表和双端链表。quickList 是 zipList 和 linkedList 的混合体，它将 linkedList 按段切分，每一段使用 zipList 来紧凑存储，多个 zipList 之间使用双向指针串接起来。 4. 哈希对象哈希对象的编码有压缩列表和哈希表。 当哈希对象保存的元素的键和值的长度都小于64字节并且哈希对象保存的元素个数不超过512字节时使用压缩列表。 5. 集合对象集合对象的编码是整数集合和哈希表。 当集合中的每个元素都是数字并且元素个数不超过512时使用整数集合。 6. 有序集合对象有序集合的编码是跳跃表和压缩列表 跳表编码的有序集合使用zset作为结构作为底层实现，zset结构同时包含一个字典和一个跳跃表。字典和跳跃表都保存了对象和分数。其中字典的key保存对象，value保存分数，用来实现查找某个对象的分数。跳跃表方便实现范围操作。 当有序集合保存的元素数量小于128个并且元素的长度都小于64字节时使用压缩列表实现。 7. 类型检查与多态命令当向服务器发送一个对key进行操作的命令，首先回去检查这个key所对应redisObject对象的类型，然后判断该操作是不是可以用在这种类型的值，如果不能则返回错误给客户端，否则会根据redisObject的编码去决定调用底层数据结构的方法去执行。 8. 内存回收当创建一个对象时引用计数值会被初始化为1，当这个对象被其他引用指向时，它的引用计数值会增加1，当对象不被其他引用指向时，引用计数会减1。当对象的引用计数值为0时，对象所占用的内存会被释放。 9.对象共享通过引用计数值，可以实现对象的共享，但是只有整数对象才能被共享，因为判断两个整数是否相等的时间复杂度为常数时间。对于0到9999的共享，执行object refcount key指令会返回2147483647。 10. 对象的空转时长对象的空转时长是指当前对象有多久没有被访问过，某些内存回收策略会使得空转时长更长的元素淘汰，使用object idletime key查询空转时长。 11. 内存分配默认是jemalloc。 jemalloc作为Redis的默认内存分配器，在减小内存碎片方面做的相对比较好。jemalloc在64位系统中，将内存空间划分为小、大、巨大三个范围；每个范围内又划分了许多小的内存块单位；当Redis存储数据时，会选择大小最合适的内存块进行存储。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"redis","slug":"数据库/redis","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"redis","slug":"redis","permalink":"https://www.severin.xyz/tags/redis/"}]},{"title":"Redis单机数据库的实现原理","slug":"Redis单机数据库的实现原理","date":"2020-02-09T16:00:00.000Z","updated":"2020-03-05T05:28:59.172Z","comments":true,"path":"2020/02/10/Redis单机数据库的实现原理/","link":"","permalink":"https://www.severin.xyz/2020/02/10/Redis%E5%8D%95%E6%9C%BA%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"1. 数据库1. Redis服务器的实现struct redisServer { redisDb *db;// 指向数据库数组表示 int dbnum;// 数据库的数量，默认16 // ... 后面还有很多参数 }; redisServer代表了redis服务器，里面的db指针指向数据库数组，dbnum表示当前服务器有多少个数组。 2. 数据库的切换每个redis客户端都有一个目标数据库，当客户端执行读或写命令时操作的都是目的数据库。 struct redisClient { int fd; redisDb *db; // 还有其他字段 } db指针指向当前客户端对应的目标数据库。 切换数据库时，只需要将redisClient的db指针指向相对应的数据库即可。 3. redis数据库的实现Redis的数据库是一个键值对 struct redisDb{ dict * dict;// 就是哈希的底层结构，保存了所有的键值对 } redisDb封装了一个字典，字典的键是字符串对象，字典的值是字符串对象、列表对象、集合对象、有序集合对象、哈希对象等。对键值对的增删查改就是在这个键值对空间中操作。对一个键的读取，redis服务器还会维护命中信息，lru信息等，如果发现过期还会进行删除。 4. 键的生存时间和过期时间的实现设置过期的时间如下： expire key ttl 设置生存时间，单位秒 pexpire key ttl 设置生存时间，单位毫秒 expireat key timestamp 设置过期时间戳，单位秒 pexpireat key timestamp设置过期时间戳，单位毫秒 所有的命令都会转化成第4条命令执行。 redis数据库使用一个过期字典保存所有键的过期时间。 struct redisDb{ dict * dict;// 数据字典 dict * expire;//过期字典 } 过期判断 通过过期字典，程序可以用以下步骤检查一个给定键是否过期：检查给定键是否存在于过期字典：如果存在，那么取得键的过期时间。检查当前UNIX时间戳是否大于键的过期时间：如果是的话，那么键已经过期；否则的话，键未过期。 过期删除策略 定时删除：在设置键的过期时间的同时，创建一个定时器（timer），让定时器在键的过期时间来临时，立即执行对键的删除操作。 采用定时删除，过期的数据可以被及时删除，有利于节约内存，但是定时删除需要创建定时器，因此需要消耗额外的CPU 惰性删除：放任键过期不管，但是每次从键空间中获取键时，都检查取得的键是否过期，如果过期的话，就删除该键；如果没有过期，就返回该键。 惰性删除只有在获取键的时候才会判断，虽然不会消耗额外内存，但是会导致已经过期的数据积累在内存中，导致内存的利用率降低 定期删除：每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。至于要删除多少过期键，以及要检查多少个数据库，则由算法决定。 定期删除是定时删除和惰性删除的一种折中，可以限制对CPU的影响，有避免大量的过期对象的堆积。 在这三种策略中，第一种和第三种为主动删除策略，而第二种则为被动删除策略。Redis服务器实际使用的是惰性删除和定期删除两种策略。 5. 对过期键的操作 定期删除 在规定的时间内分多次的去遍历服务器的各数据库，从中删除过期的键。 惰性删除 读写命令都会检查命令是否过期，如果过期返回不存在该键。 RDB过程 如果键已经过期，该键不会保存在RDB文件中，导入RDB文件时如果键已经过期则不会保存在内存中。 AOF过程 无论键是否过期都会保存到AOF文件中，如果键过期了会追加一条删除命令，AOF重写时，会把过期的命令删除。 主从复制模式 从服务不会删除过期的键，即使客户端请求一个过期的键也会返回，只有当服务器判断键过期后向从服务器显视发送一条删除命令，从服务器中的过期键才会被删除。 2. RDB持久化1. RDB介绍redis是一个基于内存的数据库，当服务器关机后内存的数据会消失，为了实现redis的持久化，有两种机制，RDB和AOF。RDB的基本原理是定时将内存中的数据备份到磁盘文件中。 2. 手动创建RDB文件的两种方式save和bgsave命令是用来手动创建RDB文件。 save命令执行后，会阻塞当前线程创建rdb文件直到创建完成，期间服务器不会响应任何命令 bgsave命令会先fork一个子进程然后由子进程去创建rdb文件，不会阻塞服务器 当服务器载入rdb文件时会阻塞 3. 自动创建RDB的方式除了手动创建外，还有自动创建的方式，需要通过配置文件实现，默认是900秒内至少有一次修改或者300秒内至少有10次修改，或者60秒内至少有10000次修改就会自动创建rdb文件。服务器的内部使用savaparams来保存参数信息，还会维护一个计时器，计时器保存了距离上一次rdb一共进行了多少次修改操作。Redis的服务器周期性操作函数serverCron默认每隔100毫秒就会执行一次，该函数用于对正在运行的服务器进行维护，如果发现满足条件就会执行bgsave命令 4. RDB文件结构RDB文件的结构如下： 前面两个字段是用来确定这是某个版本的rdb文件，分别是REDIS魔数和redis版本号。database字段保存的是各个数据库的信息，如果数据库的个数为0，则database字段为空，eof表示rdb文件的末尾，checksum是校验和，用来判断当前rdb文件是否损坏。 database结构 SELECTDB是一个常量表示这是一个数据库的开始，dbnumber是数据库的编号，kev_value_pairs是键值对。 键值对结构 上面是不带过期时间的键值对，type记录了value的类型，key是一个字符串对象，value表示值对象 上面是带过期时间的键值对结构，EXPIRETIME_MS是一个常数标记这个键值对带过期时间，ms是过期的时间戳。 value的结构 3. AOF持久化1. AOF介绍AOF也是一种持久化机制，与RDB持久化通过保存数据库中的键值对来记录数据库状态不同，AOF持久化是通过保存Redis服务器所执行的写命令来记录数据库状态的。 4. 事件1. 事件介绍Redis服务器是一个事件驱动程序，服务器主要处理文件事件和时间事件。 文件事件 redis客户端与服务器通过socket进行连接，文件事件是对套接字的操作的抽象。服务器与客户端的通信会产生事件，服务器通过监听这些事件来完成网络操作。 时间事件 Redis服务器中的一些操作（比如serverCron函数）需要在给定的时间点执行，而时间事件就是服务器对这类定时操作的抽象。 2. 文件事件redis基于reactor模式开发文件事件处理器来处理各种网络事件。文件事件处理器使用io多路复用模型通过一个线程去监听多个套接字，当被监听的套接字准备好连接，读取，写入，关闭等操作时文件事件处理器会调用与当前套接字关联的事件处理器来处理事件。 文件事件处理器由四个部分组成，分别是套接字，IO多路复用程序，文件事件分派器和事件处理器。Redis的IO多路复用程序封装了如select，poll，epoll等io多路复用函数库来实现。当套接字可读，比如客户端向套接字指向write操作，执行连接操作，执行关闭操作时产生读事件。当套接字可写时，套接字产生写事件。IO多路复用同时监听多个套接字的读写事件，如果有一个套接字产生了某个事件，IO多路复用程序会把这个事件交给文件事件分派器，文件事件分派器会选择相对应的事件处理器进行处理。 3. 时间事件Redis的时间事件分为定时事件和周期性事件，定时事件是在某个时间点发生，定期时间是程序每个一个后期产生一次的事件。一个时间时间主要由三个属性组成，id，when和timeProc。id是一个全局唯一的标识号，id从小到大递增，新事件的id比旧事件的id要大。when表示这个事件到达的时间，timeProc表示时间事件处理器，当时间时间达到时，服务器会调用相应的处理器来处理事件。事件处理器的返回值标识这个事件的类型，如果时定时事件，那么这个事件会被删除，如果时定期事件，会更新when属性。 redis将所有的时间时间都放在一个无需链表中，每当时间时间执行器运行时会遍历整个链表，查找已经到达的时间事件，然后调用响应的事件处理器。 4. 事件调度因为服务器中同时存在文件事件和时间事件两种事件类型，所以服务器必须对这两种事件进行调度，决定何时应该处理文件事件，何时又应该处理时间事件，以及花多少时间来处理它们等等。 从事件处理的角度来看，Redis服务器的运行流程如下： 5. 客户端1. 客户端介绍对于每个与服务器进行连接的客户端，服务器都为这些客户端建立了相应的redis.h/redisClient结构，这个结构保存了客户端当前的状态信息，以及执行相关功能时需要用到的数据结构。表示redis服务器的数据结构redisServer有一个成员为clients的链表，用来维护所有与服务器连接的客户端。 2. 客户端常用属性 套接字描述符 客户端状态的fd属性记录了客户端正在使用的套接字描述符，如果是伪客户端，这个值为-1 名字 默认情况下，没有名字，可以通过client setname 名字设置名字 标志 客户端的标志属性flags记录了客户端的角色（role），以及客户端目前所处的状态 输入缓冲区 客户端状态的输入缓冲区用于保存客户端发送的命令请求 命令与命令参数 在服务器将客户端发送的命令请求保存到客户端状态的输入缓冲区属性之后，服务器将对命令请求的内容进行分析，并将得出的命令参数以及命令参数的个数分别保存到客户端状态的argv属性和argc属性 命令表 当服务器从协议内容中分析并得出argv属性和argc属性的值之后，服务器将根据项argv[0]的值，在命令表中查找命令所对应的命令实现函数 输出缓冲区 执行命令所得的命令回复会被保存在客户端状态的输出缓冲区里面，每个客户端都有两个输出缓冲区可用，一个缓冲区的大小是固定的，另一个缓冲区的大小是可变的 固定大小的缓冲区用于保存那些长度比较小的回复，比如OK、简短的字符串值、整数值、错误回复等等。 可变大小的缓冲区用于保存那些长度比较大的回复，比如一个非常长的字符串值，一个由很多项组成的列表，一个包含了很多元素的集合等等。 固定缓冲区使用char数组和一个下标实现，可变缓冲区使用链表来链接所有的字符串对象 身份验证 客户端状态的authenticated属性用于记录客户端是否通过了身份验证，如果该值为0表示未通过身份认证，为1表示客户端已通过身份认证，当客户端authenticated属性的值为0时，除了AUTH命令之外，客户端发送的所有其他命令都会被服务器拒绝执行，authenticated属性仅在服务器启用了身份验证功能时使用。 时间 typedef struct redisClient{ time_t ctime; time_t lastinteraction; time_t obuf_soft_limit_reached_time }redisClient; ctime记录了创建客户端的时间 lastinteraction属性记录了客户端与服务器最后一次进行互动（interaction）的时间 obuf_soft_limit_reached_time属性记录了输出缓冲区第一次到达软性限制（soft limit）的时间 3. 客户端的操作 创建客户端 如果客户端是通过网络连接与服务器进行连接的普通客户端，那么在客户端使用connect函数连接到服务器时，服务器就会调用连接事件处理器，为客户端创建相应的客户端状态，并将这个新的客户端状态添加到服务器状态结构clients链表的末尾 关闭客户端 关闭客户端的原因有很多，如： 客户端进程退出或者被杀死 客户端向服务器发送了带有不符合协议格式的命令请求 客户端成为了CLIENT KILL命令的目标 用户为服务器设置了timeout配置选项，那么当客户端的空转时间超过timeout选项设置的值时，客户端将被关闭 客户端发送的命令请求的大小超过了输入缓冲区的限制大小（默认为1 GB） 发送给客户端的命令回复的大小超过了输出缓冲区的限制大小，那么这个客户端会被服务器关闭 客户端输出缓冲区有两种模式限制 硬性限制 如果输出缓冲区的大小超过了硬性限制所设置的大小，那么服务器立即关闭客户端 软性限制 如果输出缓冲区的大小超过了软性限制所设置的大小，但还没超过硬性限制，那么服务器将使用客户端状态结构的obuf_soft_limit_reached_time属性记录下客户端到达软性限制的起始时间；之后服务器会继续监视客户端，如果输出缓冲区的大小一直超出软性限制，并且持续时间超过服务器设定的时长，那么服务器将关闭客户端；相反地，如果输出缓冲区的大小在指定时间之内，不再超出软性限制，那么客户端就不会被关闭，并且obuf_soft_limit_reached_time属性的值也会被清零 6. 服务器1. 命令请求的执行过程 发送命令请求 Redis服务器的命令请求来自Redis客户端，当用户在客户端中键入一个命令请求时，客户端会将这个命令请求转换成协议格式，然后通过连接到服务器的套接字，将协议格式的命令请求发送给服务器 读取命令请求 当客户端与服务器之间的连接套接字因为客户端的写入而变得可读时，服务器会读取套接字中协议格式的命令请求，并将其保存到客户端状态的输入缓冲区里面。对输入缓冲区中的命令请求进行分析，提取出命令请求中包含的命令参数，以及命令参数的个数，然后分别将参数和参数个数保存到客户端状态的argv属性和argc属性里面，最后调用命令执行器，执行客户端指定的命令。 命令执行器的实现 据客户端状态的argv[0]参数，在命令表中查找参数所指定的命令，并将找到的命令保存到客户端状态的cmd属性里面。然后在执行命令之前会对参数进行检测以及身份认证的检测，之后调用命令函数执行命令，最后服务器还会执行一些后续操作如AOF。 命令实现函数会将命令回复保存到客户端的输出缓冲区里面，并为客户端的套接字关联命令回复处理器，当客户端套接字变为可写状态时，服务器就会执行命令回复处理器，将保存在客户端输出缓冲区中的命令回复发送给客户端。客户端收到回复后会打印出来。 2. severCron函数Redis服务器中的serverCron函数默认每隔100毫秒执行一次，这个函数负责管理服务器的资源，并保持服务器自身的良好运转。 更新服务器时间缓存 Redis服务器中有不少功能需要获取系统的当前时间，而每次获取系统的当前时间都需要执行一次系统调用，为了减少系统调用的执行次数，服务器状态中的unixtime属性和mstime属性被用作当前时间的缓存：unixtime保存了秒级精度的系统当前unix时间戳，mstime保存了毫秒精度的系统当前unix时间戳。注意这些时间是不精确的，只能用在对时间精度要求不高的应用上 更新LRU时钟 服务器状态中的lruclock属性保存了服务器的LRU时钟，这个属性和上面介绍的unixtime属性、mstime属性一样，都是服务器时间缓存的一种。当服务器要计算一个数据库键的空转时间，程序会用服务器的lruclock属性记录的时间减去对象的lru属性记录的时间，得出的计算结果就是这个对象的空转时间 更新服务器每秒执行命令次数 更新服务器内存峰值记录 处理sigtem信号 管理客户端资源 管理数据库资源 执行被延迟的bgrewriteraof 在服务器执行BGSAVE命令的期间，如果客户端向服务器发来BGREWRITEAOF命令，那么服务器会将BGREWRITEAOF命令的执行时间延迟到BGSAVE命令执行完毕之后。 检查持久化操作的运行状态 将AOF缓冲区的内容写入AOF文件 关闭异步客户端 增加cronloops计数器的值 3. 初始化服务器一个Redis服务器从启动到能够接受客户端的命令请求，需要经过一系列的初始化和设置过程，比如初始化服务器状态，接受用户指定的服务器配置，创建相应的数据结构和网络连接等等。 初始化服务器状态结构 载入配置选项 初始化服务器数据结构 还原数据库状态 执行事件循环","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"redis","slug":"数据库/redis","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"redis","slug":"redis","permalink":"https://www.severin.xyz/tags/redis/"}]},{"title":"TCP抓包探究","slug":"TCP抓包探究","date":"2020-02-01T08:55:41.035Z","updated":"2020-02-01T09:12:26.108Z","comments":true,"path":"2020/02/01/TCP抓包探究/","link":"","permalink":"https://www.severin.xyz/2020/02/01/TCP%E6%8A%93%E5%8C%85%E6%8E%A2%E7%A9%B6/","excerpt":"","text":"1. 连接超时问题首先使用telnet命令对一个不存在的ip或端口进行测试，同时使用抓包工具抓包。 抓包数据 分析结果 可以看到时间的变化近似为1、2、4、8、16，即第一个报文发送后隔了1秒发送，然后隔了2秒发送，然后隔了4秒发送，最后隔了8秒发送。可以看出TCP连接建立超时后采用指数退避的方式重新发送连接请求报文。 2. TCP的连接释放理论告诉我们TCP的连接释放需要经过三次握手和四次挥手，我们基于这个理论进行探究，看是否可以使用抓包工具抓到七个这样的报文。 三次握手： 四次挥手： 其中第1,2,3,6这几个报文是四次挥手报文","categories":[],"tags":[]},{"title":"Spring探究之AOP","slug":"Spring探究之AOP","date":"2020-01-29T16:00:00.000Z","updated":"2020-02-26T01:46:13.857Z","comments":true,"path":"2020/01/30/Spring探究之AOP/","link":"","permalink":"https://www.severin.xyz/2020/01/30/Spring%E6%8E%A2%E7%A9%B6%E4%B9%8BAOP/","excerpt":"","text":"1. 什么是AOP Aspect-oriented Programming (AOP) complements Object-oriented Programming (OOP) by providing another way of thinking about program structure. The key unit of modularity in OOP is the class, whereas in AOP the unit of modularity is the aspect. Aspects enable the modularization of concerns (such as transaction management) that cut across multiple types and objects. (Such concerns are often termed “crosscutting” concerns in AOP literature.) 上面是Spring官网对AOP的解释，大致的意思是：面向方面编程通过提供另一种考虑程序结构的方式补充了面向对象编程。OOP中模块化的关键单元是类，而在AOP中模块化的单元是方面。方面支持跨多个类型和对象的关注点的模块化。 官方术语好像看不懂，接下来看下面这个例子： @RestController public class MyTestController { @GetMapping(&quot;/hello&quot;) public String hello(){ try { // 模拟方法执行的时间 System.out.println(&quot;方法执行...&quot;); Thread.sleep((long) (Math.random()*2000)); } catch (InterruptedException e) { e.printStackTrace(); } return &quot;success&quot;; } } 上面的方法是一个很简单的Handler用来处理http请求，现在的要求是用日志打印出这个handler处理时间。很显然只需要在业务的前面和后面增加一个获取当前时间的代码，最后计算两个值的差，就像下面这个方法一样。 @RestController public class MyTestController { @GetMapping(&quot;/hello&quot;) public String hello(){ // 求开始时间 long starTime=System.currentTimeMillis(); try { // 模拟方法执行的时间 System.out.println(&quot;方法执行...&quot;); Thread.sleep((long) (Math.random()*2000)); } catch (InterruptedException e) { e.printStackTrace(); } // 求结束时间 long endTime=System.currentTimeMillis(); // 计算消耗的时间 System.out.println(&quot;all time:&quot;+(endTime-starTime)+&quot;ms&quot;); return &quot;success&quot;; } } 但是一个web应用可能涉及到数十上百的handler，如果现在要求每个handler都要拥有打印运行时间的要求，如何实现呢？以OOP编程思想来看，只有在每一个类中都写重复的代码，显然这些冗余的代码会在后期维护时带来问题，比如有一天要求所有handler都不用打印运行时间，那就只能去每一个类里面找到这些代码逐行删除了。 其实上面的问题在于，有一些功能与业务无关，但是却需要应用到多个类中，如果使用传统的OOP思想，只能够在每个类的方法中都添加相同的代码，这些代码散落在各个类中，且与业务无关，难以维护。 那么看一下AOP是怎么解决这个问题的。 本文使用的是Spring Boot，Spring Boot开启注解的方式很简单。首先添加starter &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;/dependency&gt; 然后只需要在Spring Boot的启动类中添加注解@EnableAspectProxy代理即可（不加注解也可以） @Aspect @Component public class RunTimeAspect { @Pointcut(&quot;execution(* site.zeng.demo.springbootstudy.aoptest.controller.*.*(..))&quot;) public void runTimePointCut(){} @Around(&quot;runTimePointCut()&quot;) public Object printRunTime(ProceedingJoinPoint joinPoint){ Object ret=null; System.out.println(&quot;hello&quot;); try { long startTime=System.currentTimeMillis(); ret = joinPoint.proceed(); long endTime=System.currentTimeMillis(); System.out.println(&quot;all time:&quot;+(endTime-startTime)+&quot;ms&quot;); } catch (Throwable throwable) { throwable.printStackTrace(); } return ret; } } 只需要实现上面这个类就可以统计所有的handler中方法的执行时间，并且也只需在这个类中进行修改就可以改变功能。 通过上面的例子我们大概可以总结一下AOP的思想： AOP编程思想是对OOP编程思想的一种补充，解决了OOP难以解决的问题，在OOP编程中，模块化的单位是类，当需要实现业务功能时是依靠不同类的对象的方法采用从上至下的方式组合而成，比如一个普通的web应用程序，业务功能的实现是从controller层到service层再到dao层，这种良好的分层结构使得代码的耦合性降低，但是有一类与业务无关的方法可能会散落再每一层的各个类中，比如一个统计方法运行时间的代码，如果我们的需求是作用再每一个controller方法中，或者指定的某些方法中，那么我们就需要将重复的代码写多编，显然这样会带来几个问题，第一个是散落的代码难以维护，第二个是业务无关的代码与业务代码发生了耦合。而AOP就可以解决这样的问题，AOP关注的不是某一个类或者式某一个方法，而是关注的是一系列方法运行的某些点，比如方法调用前，方法调用后等等，这样就可以使用AOP在这些散落在各个类的关注点增加类方法的功能，其基本的做法是使用一些表达式来描述这些位置，然后使用方法对这些位置进行增强，这样的好处是，不在需要将与业务无关的代码与业务代码耦合再一起，第二是这些与业务无关的代码可以集中写在一个地方，而不需要把它重复写在各个类中，使得代码更加容易维护。 2. AOP的基本使用使用AOP只需要掌握三个知识点：AOP的基本概念，表达式的规范，通知的类型。 AOP的基本概念 Aspect Join Point Advice Poincut Introduction Target Object Aop proxt Weaving 表达式规范 通知类型 3. AOP源码分析总所周知，AOP的实现靠的时JDK的Proxy类和Cglib（真的是众所周知），那么我们来用代码验证一下全部过程。既然我们都知道Spring AOP会生成代理对象，那么一定会经历这样一个流程，在获取bean对象的某一步会先创建一个非代理类对象，然后调用某些方法生成代理对象，我们的目标就是找到这个调用点。 一个简单的Demo: @SpringBootApplication @EnableAspectJAutoProxy public class SpringBootAopApplication { public static void main(String[] args) { ConfigurableApplicationContext context = new SpringApplicationBuilder(SpringBootAopApplication.class) .web(WebApplicationType.NONE).run(args); // 通过beanName从容器中获取bean对象 DemoBean bean = (DemoBean) context.getBean(&quot;demoBean&quot;); bean.test(); } } 这里的DemoBean这个bean对象的作用域是prototype，这意味着获取bean不能从单例池中获取而是要重新创建。 进入context.getBean(&quot;demoBean&quot;)方法 //所属类：AbstractApplicationContext public Object getBean(String name) throws BeansException { assertBeanFactoryActive(); return getBeanFactory().getBean(name); } 继续进入getBeanFactory().getBean(name)方法 public Object getBean(String name) throws BeansException { return doGetBean(name, null, null, false); } 继续进入doGetBean(name, null, null, false);方法 protected &lt;T&gt; T doGetBean(final String name, @Nullable final Class&lt;T&gt; requiredType, @Nullable final Object[] args, boolean typeCheckOnly) throws BeansException { final String beanName = transformedBeanName(name); Object bean; // 从单例池中获取对象，如果存在就返回 Object sharedInstance = getSingleton(beanName); if (sharedInstance != null &amp;&amp; args == null) { // 省略日志代码 bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); } else { // 当前正在创建这个bean，说明出现了循环引用并且无法解决，如何解决循环引用？ if (isPrototypeCurrentlyInCreation(beanName)) { throw new BeanCurrentlyInCreationException(beanName); } // 检查bean所对应的定义BeanDefinition是否在容器中 BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) { // 如果找不到就找其父容器，这部分代码省略 } // 省略部分代码 try { final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); // 保证当前bean所依赖的bean初始化 String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) { // 这部分代码省略，与我们的目标无关 } // 下面这个if选择语句时根据bean的类型去决定后面的操作，我们只考虑prototype类型的 if (mbd.isSingleton()) { // 这里省略处理的代码 } else if (mbd.isPrototype()) { Object prototypeInstance = null; try { // 在创建bean之前进行某些操作 这里先不管 beforePrototypeCreation(beanName); // 这里创建bean的实例，经过debug发现，返回值已经是代理对象 prototypeInstance = createBean(beanName, mbd, args);// 下一步分析 } // 省略 } // 后面代码与目标无关，全部省略 } 继续进入createBean(beanName, mbd, args)方法 @Override protected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException { if (logger.isTraceEnabled()) { logger.trace(&quot;Creating instance of bean &#39;&quot; + beanName + &quot;&#39;&quot;); } RootBeanDefinition mbdToUse = mbd; // 通过bean的名称获取class对象 Class&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName); if (resolvedClass != null &amp;&amp; !mbd.hasBeanClass() &amp;&amp; mbd.getBeanClassName() != null) { mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); } // try { // Give BeanPostProcessors a chance to return a proxy instead of the target bean instance. // 上面的官方注释的翻译是：给beanpostprocessor一个机会来返回代理而不是目标bean实例 // 到此我为止我们知道了产生代理的过程与beanpostprocessor有关 // 进入下一步，目标越来越清晰了,不信的是这个方法返回了null，所以需要继续分析 Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) { return bean; } } // 异常处理代码省略 try { // 创建bean实例，由于后面直接返回了，所以需要进入这个方法 Object beanInstance = doCreateBean(beanName, mbdToUse, args); // 日志代码省略 // 返回bean实例 return beanInstance; } // 后面代码全部省略 } 继续进入doCreateBean(beanName, mbdToUse, args) protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException { // 前面代码全部省略 // Initialize the bean instance. 初始化bean实例，此时bean不是代理对象 // 而这个方法返回值确实exposedObject，所以肯定后面作了某些操作 Object exposedObject= bean; try { populateBean(beanName, mbd, instanceWrapper); // debug发现这一步返回的是一个代理对象 exposedObject = initializeBean(beanName, exposedObject, mbd);// 下一步进入 } // 后面代码省略 return exposedObject; } 继续进入initializeBean(beanName, exposedObject, mbd) protected Object initializeBean(final String beanName, final Object bean, @Nullable RootBeanDefinition mbd) { // 前面代码省略 if (mbd == null || !mbd.isSynthetic()) { // 这一步返回了代理对象，继续深入 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } return wrappedBean; } 接下来进入applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName) @Override public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException { Object result = existingBean; for (BeanPostProcessor processor : getBeanPostProcessors()) { Object current = processor.postProcessAfterInitialization(result, beanName); if (current == null) { return result; } result = current; } return result; } 首先代码没有手动添加任何BeanPostProcessor对象，那么我们看一下容器自身有哪些BeanPostProcessor，其中有一个一定与动态代理有关。 框出来的那个类看上去就和生成代理对象有关，debug发现确实与这个对象有关。 至此我们知道了生成代理对象与AnnotationAwareAspectJAutoProxyCreator的实例有关，接下来的分析过程很简单，只需要研究这个类即可。首先看一下类的继承关系。 可以看到AnnotationAwareAspectAutoProxyCreator是AbstractAutoProxyCreator的子类，并且也是BeanPostProcessor的实现类。 只要探究一下AnnotationAwareAspectAutoProxyCreator的方法就可以知道如何生成代理的。 public Object postProcessAfterInitialization (@Nullable Object bean, String beanName) { if (bean != null) { Object cacheKey = getCacheKey(bean.getClass(), beanName); if (this.earlyProxyReferences.remove(cacheKey) != bean) { // 返回代理对象 return wrapIfNecessary(bean, beanName, cacheKey); } } return bean; } protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) { if (StringUtils.hasLength(beanName) &amp;&amp; this.targetSourcedBeans.contains(beanName)) { return bean; } if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) { return bean; } if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) { this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; } // Create proxy if we have advice. Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) { this.advisedBeans.put(cacheKey, Boolean.TRUE); // 生成代理对象的语句，下一步分析 Object proxy = createProxy( bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; } this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; } protected Object createProxy(Class&lt;?&gt; beanClass, @Nullable String beanName, @Nullable Object[] specificInterceptors, TargetSource targetSource) { // 前面代码省略 // 继续深入 return proxyFactory.getProxy(getProxyClassLoader()); } public Object getProxy(@Nullable ClassLoader classLoader) { return createAopProxy().getProxy(classLoader); } protected final synchronized AopProxy createAopProxy() { if (!this.active) { activate(); } return getAopProxyFactory().createAopProxy(this); } public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException { if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) { // 获取被代理的对象的Class对象 Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) { // 抛出异常省略 } if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) { // jdk代理对象 return new JdkDynamicAopProxy(config); } // cglib代理对象 return new ObjenesisCglibAopProxy(config); } else { return new JdkDynamicAopProxy(config); } } 到此为止大功告成，我们找到了如何生成AOP代理对象。 4. AOP总结Aop的实现与IoC容器有关，Ioc容器在实现的时候提供了BeanPostProcessor接口供用户实现，这个类的作用是在IoC容器创建bean实例之后提供一个机会给用户去改变这个bean，BeanPostProcessor有两个接口，一个是postProcessBeforeInitialization方法，另一个是postProcessAfterInitialization方法，前者在创建完bean实例之后，调用初始化方法之前调用，后再在初始化方法调用完之后再调用，这两个方法的参数都是传入一个bean，然后返回值也是一个bean，这样就可以在方法中对bean进行操作甚至是修改替换，然后返回。而Aop正是利用这个接口实现的，AOP实现了一个叫做AbstractAutoProxyCreator抽象类，这个抽象类的子类以不同的方式实现AOP，比如常见的使用注解那一套，就对应着子类AnnotaionAwareAspectAutoProxyCreator。AOP会根据bean实例的特点选择不同的方式生成动态代理，比如如果bean实现了接口接口，那么就使用JDK的Proxy类，否则会使用Cglib实现。","categories":[{"name":"框架","slug":"框架","permalink":"https://www.severin.xyz/categories/%E6%A1%86%E6%9E%B6/"},{"name":"spring","slug":"框架/spring","permalink":"https://www.severin.xyz/categories/%E6%A1%86%E6%9E%B6/spring/"}],"tags":[{"name":"框架","slug":"框架","permalink":"https://www.severin.xyz/tags/%E6%A1%86%E6%9E%B6/"},{"name":"spring","slug":"spring","permalink":"https://www.severin.xyz/tags/spring/"},{"name":"原理","slug":"原理","permalink":"https://www.severin.xyz/tags/%E5%8E%9F%E7%90%86/"}]},{"title":"Linux五种IO模型与Java三种IO模型","slug":"Linux五种IO模型与Java三种IO模型","date":"2020-01-09T16:00:00.000Z","updated":"2020-02-22T03:20:52.658Z","comments":true,"path":"2020/01/10/Linux五种IO模型与Java三种IO模型/","link":"","permalink":"https://www.severin.xyz/2020/01/10/Linux%E4%BA%94%E7%A7%8DIO%E6%A8%A1%E5%9E%8B%E4%B8%8EJava%E4%B8%89%E7%A7%8DIO%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"1. 一次IO操作的分析所谓的IO操作是指对除了寄存器和内存以外的设备进行读写请求，如文件、网络、键盘等，站在用户的角度来看就是调用一次IO系统调用。对于一次读操作具体过程如下： 用户进程发起IO系统调用从用户态进入内核态完成上下文切换，内核中存在一个内核缓冲区，读或写时都是将数据从内核缓冲区拷贝到进程缓冲区中 内核不会直接使用CPU去操作数据，而是发送操作指令给DMA，DMA完成数据的读写后会通过总线发送一个系统中断给内核。 数据从外存拷贝到内核缓冲区后，再由内核将内核缓冲区中的数据拷贝到进程缓冲区中，之后切换到用户态，完成系统调用，这样用户进程就得到了数据 2. Linux的五种IO模型Linux有五种IO模型，分别是同步阻塞、同步非阻塞、IO多路复用、信号驱动、异步IO模型 同步阻塞 发出IO系统调用后，内核会通知DMA准备数据，这个过程有两个特点，第一是不占用CPU，第二是需要一定的时间，同步阻塞就是指发出IO系统调用后，用户进程会进入阻塞状态等待内核缓存区数据准备好后，直到数据从内核缓冲区复制到应用进程缓冲区中才返回 同步非阻塞 同步阻塞的问题在于只要内核没有准备好数据，那么就会进入阻塞状态而不同完成其他操作，同步非阻塞则是采用询问的方式不断的从用户态进入内核态询问数据是否准备好。其有点是不需要阻塞，但是缺点是需要多次系统调用询问。 IO多路复用 同步阻塞和同步非阻塞每次从用户态进入内核态都只能得到一次IO的情况，如果有多个IO操作，那么就需要使用多个线程才能同时完成。IO多路复用模型是在一个线程中处理多个IO请求的模型。如select/poll/epoll等函数就是IO多路复用函数。可以向IO多路复用函数中注册多个文件描述符，然后交给内核去监控每个文件描述符是否有对应的IO事件产生，如果有则从内核态回到用户态，然后用户态进程会找到有IO事件产生的文件描述符然后进行处理。 信号驱动模型 应用进程使用系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。内核在数据到达时向应用进程发送 信号，应用进程收到之后在信号处理程序中将数据从内核复制到应用进程中。相比于非阻塞式 I/O 的轮询方式，减少了轮询过程中系统调用的开销，信号驱动 I/O 的 CPU 利用率更高。 异步IO模型 应用进程执行系统调用会立即返回，应用进程可以继续执行，不会被阻塞，内核会在所有操作完成之后向应用进程发送信号。异步 I/O 与信号驱动 I/O 的区别在于，异步 I/O 的信号是在内核数据拷贝到进程缓冲区后通知应用进程 I/O 完成，而信号驱动 I/O 的信号是通知应用进程内核数据准备好了。 3. IO多路复用linux中主要由三种io多路复用函数，分别是select，poll和epoll。按照时间顺序，select是出现的最早的IO多路复用函数，其次是poll，最后是epoll。同样后出现的性能要比先出现的好。 select select的函数原型有五个参数，第一个参数是最大的文件描述符加1，接下来三个参数都是fd_set类型，分别表示需要被监控的读、写和异常事件的文件描述符集合，最后一个参数是超时时间，超时时间有两个成员一个是代表超时时间为秒一个为微秒。fd_set的底层是一个长度为1024位的bitmap，每一位表示一个文件描述符的状态，所以select最多支持监控1024个文件描述符。每次调用select会把文件描述符集合拷贝到内核，然后遍历文件描述符集合为每一个文件描述符申请一个等待队列元素，然后将其添加到对应驱动程序的等待队列中，等待条件满足时唤醒。唤醒后再次遍历所有的文件描述符得到一个掩码，根据这个页码设置每一个文件描述符的状态，最后将结果拷贝到用户空间。用户进程等select函数返回后重新遍历所有的文件描述符，判断文件描述符是否在fd_set中，如果是则需要进行下一步操作。 select的缺点是只支持1024个文件描述符，另外select比较慢，优点是跨平台，简单。 select慢的原因 第一次调用select，内核需要遍历所有的文件描述符并把进程挂在所监听的文件描述符一次 有事件到来时，不知道是哪些文件描述符有数据可以读写，需要把所有的文件描述符都轮询一遍才能知道 通知用户进程还需要把整个bitmap复制到用户空间，由用户进程再次遍历查询出可读写的文件描述符 poll poll与select的区别在于不使用bitmap来表示文件描述符，因此理论上poll监控的文件描述符是没有限制的。poll函数要求传入一个pollfd数组，数组元素包含三个成员，分别是文件描述符，需要监控的事件，实际发生的事件。系统调用会将该数组拷贝到内核空间，然后遍历每一个文件描述符对应的驱动程序的poll函数，申请一个等待队列元素，将其添加到驱动程序的等待队列中，然后睡眠，直到驱动程序将其唤醒，再遍历每一个文件描述符对应的驱动程序的poll函数，得到一个掩码，再将结果拷贝回用户空间。 poll与select的区别 poll不限制监控的文件描述符数量 poll与select描述文件描述符的方式不同，select使用掩码的方式，更节约内存，poll使用数组 epoll epoll是select/poll的增强版，使用epoll时首先需要创建一个 eventpoll 对象，这个对象的底层使用红黑树实现，然后把需要监控的对象插入到 eventpoll对象中，然后用户进程调用 epoll_wait函数，这个函数会把当前进程添加到eventpoll对象的等待队列中。当被监控的对象有事件发生时，中断程序会将该对象的引用放到 eventpoll对象的就绪列表中，然后 eventpoll对象的等待队列中的进程会被唤醒，唤醒后就可以直接从就绪队列中拿到发生事件的对象所对应的文件描述符。 为什么epoll高效 epoll不需要每次都把文件描述符集合从用户空间复制到内核空间 epoll不需要向select/poll那样主动的轮询每一个文件描述符，而是为文件描述符指定回调函数，当事件触发时会将对应的fd加入就绪队列然后唤醒等待的进程 epoll返回后用户进程可以从就绪列表中获取所有就绪的事件而不需要向select和poll那样重新遍历一遍 4. java中的三种IO模型 BIO BIO对应的是同步阻塞IO模型 NIO NIO对应的是IO多路复用模型，Selector就是对底层IO多路复用函数的封装，底层实现是epoll。 AIO AIO对应的是异步IO模型","categories":[{"name":"java进阶","slug":"java进阶","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/"},{"name":"java io","slug":"java进阶/java-io","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/java-io/"}],"tags":[{"name":"java","slug":"java","permalink":"https://www.severin.xyz/tags/java/"},{"name":"io","slug":"io","permalink":"https://www.severin.xyz/tags/io/"}]},{"title":"MySQL探究之InnoDB存储行格式","slug":"MySQL探究之InnoDB存储行格式","date":"2020-01-09T16:00:00.000Z","updated":"2020-02-20T13:13:43.027Z","comments":true,"path":"2020/01/10/MySQL探究之InnoDB存储行格式/","link":"","permalink":"https://www.severin.xyz/2020/01/10/MySQL%E6%8E%A2%E7%A9%B6%E4%B9%8BInnoDB%E5%AD%98%E5%82%A8%E8%A1%8C%E6%A0%BC%E5%BC%8F/","excerpt":"","text":"1. 介绍MySQL关于存储的任务是交给存储引擎来做的，存储引擎最终把数据存放才磁盘中，为了方便管理和理解，存储引擎定义了不同的概念，以InnoDB存储引擎为例。InnnoDB每一个表对应一个文件，称为表空间，表分成若干个段，一个段又包含若干个的区，一个区包含若干个页，InnoDB的页的默认大小是16k，每次将数据从磁盘读取到内存都是以页为最小单位，然后页中存放着许多行，行按照一定的格式存放。 2. InnoDB存储引擎行格式InnoDB存储引擎支持的行格式有四种：Compact、Redundant、Dynamic、Compress 3. CompactCompact的结构如下所示 变长长度列表 MySQL支持一些变长数据类型，比如varchar，这种数据类型占用的内存空间是不定的，所以需要额外的信息来说明变长数据实际占了多少字节。 变长长度列表采用逆序的方式存放了行记录中变长数据的长度。注意如果该表没有变长字段，这个列表是不存在的。 对于char类型的数据，如果采用变长编码则会加入到变长长度列表，否则不会。 NULL值列表 Compact的中文意思是紧凑的，正如其名字一样，这种格式的特点就是尽量少占用内存空间。而对于某种数据类型，如果这个字段为null，那么其实没有必要为其开辟内存空间，只需要记下来这个位置为null就可以了。 对于一张表，InnoDB会把所有的可能为null的字段映射到一个bit位中，如果该位为1说明这个字段为null。注意NULL值列表也是逆序的并且占用整数倍字节。 记录头信息 记录头信息包含的信息很多，主要是用来描述当前行的状态的，比如是否被删除了，上一条记录的位置，下一条记录在位置，是否B+树索引的叶子节点等信息。 列信息 保存真实的数据，每一个字段的数据按顺序排放，但是会添加三个隐藏列。分别是行号、事务id、回滚指针等 对行溢出的处理 假设某一行的数据比整个页的最大容量还大就会发生溢出，对于溢出的处理如下： 对于溢出数据，当前行只会存储数据的一小部分，其他数据放在其他页上，然后用一个20字节的指针指向溢出的数据。 4. Redundantredundant的中文意思是紧凑的 字段长度偏移列表 字段长度偏移列表逆序存放了每一个字段的开始的位置，这样就可以在真实的列数据中找到每一行数据 记录头信息 类似于Compact格式 列信息 保存每一列的数据，注意null值也会占用全部的内存。对于char(M)类型，占用字符集最大长度乘以M 5. Dynamic与Compact格式类似，只是在处理溢出数据时会把所有的溢出数据存在其他页，当前行保存溢出页的地址 6. Compress与Dynamic类似，不同点在于会对数据进行压缩从而节省空间 7. 实践设置和修改表的行格式语句如下： CREATE TABLE 表名 (列的信息) ROW_FORMAT=行格式名称 ALTER TABLE 表名 ROW_FORMAT=行格式名称","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"数据库/mysql","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"https://www.severin.xyz/tags/mysql/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://www.severin.xyz/tags/InnoDB/"}]},{"title":"MySQL探究之数据目录","slug":"MySQL探究之数据目录","date":"2020-01-09T16:00:00.000Z","updated":"2020-02-21T01:38:28.270Z","comments":true,"path":"2020/01/10/MySQL探究之数据目录/","link":"","permalink":"https://www.severin.xyz/2020/01/10/MySQL%E6%8E%A2%E7%A9%B6%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95/","excerpt":"","text":"1. 数据库与文件系统的关系基于磁盘的数据库都是将数据存储在磁盘的文件系统时，当需要读取数据时从文件系统中读出数据返回给用户，写入数据时，存储引擎会将数据写入磁盘。 2. MySQL数据目录查看数据目录 show variables like &#39;datadir&#39; MySQL所有的数据都存放在这个目录中 数据目录的结构 对于每一个database都会在数据目录中创建一个同名的子目录 表在文件系统中的表示 每个表由表结构的定义和表中的数据组成，会在数据库对应的目录中生成.frm文件 InnoDB如何存储表数据的 对于InnoDB来说以页为基本单位来管理存储空间，默认大小为16kb。InnoDB将页存放在表空间中，表空间有两种一种是系统表空间，一种是独立表空间。 系统表空间 系统表空间对应的文件是数据库中的ibdata1，在MySQL5.6之前表中的所有数据都会被默认存储到这个系统表空间中 独立表空间 在MySQL5.6之后，InnoDB会为每一个表建立一个独立的表空间，扩展名为表名.ibd，独立表空间存放表的数据和索引 MyISAM如何存储表数据 每个表对应三个文件表名.frm、表名.MYD、表名.MYI。 表名.frm表示表结构文件。表名.MYD表示数据文件；表名.MYI表示索引文件 视图在文件系统中的表示 视图是虚拟的表不需要存放数据，只需要把他的结构存储起来就行了，和表一样，描述视图结构的文件也会被存储到所属数据库对应的子目录下边，只会存储一个视图名.frm的文件。 3. MySQL系统数据库 mysql 这个数据库贼核心，它存储了MySQL的用户账户和权限信息，一些存储过程、事件的定义信息，一些运行过程中产生的日志信息，一些帮助信息以及时区信息等。 information_schema 这个数据库保存着MySQL服务器维护的所有其他数据库的信息，比如有哪些表、哪些视图、哪些触发器、哪些列、哪些索引吧啦吧啦。这些信息并不是真实的用户数据，而是一些描述性信息，有时候也称之为元数据。 performance_schema 这个数据库里主要保存MySQL服务器运行过程中的一些状态信息，算是对MySQL服务器的一个性能监控。包括统计最近执行了哪些语句，在执行过程的每个阶段都花费了多长时间，内存的使用情况等等信息。 sys 这个数据库主要是通过视图的形式把information_schema和performance_schema结合起来，让程序员可以更方便的了解MySQL服务器的一些性能信息。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"数据库/mysql","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"https://www.severin.xyz/tags/mysql/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://www.severin.xyz/tags/InnoDB/"}]},{"title":"MySQL探究之InnoDB存储页格式","slug":"MySQL探究之InnoDB存储页格式","date":"2020-01-09T16:00:00.000Z","updated":"2020-02-20T13:52:56.321Z","comments":true,"path":"2020/01/10/MySQL探究之InnoDB存储页格式/","link":"","permalink":"https://www.severin.xyz/2020/01/10/MySQL%E6%8E%A2%E7%A9%B6%E4%B9%8BInnoDB%E5%AD%98%E5%82%A8%E9%A1%B5%E6%A0%BC%E5%BC%8F/","excerpt":"","text":"1. 页格式简介页是InnoDB从磁盘中存取的最小单位，默认大小为16kb，可以用来存放数据、索引、日志等信息。数据页的格式如下： File Header 存放页的一些通用信息 Page Header 存放数据页专有的信息 Infimum和supremum 这是两个虚拟的行记录 User Records 用户行记录 Free Space 空闲空间 Page Directory 页目录 File Tailer 用来校验页是否完整 2. 页如何管理记录向页中插入一条记录，会从Free Space中申请一定的空间存放记录，插入的记录就位于User Records。页中的多条记录按照主键的大小（一定会有一个主键，如果不存在则由第一个唯一索引字段当做主键，否则自动生成row_id充当主键）排序，使用记录头中的信息描述每一条记录的下一条记录的位置，也就是说页中的记录会构成一个链表。而页中的两个虚拟行记录Infimum和supremum则相当于头结点和尾节点。如图所示： 3. 如何在页内查找记录存储引擎存放数据处理要考虑内存占用问题还要定义查找方法，如何查才能查找的更快呢？假设已经知道待查找的记录在某一页，并且这一页已经读取到内存中，如何进行查找呢？由于记录组织成一个链表结构，那么查找的时间复杂度是O(N)，显然这是不能接受的，因此页内定义了一个叫做页目录的东西，解决页内查找的方案如下： 页中的记录是按照主键大小顺序存放的，但是每条记录占用的内存空间不是连续的所以不能使用二分查找来加速查找。但是可以借助一种中间结果来实现二分查找，这就是页目录。页目录由不同的连续的槽构成，将记录链表分成不同的连续的组。将每个组的最后一条记录的地址偏移量单独提取出来按顺序存储到页目录作为槽。这样在查找的过程中首先在页目录中进行二分查找找出对应的组，然后在组里顺序查找到记录。 4. 页面头部主要存放页中已经存储了多少条记录，第一条记录的地址是什么，页目录中存储了多少个槽等信息 5. File HeaderFile Header针对各种类型的页都通用，也就是说不同类型的页都会以File Header作为第一个组成部分，它描述了一些针对各种页都通用的一些信息，比方说这个页的编号是多少，它的上一个页、下一个页是谁 6. File Trailer当页在内存中被修改就成为了脏页，可以通过该部分来判断，File Trailer主要包括两部分，一部分是校验和，通过校验和就可以知道页是否被修改过，还有一部分是最后修改时对应的日志序列位置（LSN）","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"数据库/mysql","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"https://www.severin.xyz/tags/mysql/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://www.severin.xyz/tags/InnoDB/"}]},{"title":"MySQL探究之字符集","slug":"MySQL探究之字符集","date":"2020-01-09T16:00:00.000Z","updated":"2020-02-20T11:46:41.781Z","comments":true,"path":"2020/01/10/MySQL探究之字符集/","link":"","permalink":"https://www.severin.xyz/2020/01/10/MySQL%E6%8E%A2%E7%A9%B6%E4%B9%8B%E5%AD%97%E7%AC%A6%E9%9B%86/","excerpt":"","text":"MySQL探究之字符集1. 字符集在计算机中数据都是以二进制形式进行存储的，字符集就是一个存储的规则，比如存储每个字母或者汉字需要存储一个怎样的二进制串，这个二进制串是唯一的。有了这个规则，我们在查看某个文本时就可以按照规则转换出来正确的符号表示。字符集的原理如下。 MySQL支持的字符集 show charset 执行结果如下： 可以看到mysql支持许多字符集。下面是对常用字符集的介绍: ascii 每个字符占用一个字节，最高位不使用，所以最多支持2^7=128个字符，包括字母符号（键盘上的每个键） gbk 支持汉字，兼容ascii，当表示ascii时占用一个字节，否则两个字节 utf8 兼容ascii，采用边长编码。长度为1到4个字节。utf8是Unicode字符集的一种，Unicode包含了世界上所有的字符，分为utf8、utf16、utf32。utf8使用1到4个字节编码，utf16使用2到4字节，utf32使用4字节。 在mysql中为了节约内存utf8分为两个版本utf8mb3和utf8mb4。其中utf8mb3是阉割版的utf8，最长只占用3字节。utf8mb4才是真正的utf8，但是mysql中的utf8是指最大长度为3字节的utf8mb3。如果要存储emoji表情，可以使用utf8mb4。 2. 字符集比较规则字符有很多种，如数字，汉字，英文字母，符号等。给定一堆字符串，如何进行排序，应该按照什么样的规则排序呢？所谓的排序规则就是按照这个规则可以确定一系列字符串的顺序。对于向数字，我们可以规定小的排在前面，大的排在后面，字母可以按照字典序排列，但是有的如汉字，其他国家的文字如何指定一个排序规则呢？不同的字符集有不同的排序规则，有的按照大小的逻辑关系，有的直接按照字符对应的二进制位的大小排序，有的不区分大小写，有的区分大小写。 查看MySQL支持的字符集比较规则 show collation 比较规则的命名是有规律的 比较规则名称以与其关联的字符集的名称开头。如上图的查询结果的⽐较规则名称都是以utf8开头的 后边紧跟着该比较规则主要作用于哪种语言，比如 utf8_polish_ci表示以波兰语的规则比较 utf8_spanish_ci是以西班牙语的规则比较 utf8_general_ci是⼀种通⽤的⽐较规则。 名称后缀意味着该比较规则是否区分语言中的重音、大小写啥的，具体可以用的值如下： 后缀 英文释义 描述 _ai accent insensitive 不区分重音 _as accent sensitive 区分重音 _ci case insensitive 不区分大小写 _cs case sensitive 区分大小写 _bin binary 以二进制方式比较 3. MySQL各级别的字符集和比较规则mysql有四种级别的字符集和比较规则，分别是： 服务器级别 数据库级别 表级别 列级别 服务器级别 SHOW VARIABLES LIKE &#39;character_set_server&#39;; SHOW VARIABLES LIKE &#39;collation_server&#39; 如果创建数据库没有指定字符集合比较规则默认使用服务器级别 数据库级别 SHOW VARIABLES LIKE &#39;character_set_database&#39;; SHOW VARIABLES LIKE &#39;collation_database&#39;; 如果创建表的时候没有指定字符集和比较规则则默认使用数据库级别 表级别 设置表级别字符集和比较规则 CREATE TABLE 表名 (列的信息) [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称]]如果字段不指定字符集合比较规则，默认与表级别一样 列级别 CREATE TABLE 表名( 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称], 其他列... ); ALTER TABLE 表名 MODIFY 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称];修改规则 由于字符集和比较规则是互相有联系的，如果只修改字符集，比较规则会跟着变化，如果只修改了比较规则，字符集也会跟着变化，具体规则如下： 只修改字符集，则比较规则将变为修改后的字符集默认的比较规则。 只修改比较规则，则字符集将变为修改后的比较规则对应的字符集。 4. MySQL乱码问题从客户端发送sql语句通过网络传给服务器到服务器返回结果给客户端展出出来，涉及到不同的编码转换，如果说编码转换出错了，则可能出现乱码问题。MySQL服务器与编码转换有关的系统变量如下。 系统变量 描述 character_set_client 服务器解码请求时使用的字符集 character_set_connection 服务器处理请求时会把请求字符串从character_set_client转为character_set_connection character_set_results 服务器向客户端返回数据时使用的字符集 服务器认为客户端发送过来的数据是以character_set_client编码，于是会按照相同的方式解码，然后处理请求的时候会把他转换为character_set_connection编码，返回信息时会编码成character_set_result。也就是说假如客户端的编码与character_set_client和character_set_results不兼容的话就会发生乱码问题。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"数据库/mysql","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"https://www.severin.xyz/tags/mysql/"}]},{"title":"MySQL探究之InnoDB索引","slug":"MySQL探究之索引","date":"2020-01-09T16:00:00.000Z","updated":"2020-02-21T01:18:24.389Z","comments":true,"path":"2020/01/10/MySQL探究之索引/","link":"","permalink":"https://www.severin.xyz/2020/01/10/MySQL%E6%8E%A2%E7%A9%B6%E4%B9%8B%E7%B4%A2%E5%BC%95/","excerpt":"","text":"1. 什么是索引索引是一种用于快速查找的有序的数据结构 2. InnoDB中的索引方案InnoDB使用B+树作为索引，B+树是一棵多路的搜索树，在MySQL这棵树一般不会超过4层。由于InnoDB读写磁盘的最小单位是页，所以使用页来作为B+树的节点，也就是搜索时最多只需要访问4次IO就可以找到对应的数据页。用户记录只存在底层的叶子节点中，数据页按顺序使用链表连接起来，然后非叶子节点时一些目录项，这些目录项只包含主键和下一层所在的页号，根据这个页号就可以找到下一级节点，目录项按照主键顺序连接在一起，并使用页目录辅助进行二分查找，所以在页内查找的速度很快。 3. MyISAM中的索引方案MyISAM存储引擎建立一张表后会生成三个文件.frm、.MYI和.MYD。其中.MYD存放用户记录，.MYI存放的是索引，也就是数据和索引是分开存放的，索引中存放的是主键和行号，当进行索引查询时首先在索引中找到主键对应的行号，然后根据行号去数据文件中进行查找（类似于回表操作） 4. 索引分类聚簇索引 B+树的叶子节点存储了完整的用户记录，即索引和数据一起存放，在查找聚簇索引时可以直接在这棵B+树索引中找到用户记录，所以一般查询很快。InnoDB存储引擎会为主键自动创建聚簇索引，并且一张表只有一个聚簇索引。 辅助索引 辅助索引的叶子节点中的记录按照建索引的那个字段按序存放，但是不存储所有的用户记录，只存储建索引的那个字段和主键，在查找辅助索引时找到了待查找的字段和主键，需要使用主键去聚簇索引中继续查找，这个过程称为回表。 联合索引 按照多个列的顺序为多个列同时建立的索引称为联合索引。 5. 索引带来的额外开销索引虽然可以极大的提高查询效率，但是却会带来其他的开销，所以不能随便建立索引，开销主要包含以下两个方面： 空间 B+树的每个节点就是一个页，所以会带来额外的空间开销。 时间 对表中数据进行增、删、改操作时会破坏B+树的性质，所以需要调整。如果建立很多索引，每个B+树都得进行维护。 6. 使用索引的条件 全值匹配 如果搜索条件与索引列一致则可以利用索引进行查找，这就是全值匹配 匹配左边的列 搜索条件中有多个列，但是只有左边连续的列与索引列一样，那么也可以利用到索引，因为对多个列建联合索引的比较条件是从左边的列开始比较的 匹配列前缀 如果索引列是字符串类型，搜索条件是一个包含前缀的模糊查询可以使用到索引。因为对字符串排序是按照字典序排列的，从字符串的左边到右边依次比较的。 匹配范围值 因为索引中的每一条记录都是按照顺序使用链表连接起来的，所以只要找到两个边界就可以快速查找出边界内所有的记录 精确匹配某一列并范围匹配另外一列 用于排序 索引本身是一个有序的结构，所以可以利用索引进行排序，如果没有索引就需要在内存中重新排序，开销非常大 用于分组 所谓的分组就是按照某个列，将该列一样的记录当做一组，所以可以利用索引有序的性质，因为有序，所以列相当的记录相邻。 覆盖索引 对于辅助索引需要进行回表的操作，如果回表的次数太多了，会带来可见的代价，为了减小这个代价可以使用覆盖索引来避免回表。覆盖索引就是查询的列包含在索引中，这样就只需要找到索引的叶子结点就可以找出查询的列而不需要去聚簇索引中找到所有的列。 7. 如何建立索引 只为用于搜索、排序、分组的列建立索引 需要考虑列的区分度，如果某个字段所有行记录中的值基本都一样，比如说性别只有男女两种，那么建立索引就没有意义 索引列的类型尽量小，这样比较的时候开销会小，查询的速度就更快 对于很长的字符串，如果对整个字符串建立索引，那么索引占用的空间会非常大，所以一般只对长字符串的前缀建立索引 索引列不能进行任何计算 对于主键应该使主键按照顺序进行自增，这样插入时可以顺序插入 索引有额外的时间和空间代价，所以不要建立冗余的索引","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"数据库/mysql","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"https://www.severin.xyz/tags/mysql/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://www.severin.xyz/tags/InnoDB/"}]},{"title":"MySQL探究之单表访问","slug":"MySQL探究之单表访问","date":"2020-01-09T16:00:00.000Z","updated":"2020-02-21T12:18:52.975Z","comments":true,"path":"2020/01/10/MySQL探究之单表访问/","link":"","permalink":"https://www.severin.xyz/2020/01/10/MySQL%E6%8E%A2%E7%A9%B6%E4%B9%8B%E5%8D%95%E8%A1%A8%E8%AE%BF%E9%97%AE/","excerpt":"","text":"1. 访问方法在MySQL服务器中有一个查询优化器模块，一条查询语句进行语法解析后会交给查询优化器来优化，优化的结构就是生成一个所谓的执行计划，这个执行计划表明了应该使用那些所以进行查询，表之间的连接顺序是等。本文要记录的是怎么执行单表查询。对于单表查询大致可以分成三种方法： 使用全表扫描进行查询 使用索引进行查询 索引查询有可以分为对索引进行等值查询、范围查询，对聚簇索引查询，对辅助索引查询等 常用的访问方法有const、ref、ref_or_null、range、index、all等 首先建立如下表: CREATE TABLE single_table ( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), UNIQUE KEY idx_key2 (key2), KEY idx_key3 (key3), KEY idx_key_part(key_part1, key_part2, key_part3) ) Engine=InnoDB CHARSET=utf8; 在这张表中索引情况如下： 为id列建立的聚簇索引。 为key1列建立的idx_key1二级索引。 为key2列建立的idx_key2二级索引，而且该索引是唯一二级索引。 为key3列建立的idx_key3二级索引。 为key_part1、key_part2、key_part3列建立的idx_key_part二级索引，这也是一个联合索引。 2. const对主键或者唯一索引进行等值查询。 对主键进行等值查询 SELECT * FROM single_table WHERE id = 1438; 对唯一索引进行等值查询 SELECT * FROM single_table WHERE key2 = 3841; const访问方法，只需要访问一次聚簇索引，或者访问一次辅助索引并进行一次回表操作，所以查询速度非常快。 3. ref对普通的辅助索引进行等值查询 SELECT * FROM single_table WHERE key1 = &#39;abc&#39;; 查询的搜索条件中的字段不是唯一的所以会得到多条结果，并且每个结果都需要进行回表操作，所以速度比const慢 4. ref_or_null查询某个辅助索引列的值等于某个常数的记录，还想把该列的值为NULL的记录也找出来 SELECT * FROM single_table WHERE key1 = &#39;abc&#39; OR key1 IS NULL; 5. range对索引列进行范围查找 SELECT * FROM single_table WHERE key2 IN (1438, 6328) OR (key2 &gt;= 38 AND key2 &lt;= 79); 6. index查询条件没有用到任何索引，但是查询列建有索引列，所以可以在辅助中进行全表扫描 SELECT key_part1, key_part2, key_part3 FROM single_table WHERE key_part2 = &#39;abc&#39;; 由于辅助索引不存放全部的记录，所以辅助索引要比聚簇索引小的多，因此即使全表扫描也比全表扫描聚簇索引速度要快。 7. all对聚簇索引进行全表扫描，速度最慢。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"数据库/mysql","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"https://www.severin.xyz/tags/mysql/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://www.severin.xyz/tags/InnoDB/"}]},{"title":"MySQL探究之连接的原理","slug":"MySQL探究之连接的原理","date":"2020-01-09T16:00:00.000Z","updated":"2020-02-21T12:49:49.088Z","comments":true,"path":"2020/01/10/MySQL探究之连接的原理/","link":"","permalink":"https://www.severin.xyz/2020/01/10/MySQL%E6%8E%A2%E7%A9%B6%E4%B9%8B%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%8E%9F%E7%90%86/","excerpt":"","text":"1. 连接简介涉及到多表查询最常用的操作是连接操作，在MySQL中连接分成两大类分别是内连接和外连接，外连接由分为左外连接和右外连接，简称左连接和右连接。 连接的本质就是生成笛卡尔集然后从中筛选出符合条件的记录。 2. 连接的原理驱动表与被驱动表 两个表左连接查询有一张表称为驱动表，另一张表是被驱动表，假设tableA是驱动表，tableB是被驱动表。连接操作如下图所示： 由于需要多次访问被驱动表，所以可以考虑给被驱动表建立索引加快查找速度 join buffer join buffer是执行连接查询前申请的一块固定大小的内存，先把若干条驱动表结果集中的记录装在这个join buffer中，然后开始扫描被驱动表，每一条被驱动表的记录一次性和join buffer中的多条驱动表记录做匹配，因为匹配的过程都是在内存中完成的，所以这样可以显著减少被驱动表的I/O代价","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"数据库/mysql","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"https://www.severin.xyz/tags/mysql/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://www.severin.xyz/tags/InnoDB/"}]},{"title":"Netty启动过程分析","slug":"Netty启动过程分析","date":"2020-01-09T16:00:00.000Z","updated":"2020-02-23T06:39:39.606Z","comments":true,"path":"2020/01/10/Netty启动过程分析/","link":"","permalink":"https://www.severin.xyz/2020/01/10/Netty%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90/","excerpt":"","text":"1. Netty的工作模式Netty基于主从Reactor模式实现，它的基本结构如下图所示: Neety的工作方式基本如上图所示： 首先Netty基于主从Reactor模式，有两个EventLoopGroup，一般称为BossGroup和WorkerGroup。其中BossGroup充当主Reactor的角色。EventLoopGroup包含多个EventLoop，每个EventLoop代表一个线程，EventLoop内部会运行select这样的IO多路复用函数用来监听事件。BossGroup和WorkerGroup的职责不同，BossGroup负责监听连接事件，当连接事件到达后会将事件注册到WorkerGroup中，而WorkerGroup监听读写事件，当监听到读写事件发送后会将这个事件交给Pipeline去处理业务。 本小节主要探究的是Netty如何启动EventLoopGroup去监听事件，监听到事件后如何触发pipeline去处理业务方法。 2. BossGroup是怎么启动的面对BossGroup，我们已经知道了其功能与作用是绑定端口监听连接事件，并且把连接的channel的读写事件注册到workerGroup中，为了验证这个过程，我们有如下几个问题需要解决： EventLoopGroup每次启动如何创建线程的，创建几个？ EventLoopGroup有多个EventLoop，如何选择新连接绑定到哪个EventLoop呢？ BossGroup是如何绑定端口的 BossGroup是如何处理连接的 EventLoopGroup每次启动如何创建线程的，创建几个 代码跟踪过程如下图所示： protected MultithreadEventLoopGroup(int nThreads, Executor executor, Object... args) { super(nThreads == 0 ? DEFAULT_EVENT_LOOP_THREADS : nThreads, executor, args); } // DEFAULT_EVENT_LOOP_THREADS的计算如下 static { DEFAULT_EVENT_LOOP_THREADS = Math.max(1, SystemPropertyUtil.getInt( &quot;io.netty.eventLoopThreads&quot;, NettyRuntime.availableProcessors() * 2)); if (logger.isDebugEnabled()) { logger.debug(&quot;-Dio.netty.eventLoopThreads: {}&quot;, DEFAULT_EVENT_LOOP_THREADS); } } 从这个构造函数可以看出来，nThread构造参数代表最终创建的线程数量，默认是CPU核心数*2 protected MultithreadEventExecutorGroup (int nThreads, Executor executor, EventExecutorChooserFactory chooserFactory, Object... args) { // 前面的参数校验的代码省略 // 重点代码 if (executor == null) { // 创建线程执行器，并且包含一个创建线程的工厂，重点代码 executor = new ThreadPerTaskExecutor(newDefaultThreadFactory()); } children = new EventExecutor[nThreads]; for (int i = 0; i &lt; nThreads; i ++) { children[i] = newChild(executor, args); // 异常处理的代码省略 } } 这段代码是NioEventLoopGroup的父类的父类的构造方法，这个方法中使用ThreadPerTaskExecutor创建了nThread个线程实体。具体看一下newChild方法的作用 protected EventLoop newChild(Executor executor, Object... args) throws Exception { EventLoopTaskQueueFactory queueFactory = args.length == 4 ? (EventLoopTaskQueueFactory) args[3] : null; // 创建一个NioEventLoop return new NioEventLoop(this, executor, (SelectorProvider) args[0], ((SelectStrategyFactory) args[1]).newSelectStrategy(), (RejectedExecutionHandler) args[2], queueFactory); } 可以看到NioEventLoopGroup调用newChild方法创建了nThread个NioEventLoop，而创建NioEventLoop时把executor给传进去了，因此下一步要分析executor有什么用，NioEventLoop怎么利用executor。 public final class ThreadPerTaskExecutor implements Executor { private final ThreadFactory threadFactory; public ThreadPerTaskExecutor(ThreadFactory threadFactory) { // 传入一个线程工厂，线程工厂负责创建线程 this.threadFactory = ObjectUtil.checkNotNull(threadFactory, &quot;threadFactory&quot;); } // 每调用一次execute都会用线程工厂去创建一个线程执行传过来的任务 @Override public void execute(Runnable command) { threadFactory.newThread(command).start(); } } 再看一下NioEventLoop的构造方法 NioEventLoop(NioEventLoopGroup parent, Executor executor, SelectorProvider selectorProvider, SelectStrategy strategy, RejectedExecutionHandler rejectedExecutionHandler, EventLoopTaskQueueFactory queueFactory) { // executor super(parent, executor, false, newTaskQueue(queueFactory), newTaskQueue(queueFactory), rejectedExecutionHandler); this.provider = ObjectUtil.checkNotNull(selectorProvider, &quot;selectorProvider&quot;); this.selectStrategy = ObjectUtil.checkNotNull(strategy, &quot;selectStrategy&quot;); final SelectorTuple selectorTuple = openSelector(); // 保存selector,这个selector就是nio中的selector this.selector = selectorTuple.selector; this.unwrappedSelector = selectorTuple.unwrappedSelector; } protected SingleThreadEventExecutor(EventExecutorGroup parent, Executor executor, boolean addTaskWakesUp, Queue&lt;Runnable&gt; taskQueue, RejectedExecutionHandler rejectedHandler) { super(parent); this.addTaskWakesUp = addTaskWakesUp; this.maxPendingTasks = DEFAULT_MAX_PENDING_EXECUTOR_TASKS; this.executor = ThreadExecutorMap.apply(executor, this);// 保存executor this.taskQueue = ObjectUtil.checkNotNull(taskQueue, &quot;taskQueue&quot;);// 保存任务队列 this.rejectedExecutionHandler = ObjectUtil.checkNotNull(rejectedHandler, &quot;rejectedHandler&quot;); } 此时到目前为止只是成功创建了EventLoopGroup和其中的多个EventLoop，但是通过debug发现并没有创建线程，目前EventLoop实际上已经有创建线程的能力了。通过debug发现真正创建线程得到绑定端口时才会创建。 总结： NioEventLoopGroup的构造参数决定了NioEvnetLoopGroup包含几个NioEvnetLoop，默认是CPU核心数*2。然后NioEventLoop包含了Nio中的selector实现IO多路复用以及一个线程执行器用来创建线程执行任务，还有一个任务队列。 如何选择新连接绑定到哪个EventLoop protected MultithreadEventExecutorGroup(int nThreads, Executor executor, EventExecutorChooserFactory chooserFactory, Object... args) { // 前面创建EventLoop的方代码省略 // 这里创建的是一个选择器，用来选择一个EventLoop绑定新连接 chooser = chooserFactory.newChooser(children); // 后面代码省略 } public EventExecutor next() { // EventLoopGroup调用选择方法，封装了选择器的next方法 return chooser.next(); } public EventExecutor next() { // 从第一个开始每次选择下一个，知道最后一个后又回到第一个，这是children的长度是2的幂的情况 return executors[idx.getAndIncrement() &amp; executors.length - 1]; } public EventExecutor next() { // 选择的方法和前面一样，只是计算方法不一样，读children的数量没有要求 return executors[idx.getAndIncrement() &amp; executors.length - 1]; } 总结： 创建选择器时会根据chilren的个数来创建不同的选择器，选择器的next方法的结果是一样的，每次选择下一个然后回到第一个。只不过如果个数是2的幂，可以进行一定的优化提高选择速度。 BossGroup是如何绑定端口的 首先应该知道BossGroup监控的是ServerSocketChannel，ServerSocketChannel需要和一个EventLoop进行绑定才能够开启一个监控的任务，所以第一步是要找到ServerSocketChannel与EventLoop绑定的过程，前面分析了有一个组件是选择器，它的作用就是选择一个指定的EventLoop返回，ServerSocketChannel与EventLoop绑定的过程肯定要调用选择器的next方法，绑定EventLoop的过程暂时不描述，只要记住ServerSocketChannel一定已经和一个EventLoop关联了，然后看下面的代码。 private static void doBind0( final ChannelFuture regFuture, final Channel channel, final SocketAddress localAddress, final ChannelPromise promise) { // 向EventLoop提交任务 channel.eventLoop().execute(new Runnable() { @Override public void run() { if (regFuture.isSuccess()) { // 这是一个绑定端口的方法 channel.bind(localAddress, promise). addListener(ChannelFutureListener.CLOSE_ON_FAILURE); } else { promise.setFailure(regFuture.cause()); } } }); } 上面可以看到channel的EventLoop执行了一个绑定端口的方法，而我们的BossGroup的作用之一就是监听端口等待连接，所以这个bind方法最后应该能够追踪到Nio中的bind方法。 最终端口绑定的方法如下： protected void doBind(SocketAddress localAddress) throws Exception { if (PlatformDependent.javaVersion() &gt;= 7) { javaChannel().bind(localAddress, config.getBacklog()); } else { javaChannel().socket().bind(localAddress, config.getBacklog()); } } BossGroup如何处理新的连接 从bind方法进入 void init(Channel channel) { p.addLast(new ChannelInitializer&lt;Channel&gt;() { @Override public void initChannel(final Channel ch) { final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) { pipeline.addLast(handler); } ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); } }); } }); } channel使用pipeline组件来处理业务逻辑，当ServerSocketChannel产生连接事件后需要将这个事件交给pipline去处理，因此上面的代码中实际上为pipline添加了一个业务处理类ServerBootstrapAcceptor。下面是ServerBootstrapAcceptor的channelRead方法用来处理连接。 public void channelRead(ChannelHandlerContext ctx, Object msg) { final Channel child = (Channel) msg; child.pipeline().addLast(childHandler); setChannelOptions(child, childOptions, logger); setAttributes(child, childAttrs); try { // 将channel注册到childGroup中 childGroup.register(child).addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { if (!future.isSuccess()) { forceClose(child, future.cause()); } } }); } catch (Throwable t) { forceClose(child, t); } } 可以看到上面这个方法首先将客户端对应的channel与对应的pipeline业务关联，然后将其注册到childGroup中，也就是workerGroup。 3. EventLoop如何监听事件 @Override protected void run() { int selectCnt = 0; // 这里是一段无限for循环 for (;;) { try { int strategy; try { strategy = selectStrategy.calculateStrategy(selectNowSupplier, hasTasks()); switch (strategy) { case SelectStrategy.CONTINUE: continue; case SelectStrategy.BUSY_WAIT: case SelectStrategy.SELECT: // 内部有一个优先队列用来存放用户自定义的定时任务，这一行代码的目的是判断一下 // 最近需要执行的定时任务的时间 long curDeadlineNanos = nextScheduledTaskDeadlineNanos(); // 如果没有用户自定义任务就赋给NONE if (curDeadlineNanos == -1L) { curDeadlineNanos = NONE; } nextWakeupNanos.set(curDeadlineNanos); try { // 如果任务队列中没有任务需要去执行才执行select方法 if (!hasTasks()) { // 1. 调用select方法 strategy = select(curDeadlineNanos); } } finally { nextWakeupNanos.lazySet(AWAKE); } default: } } catch (IOException e) { // 这里是重建selector，为了解决jdk空轮询bug rebuildSelector0(); selectCnt = 0; handleLoopException(e); continue; } selectCnt++; cancelledKeys = 0; needsToSelectAgain = false; final int ioRatio = this.ioRatio; boolean ranTasks; if (ioRatio == 100) { try { if (strategy &gt; 0) { // 处理发送的事件 processSelectedKeys(); } } finally { // 处理队列中的任务 ranTasks = runAllTasks(); } } else if (strategy &gt; 0) { final long ioStartTime = System.nanoTime(); try { // 处理发送的事件 processSelectedKeys(); } finally { // 处理队列中的任务 final long ioTime = System.nanoTime() - ioStartTime; ranTasks = runAllTasks(ioTime * (100 - ioRatio) / ioRatio); } } else { ranTasks = runAllTasks(0); } } } 上面代码基本上已经说明了监听事件的过程，首先调用select方法，返回后处理产生的事件然后处理队列中的任务。 private int select(long deadlineNanos) throws IOException { // 进入if的条件是没有自定义的定时任务并且任务队列为空（执行select方法前就判断了） // 所以此时可以放心的调用阻塞的select方法 if (deadlineNanos == NONE) { return selector.select(); } // 这一步的作用是如果还有5微秒就有一个延时任务需要处理，那么就调用非阻塞的select // 否则调用阻塞的select long timeoutMillis = deadlineToDelayNanos(deadlineNanos + 995000L) / 1000000L; return timeoutMillis &lt;= 0 ? selector.selectNow() : selector.select(timeoutMillis); } 4. 总结通过前面的代码，大致了解了EventLoopGroup是如何创建的，如何监听事件的以及如何处理用户自定义任务的。下面是总结。 以NioEventLoopGroup为例，首先NioEventLoopGroup的构造函数可以设置每个NioEventLoopGroup可以创建多少个NioEventLoop，默认是当前可用的CPU核心数*2。 知道创建多少个NioEventLoop后需要知道怎么创建NioEventLoop，在NioEventLoopGroup中会首先构造出一个线程创建执行器，这个执行器里面包含了一个创建线程的线程工厂。 然后开始创建NioEventLoop，构造方法会将selector，线程创建执行器以及一个任务队列保存早NioEventLoop的，此时NioEventLoop和NioEventLoopGroup就准备好了。 然后服务端的启动需要一个ServerSocketChannel对象，这个对象在调用bootstrap的bind方法时会通过反射创建并且将其与Pipeline和一个NioEventLoop绑定并最后绑定端口号完成端口监听。NioEventLoop负责监控这个channel的事件，PipeLine包含处理新连接的业务方法，具体过程是将客户端对应的channel与客户端对应的pipeline绑定，然后注册到workerGroup中，由workerGroup去监听所有的客户端。 NioEventLoopGroup中的NioEventLoop内部会执行一个无限for循环，这个for循环中会去不断的执行select方法，然后处理发生的事件。 由于NioEventLoop内部包含了一个任务队列和一个定时任务队列用来存放用户的任务和定时任务，所以在for循环中除了要执行select方法外还要在循环的过程中去处理任务，具体为，只有当任务队列为空采取执行select，并且如果有一个定时任务快要发生了，会去执行一个非阻塞的select方法，否则执行一个阻塞的select方法直到定时任务发生，最后处理完连接后回去处理定时任务和任务队列。","categories":[{"name":"java进阶","slug":"java进阶","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/"},{"name":"java io","slug":"java进阶/java-io","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/java-io/"}],"tags":[{"name":"java","slug":"java","permalink":"https://www.severin.xyz/tags/java/"},{"name":"io","slug":"io","permalink":"https://www.severin.xyz/tags/io/"},{"name":"netty","slug":"netty","permalink":"https://www.severin.xyz/tags/netty/"}]},{"title":"java io与零拷贝分析","slug":"java IO与零拷贝分析","date":"2020-01-09T16:00:00.000Z","updated":"2020-02-22T05:08:46.135Z","comments":true,"path":"2020/01/10/java IO与零拷贝分析/","link":"","permalink":"https://www.severin.xyz/2020/01/10/java%20IO%E4%B8%8E%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%88%86%E6%9E%90/","excerpt":"","text":"1. 一次BIO分析本节分析的例子是向网络中发送一条消息，代码如下 public class Demo { public static void main(String[] args) throws IOException { } // 客户端代码 public void client() throws IOException { Socket socket=new Socket(&quot;127.0.0.1&quot;,8088); OutputStream outputStream = socket.getOutputStream(); byte[] buffer=&quot;hello&quot;.getBytes(); outputStream.write(buffer); outputStream.close(); socket.close(); } // 服务端代码 public void server() throws IOException { ServerSocket serverSocket=new ServerSocket(); serverSocket.bind(new InetSocketAddress(8088)); while (true){ Socket client = serverSocket.accept(); new Thread(()-&gt;{ try { InputStream inputStream = client.getInputStream(); byte[] buffer=new byte[1024]; int len = inputStream.read(buffer); System.out.println(new String(buffer,len)); } catch (IOException e) { e.printStackTrace(); } }).start(); } } } 上面这个代码的流程大致如下： 客户端先准备一个缓冲区，里面的内容是待发送的数据，然后调用输出流的write方法将数据发送到网络中 服务端准备一个缓冲区来接收数据，调用输入流的read方法将接收的数据读到缓冲区中 如果更深入的分析，整个过程如图所示 发送者通过系统调用完成用户态到内核态的切换，然后将数据从用户空间缓冲区拷贝到内核缓冲区，然后通过CPU将内核缓冲区的数据复制到socket缓冲区，然后CPU通知DMA将数据从socket缓冲区通过网卡发送储区，发送完成后DMA产生中断，用户进程从内核态切换到用户态。 接收者通过系统调用完成用户态到内核态的切换，CPU会通知DMA完成数据从网卡读取到socket缓冲区，完成后将数据从socket缓冲区拷贝到内核缓冲区，然后数据从内核缓冲区复制到用户缓冲区，然后用户进程从内核态切换到用户态。 问题 可以明确的一点数据在发送和接受的过程中没有经过任何处理，但是却实现了四次从用户缓冲区向内核缓冲区的拷贝以及四次上下文切换，如果数据量很大，这个拷贝的过程肯定会极大的影响性能。有没有一种方法可以降低拷贝的次数呢？ 2. 零拷贝技术 零复制（英语：Zero-copy；也译零拷贝）技术是指计算机执行操作时，CPU不需要先将数据从某处内存复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。 sendfile sendfile函数在两个文件描述符之间传递数据（完全在内核中操作），从而避免了内核缓冲区和用户缓冲区之间的数据拷贝，效率很高。 mmap mmap技术通过内存映射将文件映射到内核缓冲区，用户空间可以共享内核空间的数据，这样在进行网络传输的时，就可以减少内核空间到用户空间的拷贝次数。 sendfile和mmap的对比 mmap 适合小数据量读写，sendFile 适合大文件传输 mmap 需要 4 次上下文切换，3 次数据拷贝；sendFile 需要 3 次上下文切换，最少 2 次数据拷贝。 sendFile 可以利用 DMA 方式，减少 CPU 拷贝，mmap 则不能（必须从内核拷贝到 Socket 缓冲区） 3. java中的零拷贝java的nio支持mmap和sendfile","categories":[{"name":"java进阶","slug":"java进阶","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/"},{"name":"java io","slug":"java进阶/java-io","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/java-io/"}],"tags":[{"name":"java","slug":"java","permalink":"https://www.severin.xyz/tags/java/"},{"name":"io","slug":"io","permalink":"https://www.severin.xyz/tags/io/"}]},{"title":"TCP超时重传","slug":"TCP超时重传","date":"2019-12-30T16:00:00.000Z","updated":"2020-02-02T11:43:37.687Z","comments":true,"path":"2019/12/31/TCP超时重传/","link":"","permalink":"https://www.severin.xyz/2019/12/31/TCP%E8%B6%85%E6%97%B6%E9%87%8D%E4%BC%A0/","excerpt":"","text":"1. 简单的超时与重传举例2. 设置重传超时TCP超时重传的基础是怎样根据给定的RTT设置RTO，如果RTO小于RTT，会导致不必要的重传，反之如果RTO远大于RTT，会导致网络利用率下降。RTT的测量非常复杂，TCP收到数据确认信息，该信息包含携带一个字节的数据来测量传输该确认信息所需时间，每个此类的测量结果称为一个RTT样本，TCP首先要根据一段时间的RTT样本值来建立好估计值。然后基于估计值来设置RTO。 经典方法 最初的TCP规范采用如下公式计算得到平滑的RTT估计值 先计算平滑RTT估计值（称为SRTT） SRTT=αSRTT+(1-α)RTT α称为平滑因子，推荐取值为0.8到0.9 采用如下公式设置RTO RTO=min(ubound,max(lbound，(SRTT)β)ubound为RTO上界，Ibound为RTO下界，β为时延离散因子，取值为1.3到2.0 对于比较稳定的RTT分布，经典方法可以取得不错的效果，但是如果RTT变化较大，则无法获取期望的结果。 标准方法 标准方法是基于平均值和方差来估计的，但是方差计算复杂，不适合快速TCP的实现，因此使用计算更加简单快速的平均偏差来代替方差。因此需要计算平均值和平均偏差。 计算srtt srtt=(1-g)(srtt)+g(M) 计算rttvar rttvar=(1-h)(rttvar)+(h)(|M-strr|) 计算RTO RTO=srtt+4(rttvar) 初始值 根据RFC6298规定RTO初始值为1s，而初始SYN报文段采用的超时间隔为3s。当接收到首个RTT测量结果为M，采用下面的公式初始化。 srtt=M rttvar=M/2 重传二义性和Karn算法 假设一个包的传输出现了问题，回进行重传，然后收到确认，但是这个确认是对第一次还是第二次传输的确认存在二义性。 Karn算法的第一部分：接收到重传数据的确认信息，不更新RTT估计值 Karn算法的第二部分：每当重传计时器出现超时，使用二进制退避系数更新超时时间直到不发生重传 带时间戳选项的RTT测量 TCP时间戳选项可以用于RTT的测量。 发送方在发送报文段时把当前TCP时钟值放入时间戳字段，接收方在确认该报文段时把时间戳字段值复制到时间戳回送回答字段。因此，发送方在收到确认报文后，利用当前TCP时钟减去时间戳就可以准确计算出RTT。 Linux采用的方法 3. 基于计时器的重传一旦TCP发送端得到了基于时间变化的RTT测量值，就能据此设置RTO，发送报文段时确保重传计时器设置合理。在设定计时前，需记录被计时的报文段序列号，若及时收到了该报文段的ACK，那么计时器被取消。之后发送一个新的数据包，需要设定一个新的计时器，并记录新的序列号。若在连接设定的RTO内，没有收到计时报文段的ACK，将会触发超时重传，发生超时重传会通过降低当前数据发送速率来对此进行快速响应。有两种实现方式：第一种方式是基于拥塞控制机制减小发送窗口大小；另一种方法为每当一个重传报文段被再次重传时，则增加RTO的退避因子，当一个报文段出现多次重传时，RTO的值等于RTO乘以一个系数，这个系数呈二进制指数退避变化。 4. 快速重传快速重传基于接收端的反馈信息来引发重传，而非重传计时器的超时。 与超时重传相比快速重传能更加有效的修复丢包情况。 当接收端收到失序数据时，应该立即返回重复ACK，不能延时发送。 快速重传算法可以概述为： TCP发送端在观测到至少dupthresh个重复ACK后，即重传可能丢失的数据分组，而不必等到重传计时器超时。快速重传算法也可以触发拥塞控制。 5. 带选择确认的重传接收端收到失序的报文段后，不使用选择确认选项会返回一个ACK对按顺序收到的最后一个报文段进行确认，失序的报文段会丢失，发送端需要重传所有失序的报文段，因此会出现重复发送多个报文段的现象，开启选择确认选项后接收放可以在ACK中告诉发送端失序数据的哪些部分需要填补，这样就避免了发送重复的报文。 每个SACK信息包含32位的序列号，代表接收端存储的失序数据的起始至最后一个序列号。每个ACK可以包含三个SACK信息，即可以向发送端报告三个空缺。 SACK接收端行为 TCP连接建立期间如果运行SACK选项，接收端缓存中存在失序数据，就可以生成SACK，并且SACK信息按照接受的先后依次排列。 SACK发送端行为 发送端提供SACK功能并合理地利用接收到的SACK块来进行丢失重传，该过程称为选择性重传。发送方选择重传不会清楚缓存中的数据。只有收到普通的ACK时才可清除。选择重传不会启动超时重传计时器，即使接收端仍有失序数据，那么接收端还会发送SACK信息。 6. 伪超时与重传某些情况下，即使没有出现数据丢失也可能引发重传。这种不必要的重传称为伪重传，其主要造成的原因是伪超时。 重复SACK（DSACK） SACK只能解决报文段失序时告知发送端，但是不能够解决接收端收到重复数据的问题。DSACK可以在第一个SACK块中告知接收端收到的重复报文段序列号，DSACK的主要目的是判断何时的重传是不必要的。DSACK接收端的的变化在于运行包含序列号小于累积ACK字段的SACK块，DSACK只包含在单个ACK中，该ACK称为DSACK，与通常的SACK信息不同，DSACK信息不会在多个SACK中重复，因此DSACK较普通的SACK鲁棒性低。 Eifel检测算法 Eifel检测算法利用时间戳选项来检测伪重传，在发现超时重传后，Eifel算法等待接收下一个ACK，若下一个ACK是对原始传输的确认，则判定该重传为伪重传。 Eifel检测算法的机制：当发生一个重传后，保存其TSV值，当接收到相应分组的ACK后，检查该ACK的TSER部分，如果TSER值小于当前TSV值，则可以判定该ACK对应的是原始传输分组，即该重传是伪重传。 前移RTO恢复 前移RTO恢复时检测伪重传的标准算法，不需要任何TCP选项实现，但只能检测基于重传计时器超时引发的伪重传。 在超时重传后收到第一个ACK时，TCP会发送新数据，之后再响应后一个达到的ACK。如果其中有一个是重复ACK，则认为此次重传没问题。如果都不是重复ACK，则表示重传是伪重传。 Eifel响应算法 旦判断出现伪重传，则会引发一套标准操作，即Eifel响应算法。 在重传计时器超时后，它会查看srtt和rttval的值，并且按照如下方式记录新的变量srtt_prev和rttvar_prev srtt_prev=srtt+2(G) rttvar_prev=rttvar 在判断伪超时后会使用这些值来恢复srtt和rttvar 7. 包失序与包重复 失序 在IP网络中出现失序的原因在于IP层不能保证传输是有序进行的，这样做的好处是IP可以选择不同的传输链路来传输数据。失序问题可能发生在正向或反向链路（ACK报文段失序）中。 如果失序发生在反方向，可能会使得TCP发送端窗口快速前移，接着又可能收到一些显然重复而应被丢弃的ACK，出于TCP的拥塞控制行为，会导致发送端出现不必要的流量突发（每收到一个ACK，拥塞窗口会增加，由于窗口前移了，所以窗口内可以发送的数据突然变多） 如果失序发生在正方向，TCP可能无法正确识别丢包和失序，导致伪重传，为了解决这个问题，快速重传仅在达到重复阈值后才会被触发。互联网中严重的失序不常见，所有重复阈值设置的相对较小，默认为3。 重复 包的重复会使得接收端生成一系列重复的ACK，从而触发伪快速重传。利用SACK特别是DSACK可以忽略这个问题。 8. 目的度量TCP能不断学习发送端与接收端之间的链路特征。学习的结果显示为发送端记录一些状态变量，如srtt和rttvar，假如连接关闭，这些状态信息会丢失。而较新的TCP实现维护了这些度量值，即使在连接断开后，也能利用保存的之前的信息作为初始值。 9. 重新组包当TCP超时重传，并不需要完全重传相同的报文段。TCP允许执行重新组包，发送一个更大的报文段来提高性能。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"传输层","slug":"计算机网络/传输层","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E4%BC%A0%E8%BE%93%E5%B1%82/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"传输层","slug":"传输层","permalink":"https://www.severin.xyz/tags/%E4%BC%A0%E8%BE%93%E5%B1%82/"},{"name":"超时重传","slug":"超时重传","permalink":"https://www.severin.xyz/tags/%E8%B6%85%E6%97%B6%E9%87%8D%E4%BC%A0/"}]},{"title":"TCP拥塞控制","slug":"TCP拥塞控制","date":"2019-12-29T16:00:00.000Z","updated":"2020-02-03T06:17:37.643Z","comments":true,"path":"2019/12/30/TCP拥塞控制/","link":"","permalink":"https://www.severin.xyz/2019/12/30/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/","excerpt":"","text":"1. TCP拥塞检测 什么是拥塞 路由器因无法处理高速率到达的流量而被迫丢弃数据信息的现象称为拥塞。 什么是TCP的拥塞控制 防止网络因为大规模的通信负载而瘫痪，在网络即将进入拥塞状态时减缓TCP传输。 怎么判断是否发送了拥塞 通常如果TCP发生了丢包，TCP首先采取的机制的重传，包括超时重传和快速重传，但是当网络处于拥塞崩溃状态时，TCP发送的包很可能会丢掉，如果选择重传数据包会加剧网络的拥塞状态。 判断网络拥塞状态是否已发生通常看丢包情况。其他的探测方法有，时延测量和显式拥塞通知。 如何减缓TCP发送 TCP使用滑动窗口机制发送和接收数据，发送速率的控制时通过调节发送窗口大小实现的，但是发送窗口大小只取决于接收方接收数据的能力。而拥塞控制需要考虑网络的拥塞状况，因此引入了一个拥塞窗口，TCP发送窗口要取通知窗口(awnd) 和拥塞窗口(cwnd)两者的最小值。 2. TCP拥塞控制的一些经典算法 慢启动 什么时候使用慢启动算法 当一个新的TCP连接建立或检测到由重传超时导致的丢包时，需要执行慢启动算法。 为什么要使用慢启动算法 在传输初始阶段，由于未知网络传输能力，需要缓慢探测可用传输资源，防止短时间内大量数据注入导致拥塞。慢启动算法正是针对这一问题设计的。在数据传输之处或者重传计时器检测到丢包后，需要执行慢启动。 慢启动算法的流程 以一定数目的数据段开始慢启动，称为初始窗口（Init Window,IW）。初始窗口的值设为一个SMSS（发送方的最大段大小），但在其他的RFC文档中可以设置较大的值，如果2倍SMSS或3倍SMSS或者4SMSS 每当发送一个数据包并且收到了ACK，发送窗口会增加1SMSS，也就是经过一个传输轮次，窗口的大小会翻倍。 发送窗口不能无限制增长下去，慢启动算法执行过程中有一个慢启动阈值，cwnd达到这个阈值后就会进入拥塞避免算法 拥塞避免 使用拥塞避免算法，cwnd的值呈线性增长。 慢启动和拥塞避免的选择 设置一个慢启动阈值，当cwnd的值小于阈值则使用慢启动算法，cwnd的值大于阈值则使用拥塞避免算法，cwnd等于阈值时既可以使用慢启动也可以使用拥塞避免。 注意慢启动阈值不是一个定值，他是动态变化的，当出现重传情况，TCP会认为操作窗口超过的网络传输能力范围，这是慢启动阈值会变成当前窗口大小的一半（慢启动阈值并不一定减小，也有可能增大） Tahoe、Reno以及快速恢复算法 Tahoe 当发生超时或者快速重传，都会重新进入慢启动状态 Reno（标准TCP的基础） 使用快速恢复机制：在恢复阶段，每收到一个重复ACK，cwnd就能临时增长1SMSS，相应地就意味着收到一个ACK就能发送一个新的数据包，拥塞窗口在一段时间内会急速增长，知道接受到一个不重复的ACK，TCP结束恢复阶段，拥塞恢复到之前的状态。 标准TCP TCP连接建立之初首先是慢启动阶段，慢启动阈值通常取一个较大值。当接收到一个好的ACK（表示新的数据传输成功），cwnd会进行相应的更新 cwnd+=SMSS(慢启动)（这个公式的意思时每收到一个ack，增加一个SMSS,与一次往返时延RTT增加一倍并不矛盾） cwnd+=SMSS*SMSS/cwnd(拥塞避免) 当收到三次重复ACK（表明丢包需要快速重传的信号） 更新慢启动阈值(ssthresh)为大于等式（ssthresh=max(在外数据值/2,2*SMSS)）不同的操作系统实现的TCP版本这个值是不一样的 启用快速重传算法，将cwnd设为ssthresh+3SMSS 之后每接受到一个重复的ACK，cwnd展示增加1SMSS 当接受到一个好的ACK，将cwnd重设为ssthresh 第二步和第三步就是快速恢复 3. 对标准算法的改进针对标准的拥塞控制算法也提出了一些新的算法 NewReno NewReno算法是为了解决什么问题、 快速恢复带来一个问题，当一个传输窗口出现多个数据包丢失时，一旦一个包重传成功，发送就收到了一个好的ACK，这样快速恢复阶段中cwnd窗口的暂时膨胀就会停止，而事实上丢失的其他数据包可能还没有重传完，导致出现这种情况的ACK称为局部ACK。 Reno算法在接收到局部ACK后就停止拥塞窗口的膨胀阶段，并将其减小至特定值，这种做法可能导致在重传计时器超时之前，传输阶段一直处于空闲状态。（这一段话可以这样理解，发送窗口的数据都发送出去了，但是都没有收到确认，假如说此时窗口缩小，那么发送方明明有数据但是无法发送导致发送通道空闲） NewReno算法是如何改进的 NewReno算法对快速恢复做了改进，它记录了上一个数据传输窗口的最高序列号（恢复点），仅当接收到序列号不小于恢复点的ACK，才停止快速恢复阶段。这样TCP发送方每接收一个ACK都能够继续发送一个新数据包，从而减少重传超时的发生。 采用选择确认机制的TCP拥塞控制 采用SACK机制的TCP，发送方可以知道多个数据段的丢失情况，而这些数据都在发送窗口，因此可以即时发送，然而，这样可能可能会在较短时间内向网络中注入大量数据，削弱拥塞控制的效果。解决方法是TCP会维护一个称为管道的变量，记录在外数据的估计值，不考虑awnd，只有当cwnd-pipe&gt;=SMSS时，SACK TCP可以发送数据。 转发确认和速率减半 当快速重传结束后cwnd值减小，tcp发送新数据之前至少要接收1半的ACK数据包。这样TCP发送端在前一半RTT时间内处于等待状态，在后一半RTT才能开始发送新数据。 转发确认策略提出了带界定参数的速率减半算法（RHBP） RHBP的基本操作是，在一个RTT时间内，每重复收到两个重复ACK，TCP发送方可以发送一个新数据包，这样在恢复阶段结束前，TCP已经发送了一部分新数据，数据发送比较均衡，不会集中到RTT的后半段。 限制传输 在Reno算法中，通常需要三次重复ACK表明数据丢失，在窗口较小的情况下，当出现丢包，网络中可能没有足够的包去引发快速重传/快速恢复机制。 采用限制传输策略，TCP发送方没接收两个连续的重复ACK，就能发送一个新数据包，这样使得网络中的数据包维持一定数量足以触发快速重传。 限制传输使TCP能在可用窗口较小的情况下更好的工作。避免TCP等待RTO导致吞吐量下降。 拥塞窗口校验（CWV） 当发送方持续发送数据，使得cwnd增加成一个较大的值，然后暂停发送数据，之后cwnd就不能反映网络中的拥塞状态。 CWV机制是在发送长时间暂停的情况下，cwnd值会衰减。 空闲发送端和应用受限发送端： 空闲发送端是指没有新数据发送需求，之前发送的数据都已经收到确认了 应用受限发送端是指有需要发送的数据，但是由于某些原因无法发送（如处理器正忙） 长时间空闲 首先判断距离上次发送操作是否超过一个RTO，如果超过则： 更新ssthresh值，设为max(ssthresh,3/4cwnd) 每经过一个RTT，cwnd的值减半 应用受限的发送端 已使用窗口记为W_used 更新ssthresh值，设为max(ssthresh,3/4cwnd) cwnd设为cwnd与W_used的平均值 4. 伪RTO处理 Eifel响应算法解决的问题 若TCP出现突发时延，即使没有丢包，也有可能造成重传超时，这就是伪重传现象，伪重传现象发生TCP会调整ssthresh并将cwnd置为IW，使得tcp进入慢开启状态，浪费传输资源。 Eifel算法包含检测算法和响应算法两部分。 Eifel响应算法用于处理重传计时器以及重传计时器超时后的拥塞控制操作。 在首次发生超时重传时，Eifel算法开始执行，若认为出现伪重传情况，会撤销对ssthresh的修改。在任何情况，若因RTO导致需要改变ssthresh值，都在修改前记录一个特殊的变量pipe_prev=min(在外数据值,ssthresh). 然后会运行一个检测算法，如果发生伪重传进行如下操作： 若接受的是包含ECN-Echo标志位的好的ACK，停止操作 cwnd=在外数据值+min(bytes_Acked,IM) ssthresh=pipe_prev 5. 共享拥塞状态信息相同的主机会建立多条TCP连接，每个tcp连接需要重新进行拥塞处理，建立自己的ssthresh和cwnd。但是实际上，新连接可能会用到相同主机之间的其他连接信息，包括已经关闭的连接或者正在处于活动状态的其他连接。 6. TCP友好性在传输路径中会经常出现几个TCP连接共享一个或多个路由的情况，然而他们并非均匀的共享带宽资源，而是根据其他连接动态地调节分配。但是也可能出现TCP与其他TCP连接恶性竞争传输资源的情况，于是提出了一种基于计算公式的速率控制方式，限制特定环境下TCP连接对带宽资源的使用。该方法称为TCP友好速率控制（RFRC）。 7. 高速环境下的TCP在BDP较大的高速网络中，传统TCP可能不能表现出良好的性能。因为它的窗口增加需要很长一段时间才能使窗口增至传输链路饱和。也就是说即使没有拥塞发生，TCP也不能很好地利用高速网络。TCP需要经过非常长的一段时间才能完全利用所有带宽。 8. 基于延迟的拥塞控制算法 Vegas算法 FAST算法 TCP Westwood算法和Westwood+算法 复合TCP 9. 缓冲区膨胀路由器缓存区不是越大越好，过大的缓冲区会导致网络拥塞。 10. 积极队列管理和ECN 积极队列管理 路由器管理队列的方法称为积极队列管理机制（AQM），发生拥塞时，如果尾端没有空闲空间，会将数据包丢弃（尾部丢弃），然后按照FIFO方法转发之前到达的数据包。 RED网关机制 随机早期检测（RED）网关机制能够探测拥塞情况的发送，并且控制数据包标记，这些网关实现了一种衡量平均占用时间的队列管理方法，如果占用队列的时间超过最小值，并且小于最大值，那么这个数据包将被标记上一个不断增长的概率值，如果平均队列占用时间超过了最大值，也可以将数据包丢弃而不是标记它们。 当接收方接受到一个被标记的包，表明这个数据包经过一个阻塞的路由器，于是会在ACK确认报文中通知发送方阻塞情况。 ECN机制 ECN机制主要在IP层操作，也可以应用于TCP协议之外的其他传输层协议。当一个包含ECN功能的路由器经过长时间的拥塞，接收到一个IP数据包后，它会查看IP头中的ECN传输功能标识，如果有效，负责发送数据包的传输层协议将开启ECN功能，此时路由器会在IP头设置一个已发送拥塞（CE）标识，然后继续向下转发数据报。若拥塞情况不会持续很长时间，路由器不会将CE标识置位。 如果TCP接收端发现接收到的数据包的cE标识被置位，那么它必须将该标识发送回发送端（也可以添加到SYN+ACK报文段中发送），它会将ACK数据包的ECN-Echo置位。TCP发送端接收到Echo-Echo标识的ACk数据包时，会像探测到单个数据包丢失一样调整cwnd值。 *参考：《TCP/IP协议详解 卷一：协议》Kevin R. Fall W.Richard Stevens","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"传输层","slug":"计算机网络/传输层","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E4%BC%A0%E8%BE%93%E5%B1%82/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"tcp","slug":"tcp","permalink":"https://www.severin.xyz/tags/tcp/"},{"name":"拥塞控制","slug":"拥塞控制","permalink":"https://www.severin.xyz/tags/%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/"},{"name":"传输层","slug":"传输层","permalink":"https://www.severin.xyz/tags/%E4%BC%A0%E8%BE%93%E5%B1%82/"}]},{"title":"TCP连接管理","slug":"TCP连接管理","date":"2019-12-28T16:00:00.000Z","updated":"2020-02-03T06:02:24.718Z","comments":true,"path":"2019/12/29/TCP连接管理/","link":"","permalink":"https://www.severin.xyz/2019/12/29/TCP%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/","excerpt":"","text":"1. TCP三次握手TCP是一种面向连接的单播协议，在发送数据之前，通信双方必须彼此建立一条连接。一个tcp连接由一个四元组构成（源ip地址，源端口号，目的ip地址，目的端口号） 三次握手流程： 主动连接者（客户端）向服务端发送一个SYN报文(TCP头部的SYN位置位并且随机生成一个seq序列号) 服务端发送自己的SYN报文段作为响应，同时这个报文段中携带了对上一个报文段的ACK确认（ack确认号为上一个报文的的序列号+1） 客户端收到第二个报文后，会对服务端的同步报文段进行确认 2. TCP四次挥手连接释放比连接建立更加复杂，涉及到的知识点也更多 四次挥手过程： 主动关闭者发送过一个FIN报文段给被动关闭者，并且指明当前的序列号（这个报文段好可以携带同一个ack对上一个报文进行确认，这是可选的不是必须的） 连接的被动关闭者序列号加1发送ACK报文段给主动关闭者，此时主动关闭者到被动关闭者这一方向的连接就关闭了，主动关闭者不能发送数据给被动关闭者但是被动关闭者发送给主动关闭者的数据依然要接收。此时处于半关闭状态 被动关闭者发送FIN报文段个主动关闭者 主动关闭者对FIN报文段进行确认，此时连接已经释放，但是主动关闭者还要等一段时间 3. TCP状态转换TCP三次握手状态变化 客户端和服务端一开始都是处于CLOSE关闭状态，服务端需要先监听端口，此时处于Listen状态，具体变化过程如下： 第一次握手（客户端-&gt; 服务端） 客户端发送完之后处于SYN_SENT(同步已发送) 第二次握手（服务端-&gt;客户端） 服务端收到第一次握手报文后立即确认，处于SYN_RESVD(同步已接收) 第三次握手（客户端发送，服务端接收） 客户端收到第二个报文后立即确认，发送完客户端处于ESTABLISHED状态（已建立连接） 服务端收到第三个报文后处于ESTABLISHED状态（已建立连接） TCP四次握手状态变化 第一个握手报文(主动关闭者-&gt; 被动关闭者) 主动关闭者发送完报文后处于FIN-WAIT-1状态 第二个握手报文(被动关闭者-&gt; 主动关闭者) 被动关闭者发送后处于CLOSE_WAIT状态 主动关闭者接收后处于FIN-WAIT-2状态 第三个握手报文(被动关闭者-&gt; 主动关闭者) 被动关闭者发送后处于LAST_ACK状态 第四个握手报文(主动关闭者-&gt; 被动关闭者) 主动关闭者发送后处于TIME-WAIT状态 被动关闭者发送后处于CLOSE状态 TCP状态详细 TCP连接与释放总共涉及到11种状态，上面已经涉及到10种，还有一种状态是CLOSEING，下面是详细的TCP状态转换图。 TIME_WAIT状态 在第三个挥手报文到达主动关闭方时，主动关闭方进入TIME_WAIT状态。TIME_WAIT状态也称2MSL等待状态，在该状态，TCP将会等待两倍于最大段生存期，MSL的值是可以修改的，一般为30秒、1分钟或者2分钟。 TIME_WAIT状态的意义 能够让TCP主动关闭者有机会重新发送ACK以避免丢失的情况 假设第四次挥手ACK报文丢失，那么被动关闭会重新发送FIN报文，如果TCP主动关闭在不处于TIME_WAIT状态而是直接关闭了，那么被动关闭在就收不到ACK报文，会一直重发。 允许老的数据包在网络中消逝。当TCP处于等待状态，通信双方将本次连接定义为不可用， 因为TIME_WAIT状态持续2MSL，就可以保证当成功建立一个TCP连接的时候，来自之前连接的重复分组已经在网络中消逝。 静默时间概念 假设某台主机的某个连接处于TIME_WAIT状态，然后突然主机关机并在MSL内重启，重启后主机并不知道上次的连接处于TIME_WAIT状态，于是使用关机前的ip和端口建立连接就有可能出现问题。 为了解决这个问题，引入了静默时间的概念，静默时间是指主机崩溃或重启后TCP协议应该在创建新的连接前等待MSL，但是实际上很多实现并没有遵循这一点 FIN_WAIT_2状态 TCP主动关闭发送FIN报文并收到ACK后会进入FIN_WAIT_2状态，而被动关闭者进入CLOSE_WAIT状态。假如被动关闭者迟迟不发生FIN报文段，那么主动关闭者会一直处于FIN_WAIT_2状态。 为了防止TCP进入FIN_WAIT_2这一无限等待状态，如果负责主动关闭的进程执行的是一个完全关闭操作而不是半关闭操作，那么就会设置一个计时器，如果计时器超时网络是空闲的，那么TCP连接就会转移到CLOSE状态，在linux中默认是60s。 同时打开和同时关闭的状态转换 同时打开 可以认为是四次握手。 同时关闭 出现了最后一种状态CLOSEING状态，此时不会进入FIN_WAIT_2状态 4. TCP为什么要三次握手建立连接TCP是一种可靠的传输协议，既要实现数据的可靠传输又要兼顾效率，三次握手恰好可以满足这两方面的需求。如果是两次握手可能会出现这样的问题，客户端发送的一个连接请求报文可能在网络中延迟，直到客户端和服务端的连接已经释放后才到达服务端，服务端接受到这个连接请求报文后会进行确认，假如没有第三次握手，服务端就以为连接建立了，而客户端此时并没有发送建立连接的请求，所以不会管收到的确认报文，这种情况下客户端认为没有建立连接，服务端却认为建立了连接等待数据的传送会导致服务端资源的浪费。这种情况出现的问题是，TCP的一端无法确认自己收到的报文段是不是合法的。 为了实现可靠传输，TCP使用序列号来标识每一个报文段，通信的双方首先要同步自己的初始序列号，初始序列号是随机产生的。只有TCP双方知道了对方的初始序列号，才能够判断接收的数据报文是不是本次连接的，所以TCP三次握手整个过程就是以尽可能小的代价同步双方的初始序列号。具体来说第一次握手的过程，发送方会选择一个随机的序列号作为自己的初始序列号发送给接收方，接收方收到后并不知道这个序列号是不是对方此次连接的初始序列号，于是会进行确认并且携带自己的初始序列号，发送方收到之后可以知道接受方的初始确认号，因为这是一个确认报文，所以还可以知道接收方已经收到了它的初始序列号，但是此时接收方还是不能够判断自己收到的初始确认号是不是本次连接的，于是发送方会发送第三次报文给接收方，接收方收到后就能够确定自己收到的初始序列号是本次连接的，这样经过三次握手通信双方都知道了各自的初始序列号，因为初始序列号是随机生成的，在有些操作系统的实现是每4微秒增加1，需要大概4小时的时间才会重复，这个时间远远大于报文段在网络中的最长寿命，因此每一个连接都有独一无二的初始序列号，知道了对方的初始序列号就能判断接收到的是不是一个合法的报文段，从而实现确认和重传等操作，另外初始报文的获取是随机的，黑客很难伪造一个报文段来干扰这次连接。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"传输层","slug":"计算机网络/传输层","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E4%BC%A0%E8%BE%93%E5%B1%82/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"tcp","slug":"tcp","permalink":"https://www.severin.xyz/tags/tcp/"},{"name":"连接管理","slug":"连接管理","permalink":"https://www.severin.xyz/tags/%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/"},{"name":"传输层","slug":"传输层","permalink":"https://www.severin.xyz/tags/%E4%BC%A0%E8%BE%93%E5%B1%82/"}]},{"title":"TCP协议报文","slug":"TCP协议报文","date":"2019-12-28T16:00:00.000Z","updated":"2020-02-02T08:01:21.655Z","comments":true,"path":"2019/12/29/TCP协议报文/","link":"","permalink":"https://www.severin.xyz/2019/12/29/TCP%E5%8D%8F%E8%AE%AE%E6%8A%A5%E6%96%87/","excerpt":"","text":"1.协议报文 2. 协议字段 源端口、目的端口 源端口、目的端口、源IP地址、目的IP地址组成的四元组唯一标识一个连接。其中源IP地址与目的IP地址在IP首部中标识。源端口与目的端口在TCP首部中标识。端口使用16位表示，范围为0到65535。端口分类如下： 系统端口号0-1023 这些端口指派给了TCP/IP最重要的一些应用程序 登记端口号1024-49151 服务器端使用的端口号，这类端口号是为没有熟知端口号的应用程序使用 短暂端口号49152-65535 客户端进程运行时动态选择的短暂端口号 序列号 序列号字段标识了TCP发送端到TCP接收端的数据流的一个字节，该字节代表着包含该序列号的报文段的数据中的第一个字节。 确认序号 确认号用于指明在接收方已经顺序收到的最大字节。 首部长度 指明了头部的长度，共四位，表示的最大值为15，但是单位是4字节，也就是说TCP的首部最大为60字节，因为固定首部长度为20字节，因此选项可使用40字节。 保留位 一个六位，实际上新的实现只有四位，有两位有作用 CWR 发送方降低它的发送速率 ECE 发送方接受到了一个更早的拥塞通告 标志位 URG 紧急选项，置位后紧急指针有效 ACK 表示这是一个ACK确认报文 PSH 接收方应该尽快将这个数据交给应用进程 PST 因为某些错误导致重置连接 SYN 初始化一个连接的同步序列号 FIN 表示这是一个释放连接的报文 窗口大小 TCP流量控制由每个端点使用窗口大小字段来通告一个窗口大小来完成。这个窗口大小是字节数、由ACK号指定，窗口大小为16位限制了窗口大小为65535字节，但是可以通过窗口缩放选项对这个值进行缩放。 校验和 TCP接收端使用校验和来校验接受的数据是否出现了差错。 紧急指针 只有在URG位字段被设置时才有效。这个“指针”是一个必须要加到报文段的序列号字段上的正偏移，以产生紧急数据的最后一个字节的序列号。TCP的紧急机制是一种让发送方给另一端提供特殊标志数据的方法。 选项 数据 数据部分是可选的 3. TCP常见选项 最大段大小选项 最大段大小是指TCP协议所允许的从对方接收到的最大报文段，因此这也是通信对方在发送数据时能够使用的最大报文段。当建立一条TCP连接时，通信的每一方都要在SYN报文段的MSS选项中说明自己允许的最大段大小。最大段大小使用16位表示，默认数值为536字节。因为以太网的MTU为1500字节，为了使得TCP数据报向下传递不分片，减去20字节的ip首部和20字节的tcp首部，MSS一般去1460。 选择确认选项 选择确认选项让发送方能够了解接受方当前的空洞，从而更好的进行重传工作。SACK信息包含在SACK选项中，一个报文段中发送的最大SACK块数目为3。 窗口缩放选项 TCP首部使用16位的窗口大小来控制发送方窗口大小，但是16位最多能表示65535个字节，在一些高速的网络中显然不够。因此提供窗口缩放选项。实际的窗口数值扩大至原先的2s。其中s是比例因子，取值为0到14。所以使用窗口缩放选项窗口的最大值为(216*x214)，这个值接近1GB。 时间戳选项与防绕回序列号 时间戳选项（TSOPT）要求发送方在每一个报文段中添加2个4字节的时间戳数值。 作用： 利用时间戳计算RTT 发送方将发送时的TCP时钟值写入时间戳选项，接收端收到后将这个值复制到ACK确认报文中返回，发送方收到ACK后，利用当前的TCP时钟值减去时间戳选项的值就可以得到RTT。 防绕回序列号 TCP报文序列号只有32位，当超过后就会重新从1开始，在一个高速网络中，一个连接中可能存在相同序列号，使用时间戳选项可以判断相同序列号的报文的发送顺序。 用户超时选项 用户超时数值指明了TCP发送者在确认对方未能成功接受数据之前愿意等待ACK确认的时间，通常通过本地TCP协议配置，用户超时选项可以将用户超时数值告诉连接的对方，让对方可以采取某些操作。 用户超时数值的计算公式: USER_TIMEOUT=min(U_LIMIT,max(ADV_UTO,REMOTE_UTO,L_LIMIT)) ADV_UTO表示本端设置的数值，REMOTE_UTO是远端告知的数值，U_LIMIT是上界，L_LIMIT是下界。 认证选项 TCP认证选项提供各种加密算法。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"传输层","slug":"计算机网络/传输层","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E4%BC%A0%E8%BE%93%E5%B1%82/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"tcp","slug":"tcp","permalink":"https://www.severin.xyz/tags/tcp/"},{"name":"传输层","slug":"传输层","permalink":"https://www.severin.xyz/tags/%E4%BC%A0%E8%BE%93%E5%B1%82/"},{"name":"TCP协议报文","slug":"TCP协议报文","permalink":"https://www.severin.xyz/tags/TCP%E5%8D%8F%E8%AE%AE%E6%8A%A5%E6%96%87/"}]},{"title":"Redis基础知识总结","slug":"Redis基础知识总结","date":"2019-12-27T16:00:00.000Z","updated":"2020-01-30T12:47:07.687Z","comments":true,"path":"2019/12/28/Redis基础知识总结/","link":"","permalink":"https://www.severin.xyz/2019/12/28/Redis%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/","excerpt":"","text":"1. 初识Redishttps://zhuanlan.zhihu.com/p/42272979 1. Redis有哪些特性 首先Redis的速度非常快，并且Redis支持多种数据结构，还提供了许多丰富的功能如提供了键过期的功能，发布订阅功能，支持Lua脚本等功能；Redis简单稳定，源码非常简洁；支持的客户端语言很多；支持持久化，发生断电不会丢失数据；提供了主从复制的功能；提供了集群的功能，适合分布式环境 2. Redis为什么快 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)； 数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的； 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 使用多路I/O复用模型，非阻塞IO； 使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； 3. Redis有哪些应用场景 缓存 redis提供了键值过期时间设置，并且也提供了灵活控制最大内存和内存溢出后的淘汰策略，适合作为缓存。 排行榜系统 Redis提供类表和有序集合数据结构，方便实现各种排行榜 计数器应用 Redis天然支持计数功能且性能非常好。 社交网络 redis可以很方便的实现点赞，共同好友等功能 消息队列系统 Redis提供了发布订阅功能和阻塞队列的功能 4. Redis不适合什么样的应用 使用Redis，基于成本考虑，最大的问题是内存的价格比较昂贵，而Redis将数据全部放在内存中，而基于内存又提供了快速访问的能力，如果一个应用中的数据量比较大并且不需要经常访问就不适合使用Redis来存放这些量大且冷门的数据。 2. Redis的数据结构 3. 小功能大用处1. Redis命令执行的生命周期是怎样的 发送命令-&gt;命令排队-&gt;命令执行-&gt;返回结果 2. 如何统计执行比较慢的命令 通过参数来设置慢查询阈值，执行时间超过这个阈值的命令就会被记录下来，使用slowlog命令就可以获取到慢查询日志。 3. 什么是pipeline机制 在redis中某些操作提供了批量操作的命令，但是许多操作是没有对应的批量操作的命令，所以在执行一批大量的命令时，总的执行包括真正的执行时间以及发送命令和接受响应的往返时间，往返时间取决于网络的状况，为了降低网络时延的影响，pipeline提供了将一组命令组合在一起发送给服务端的能力，这样就只有一次往返时间，提高了命令执行的效率。 4. redis对事务的支持情况 redis只支持简单的事务，这种事务不具备完整的ACID特性，使用事务的时候执行multi开启事务，后面的命令都在同一个事务中，使用exec来提交事务或者使用exec命令来取消事务，如果事务中的操作中有命令语法错误了整个事务将无法执行，但是redis的事务是不支付撤销的，在某些场景下可以使用类似乐观锁的机制来确保事务中的key没有被其他客户端修改过。 5. Lua语言在Redis中扮演的角色 Lua语言是一门由C语言实现的脚本语言，在Redis可以通过lua语言实现自定义的命令。 6. 介绍一下Redis的Bitmaps bitmaps是一种数据结构，叫做位图，他的基本单位是二进制位，每个位只能取0或者1。在Redis中为bitmaps提供了许多位操作的功能，并且bitmap本身在redis中不是一种独立的数据结构，他的底层是字符串，因为bitmaps的基本单元是二进制位，所以数据结构占用的空间很小，适合那些只需要统计两种状态且要求空间很少的应用，比如说统计一亿用户中每天的活跃用户。 7. 介绍一下HyperLogLog HyperlogLog的底层是字符串类型，通过HyperLogLog可以利用极小的内存空间完成独立总数的统计，但是统计值不是完全精确的，具有一定的误差，官方给出的失误率是0.81%。redis中实现的HyperLogLog，只需要12K内存，在标准误差0.81%的前提下，能够统计2^64个数据。但是，因为HyperLogLog只会根据输入元素来计算基数，而不会储存输入元素本身，所以HyperLogLog不能像集合那样，返回输入的各个元素。 8. Redis的发布订阅功能 Redis提供了基于“发布订阅”模式的消息机制，在这种模式下，消息发布者和订阅者不直接通信，发布者客户端向指定的频道发布消息，订阅该冰岛的每个客户端都可以收到该消息。 发布消息 publish channel message 订阅消息 subscribe channel [频道名称] 取消订阅 unsubscribe [频道名称] Redis的消息队列系统比专业的消息队列系统还差很多，但是足够的简单 9. GEO GEO的全称是地理信息(经纬度)定位，支持存储地理位置信息来实现如附近的人，摇一摇这类依赖于地理位置信息的功能。 4. 客户端1. java中如何操作redis jedis 5. 持久化https://juejin.im/post/5d09a9ff51882577eb133aa9 1. 什么是持久化机制 redis的数据是存放在内存中，由于内存的特点是断电后数据丢失，而redis需要保证redis服务器重启后数据还在，所以需要一种机制，这种机制就是持久化机制，他可以把数据持久化到物理磁盘中人，然后重启后从磁盘中读取数据恢复内存中的数据。 2. Redis中有哪些持久化机制，默认是哪种 Redis支持两种持久化机制，RDB和AOF，默认开启的是RDB 3. 介绍一下RDB RDB是一种快照存储持久化方式，具体就是将Redis某一时刻的内存数据保存到硬盘的文件当中，默认保存的文件名为dump.rdb，而在Redis服务器启动时，会重新加载dump.rdb文件的数据到内存当中恢复数据。 4. 怎么开启RDB 可以使用命令save和bgsave,两者的区别是save命令会阻塞Redis服务器直到rdb完成，而bgsave会fork一个子进程来处理不会发生阻塞。也可以通过服务器配置文件来配置触发rdb的条件 5. 有哪些配置触发rdb条件的配置参数 # 900s内至少达到一条写命令 save 900 1 # 300s内至少到达10条写命令 save 300 10 # 60s内至少达到10000条写命令 save 60 10000 4. 生成rdb文件的过程 生成临时rdb文件，并写入数据 完成数据写入，用临时文件替代正式rdb文件 删除原来的rdb文件 5. rdb的优点 与AOF方式想比，通过rdb文件恢复数据比较快 rdb文件非常紧凑，适合数据备份 通过rdb进行数据备份，由于使用了子进程，所以对redis服务器的性能影响比较小 6. rdb的几个缺点 如果服务器宕机的话，采用RDB的方式会造成某个时段内数据的丢失，比如我们设置10分钟同步一次或5分钟达到1000次写入就同步一次，那么如果还没达到触发条件服务器就死机了，那么这个时间段的数据会丢失。 使用save命令会造成服务器阻塞，直接数据同步完成才能接收后续请求。 使用bgsave命令在forks子进程时，如果数据量太大，forks的过程也会发生阻塞，另外，forks子进程会耗费内存。 7. 什么是AOF AOF持久化方式会记录客户端对服务器的每一次写操作命令，并将这些写操作以Redis协议追加保存到以后缀为aof文件末尾，在Redis服务器重启时，会加载并运行aof文件的命令，以达到恢复数据的目的。 8. 如何开启AOF持久化方式 Redis默认不开启AOF持久化方式，我们可以在配置文件中开启并进行更加详细的配置，如下面的redis.conf文件 # 开启aof机制 appendonly yes # aof文件名 appendfilename &quot;appendonly.aof&quot; # 写入策略,always表示每个写操作都保存到aof文件中,也可以是everysec或no appendfsync always # 默认不重写aof文件 no-appendfsync-on-rewrite no # 保存目录 dir ~/redis/ 9. 有哪些AOF的写入策略 通过appendfsync来配置，可以取三个值 always 客户端的每一个写操作都保存到aof文件当，这种策略很安全，但是每个写请注都有IO操作，所以也很慢。 everysec appendfsync的默认写入策略，每秒写入一次aof文件，因此，最多可能会丢失1s的数据。 no Redis服务器不负责写入aof，而是交由操作系统来处理什么时候写入aof文件。更快，但也是最不安全的选择，不推荐使用。 10.AOF文件太大了这么办 可以进行AOF重写，通过重写在aof文件中只保存命令的最小集 11. 重写的作用是什么 aof不进行重写的话会有很多冗余的命令，这会使得aof越来越大，加载aof文件进行恢复时会很慢，重写aof文件的目的就是加快恢复的速度。 12. 有哪些重写方式 第一章是每次同步aof内容时就会发生重写，另一种是只要客户端向服务端发送bgrewriteaof命令的时候才会发生重写。 13. AOF文件损坏了怎么办 在写入aof日志文件时，如果Redis服务器宕机，则aof日志文件文件会出格式错误，在重启Redis服务器时，Redis服务器会拒绝载入这个aof文件，可以通过以下步骤修复aof并恢复数据。 备份现在aof文件，以防万一。 使用redis-check-aof命令修复aof文件，该命令格式redis-check-aof -fix file.aof 14. AOF的优点 AOF只是追加日志文件，因此对服务器性能影响较小，速度比RDB要快，消耗的内存较少。 15. AOF的缺点 AOF方式生成的日志文件太大，即使通过AFO重写，文件体积仍然很大。 恢复数据的速度比RDB慢。 16. AOF和RDB的对比 方式 RDB AOF 启动优先级 低 高 体积 小 大 恢复速度 快 慢 数据安全性 会丢数据 由策略来决定 轻重 重 轻 17. rdb和aof同时开启的情况下，先使用哪种机制 当RDB与AOF两种方式都开启时，Redis会优先使用AOF日志来恢复数据，因为AOF保存的文件比RDB文件更完整。 6. 复制1. redis复制功能的作用 如果使用一台redis服务器来保存数据，当这个服务器不可用的时候，整个服务就会不可用，为了解决这个问题，可以通过提供相同数据的多个副本，这样当一个节点不可用，整个服务并不受影响，后续可以将不可用的服务器恢复。 2. 如何建立复制 参与复制的结点可以分为主节点和从节点，默认情况下redis都是主节点，并且每个从节点只能有一个主节点，一个主节点可以有多个从节点，复制的数据是单向的，只能由主节点复制到从节点，配置的方式有三种 在配置文件中加入slaveof 主节点host，主节点port 在redis服务器启动命令后加–slaveof 主节点host，主节点port redis服务器启动使用命令slaveof 主节点host，主节点port 3. 如何断开复制 在从节点中执行slaveof no one命令，之后这个从节点将晋升为主节点 4. 怎么保证主从复制的安全性 对于数据比较重要的节点，主节点会通过设置requirepass参数进行密码验证，这时所有的客户端访问必须使用auth命令实行校验。 5. 从节点可以进行写操作吗 默认情况下，从节点使用slave-read-only=yes配置为只读模式。由于复制只能从主节点到从节点，对于从节点的任何修改主节点都无法感知，修改从节点会造成主从数据不一致。因此建议线上不要修改从节点的只读模式。 6. 有哪些主从复制的拓扑结构，他们都有哪些特点 一主一从结构是最简单的复制拓扑结构，用于主节点出现宕机时从节点提供故障转移支持。一主多从结构（又称为星形拓扑结构）使得应用端可以利用多个从节点实现读写分离（见图6-5）。对于读占比较大的场景，可以把读命令发送到从节点来分担主节点压力。树状主从结构（又称为树状拓扑结构）使得从节点不但可以复制主节点数据，同时可以作为其他从节点的主节点继续向下层复制。通过引入复制中间层，可以有效降低主节点负载和需要传送给从节点的数据量 7. 主从复制的原理，复制过程是怎么样的 执行slaveof命令，从节点先保存主节点的ip和端口信息，然后从节点会与主节点建立网络连接，连接成功之后从节点会发生ping请求进行首次通信，主节点会返回pong命令。如果主节点设置了权限认证则需要认证权限，认证完之后就可以进行数据集的同步，首次建立连接，主节点会把所有的数据全部发送个从节点，之后如果主节点有新的数据，会将命令持续复制给从节点，保证主从数据的一致性。 8. 如何实现数据同步 通过psync命令来完成主从数据同步，同步过程有两种，全量复制和部分复制 9. 什么是全量复制，什么是部分复制 全量复制一般用于初次复制场景，它会把主节点全部数据一次性发送个从节点。他的缺点是当数据流比较大时，对主从节点和网络会造成很大的开销。部分复制是用来处理主从复制中因网络闪断等原因造成的数据丢失场景，当从节点再次连上主节点后，如果条件允许，主节点会补发丢失数据给从接待您，因为补发的数据远远小于全量数据，可以有效避免全量复制的过高开销。 10. psync命令需要哪些组件的支持 主从节点各自的复制偏移量，主从节点复制积压缓冲区，主节点运行id。 复制偏移量 参与复制的主从节点都会维护自身复制偏移量。主节点（master）在处理完写入命令后，会把命令的字节长度做累加记录，从节点（slave）每秒钟上报自身的复制偏移量给主节点，因此主节点也会保存从节点的复制偏移量，从节点在接收到主节点发送的命令后，也会累加记录自身的偏移量，通过对比主从节点的复制偏移量，可以判断主从节点数据是否一致。判断主从节点复制相差的数据量，根据这个差值判定当前复制的健康度。如果主从之间复制偏移量相差较大，则可能是网络延迟或命令阻塞等原因引起 复制积压缓冲区 复制积压缓冲区是保存在主节点上的一个固定长度的队列，默认大小为1MB，当主节点有连接的从节点（slave）时被创建，这时主节点（master）响应写命令时，不但会把命令发送给从节点，还会写入复制积压缓冲区。由于缓冲区本质上是先进先出的定长队列，所以能实现保存最近已复制数据的功能，用于部分复制和复制命令丢失的数据补救 运行id 每个Redis节点启动后都会动态分配一个40位的十六进制字符串作为运行ID。运行ID的主要作用是用来唯一识别Redis节点，比如从节点保存主节点的运行ID识别自己正在复制的是哪个主节点。 11. fsync命令的流程 从节点（slave）发送psync命令给主节点，参数runId是当前从节点保存的主节点运行ID，如果没有则默认值为，参数offset是当前从节点保存的复制偏移量，如果是第一次参与复制则默认值为-1。 主节点（master）根据psync参数和自身数据情况决定响应结果 如果回复+FULLRESYNC{runId}{offset}，那么从节点将触发全量复制流程 如果回复+CONTINUE，从节点将触发部分复制流程。 如果回复+ERR，说明主节点版本低于Redis2.8，无法识别psync命令 12. 全量复制的流程 发送psync命令进行数据同步，由于是第一次进行复制，从节点没有复制偏移量和主节点的运行ID，所以发送psync-1。 主节点根据psync-1解析出当前为全量复制，回复+FULLRESYNC响应。 从节点接收主节点的响应数据保存运行ID和偏移量offset 主节点执行bgsave保存RDB文件到本地（也可以使用无盘复制不用保存到本地） 主节点发送RDB文件给从节点，从节点把接收的RDB文件保存在本地并直接作为从节点的数据文件 对于从节点开始接收RDB快照到接收完成期间，主节点仍然响应读写命令，因此主节点会把这期间写命令数据保存在复制客户端缓冲区内，当从节点加载完RDB文件后，主节点再把缓冲区内的数据发送给从节点，保证主从之间数据一致性 从节点接收完主节点传送来的全部数据后会清空自身旧数据 从节点清空数据后开始加载RDB文件 从节点成功加载完RDB后，如果当前节点开启了AOF持久化功能，它会立刻做bgrewriteaof操作 13. 部分复制的流程 当主从节点之间网络出现中断时，如果超过repl-timeout时间，主节点会认为从节点故障并中断复制连接 主从连接中断期间主节点依然响应命令，但因复制连接中断命令无法发送给从节点，不过主节点内部存在的复制积压缓冲区，依然可以保存最近一段时间的写命令数据，默认最大缓存1MB 当主从节点网络恢复后，从节点会再次连上主节点 当主从连接恢复后，由于从节点之前保存了自身已复制的偏移量和主节点的运行ID。因此会把它们当作psync参数发送给主节点，要求进行部分复制操作 主节点接到psync命令后首先核对参数runId是否与自身一致，如果一致，说明之前复制的是当前主节点；之后根据参数offset在自身复制积压缓冲区查找，如果偏移量之后的数据存在缓冲区中，则对从节点发送+CONTINUE响应，表示可以进行部分复制。 主节点根据偏移量把复制积压缓冲区里的数据发送给从节点，保证主从复制进入正常状态 10. 主从机制的心跳机制 主从节点彼此都有心跳检测机制，各自模拟成对方的客户端进行通信 主节点默认每隔10秒对从节点发送ping命令，判断从节点的存活性和连接状态。 从节点在主线程中每隔1秒发送replconf ack{offset}命令，给主节点上报自身当前的复制偏移量 主节点根据replconf命令判断从节点超时时间，体现在info replication统计中的lag信息中，lag表示与从节点最后一次通信延迟的秒数，正常延迟应该在0和1之间。如果超过repl-timeout配置的值（默认60秒），则判定从节点下线并断开复制客户端连接。即使主节点判定从节点下线后，如果从节点重新恢复，心跳检测会继续进行 11. 什么是异步复制 主节点不但负责数据读写，还负责把写命令同步给从节点。写命令的发送过程是异步完成，也就是说主节点自身处理完写命令后直接返回给客户端，并不等待从节点复制完成。 12. 什么是读写分离 对于读占比较高的场景，可以通过把一部分读流量分摊到从节点来减轻主节点压力，同时需要注意永远只对主节点执行写操作。 13. 读写分离要注意什么问题 ·复制数据延迟、读到过期数据、从节点故障 复制数据延迟 Redis复制数据的延迟由于异步复制特性是无法避免的，延迟取决于网络带宽和命令阻塞情况，比如刚在主节点写入数据后立刻在从节点上读取可能获取不到。 读到过期数据 当主节点存储大量设置超时的数据时，如缓存数据，Redis内部需要维护过期数据删除策略，删除策略主要有两种：惰性删除和定时删除。 惰性删除 主节点每次处理读取命令时，都会检查键是否超时，如果超时则执行del命令删除键对象，之后del命令也会异步发送给从节点，为了保证复制的一致性，从节点自身永远不会主动删除超时数据。 定时删除 Redis主节点在内部定时任务会循环采样一定数量的键，当发现采样的键过期时执行del命令，之后再同步给从节点。 从节点故障 对于从节点的故障问题，需要在客户端维护可用从节点列表，当从节点故障时立刻切换到其他从节点或主节点上。 14. 什么时候会发生全量复制 全量复制是一个非常消耗资源的操作，发生全量复制的时机有 第一次建立复制 节点运行ID不匹配 复制积压缓冲区不足 15. 什么是复制风暴，如何规避复制风暴 复制风暴是指大量从节点对同一主节点或者对同一台机器的多个主节点短时间内发起全量复制的过程。复制风暴对发起复制的主节点或者机器造成大量开销，导致CPU、内存、带宽消耗。 应该把主节点尽量分散在多台机器上，避免在单台机器上部署过多的主节点。当主节点所在机器故障后提供故障转移机制，避免机器恢复后进行密集的全量复制。 7. Redis的噩梦：阻塞 8. 理解内存 9. 哨兵https://juejin.im/post/5b7d226a6fb9a01a1e01ff64 1. 什么是redis哨兵 Redis的主从复制模式下，一旦主节点由于故障不能提供服务，需要人工将从节点晋升为主节点，同时还要通知应用方更新主节点地址，对于很多应用场景这种故障处理的方式是无法接受的。当主节点出现故障时，Redis Sentinel能自动完成故障发现和故障转移，并通知应用方，从而实现真正的高可用。 2. redis主从复制带来的问题 Redis主从复制模式下，一旦主节点出现了故障不可达，需要人工干预进行故障转移，无论对于Redis的应用方还是运维方都带来了很大的不便。对于应用方来说无法及时感知到主节点的变化，必然会造成一定的写数据丢失和读数据错误，甚至可能造成应用方服务不可用。对于Redis的运维方来说，整个故障转移的过程是需要人工来介的，故障转移实时性和准确性上都无法得到保障。 3. redis哨兵方案 Redis Sentinel是一个分布式架构，其中包含若干个Sentinel节点和Redis数据节点，每个Sentinel节点会对数据节点和其余Sentinel节点进行监控，当它发现节点不可达时，会对节点做下线标识。如果被标识的是主节点，它还会和其他Sentinel节点进行“协商”，当大多数Sentinel节点都认为主节点不可达时，它们会选举出一个Sentinel节点来完成自动故障转移的工作，同时会将这个变化实时通知给Redis应用方。整个过程完全是自动的，不需要人工来介入 4. redis哨兵如何监控节点的 Redis Sentinel通过三个定时监控任务完成对各个节点发现和监控 每隔10秒，每个Sentinel节点会向主节点和从节点发送info命令获取最新的拓扑结构 每隔2秒，每个Sentinel节点会向Redis数据节点的sentinel：hello频道上发送该Sentinel节点对于主节点的判断以及当前Sentinel节点的信息，同时每个Sentinel节点也会订阅该频道，来了解其他Sentinel节点以及它们对主节点的判断。 每隔1秒，每个Sentinel节点会向主节点、从节点、其余Sentinel节点发送一条ping命令做一次心跳检测，来确认这些节点当前是否可达 5. 什么是主观下线 每个Sentinel节点会每隔1秒对主节点、从节点、其他Sentinel节点发送ping命令做心跳检测，当这些节点超过down-after-milliseconds没有进行有效回复，Sentinel节点就会对该节点做失败判定，这个行为叫做主观下线。 6. 什么是客观下线 当Sentinel主观下线的节点是主节点时，该Sentinel节点会通过sentinel ismaster-down-by-addr命令向其他Sentinel节点询问对主节点的判断，当超过个数，Sentinel节点认为主节点确实有问题，这时该Sentinel节点会做出客观下线的决定，这样客观下线的含义是比较明显了，也就是大部分Sentinel节点都对主节点的下线做了同意的判定，那么这个判定就是客观的。 7. 如何进行领导的选举 假如Sentinel节点对于主节点已经做了客观下线，那么是不是就可以立即进行故障转移了？当然不是，实际上故障转移的工作只需要一个Sentinel节点来完成即可，所以Sentinel节点之间会做一个领导者选举的工作，选出一个Sentinel节点作为领导者进行故障转移的工作。 每个在线的Sentinel节点都有资格成为领导者，当它确认主节点主观下线时候，会向其他Sentinel节点发送sentinel is-master-down-by-addr命令，要求将自己设置为领导者。 收到命令的Sentinel节点，如果没有同意过其他Sentinel节点的sentinelis-master-down-by-addr命令，将同意该请求，否则拒绝。 如果该Sentinel节点发现自己的票数已经大于等于max（quorum，num（sentinels）/2+1），那么它将成为领导者。 如果此过程没有选举出领导者，将进入下一次选举。 8. 故障转移是如何实现的 在从节点列表中选出一个节点作为新的主节点，选择方法如下 过滤：“不健康”（主观下线、断线）、5秒内没有回复过Sentinel节点ping响应、与主节点失联超过down-after-milliseconds*10秒。 选择slave-priority（从节点优先级）最高的从节点列表，如果存在则返回，不存在则继续。 选择复制偏移量最大的从节点（复制的最完整），如果存在则返回，不存在则继续。 选择runid最小的从节点。 Sentinel领导者节点会对第一步选出来的从节点执行slaveof no one命令让其成为主节点 Sentinel领导者节点会向剩余的从节点发送命令，让它们成为新主节点的从节点 Sentinel节点集合会将原来的主节点更新为从节点，并保持着对其关注，当其恢复后命令它去复制新的主节点 10. 集群1. Redis集群是如何决定数据存放在哪台主机中的 redis使用虚拟槽来进行分区的，首先规定槽的范围为0到16384，槽是集群内数据管理和迁移的基本单位，采用大范围槽的主要目的是为了方便树拆分和集群扩展。每个节点会负责一定数量的槽。所有的键通过哈希算法映射到0到16384整数槽中，计算公式为slot=CRC16(key)&amp;16383 2. 虚拟槽的特点 解耦了数据与节点之间的关系，简化了节点扩容和收缩的难度 节点自身维护槽的映射关系，不需要客户端或代理服务维护槽分区 支持节点、槽、键之间的映射查询，用于数据路由、在线伸缩等场景 3. redis集群功能有什么限制 对key的批量操作支持有限 key事务操作支持有 key作为数据分区的最小粒度，因此不能将一个大的键值对象如hash、list等映射到不同的节点 不支持多数据库空间 复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构。 4. 如何搭建集群 一种是手动方式，一种是通过ruby脚本来实现 首先准备节点，节点的个数最少为6个，通过配置参数开启集群模式 节点启动成功后会检查是否存在集群配置文件，如果没有会自动创建一份。 当集群内节点信息发生变化，如添加节点、节点下线、故障转移等。节点会自动保存集群状态到配置文件中。需要注意的是，Redis自动维护集群配置文件，不要手动修改，防止节点重启时产生集群信息错乱 集群中的每一个节点使用一个40位长度的16进制字符串的节点id表示，这个id与运行id不同.节点ID在集群初始化时只创建一次，节点重启时会加载集群配置文件进行重用，而Redis的运行ID每次重启都会变化 通过节点握手让6个节点彼此建立联系从而组成一个集群 节点握手是指一批运行在集群模式下的节点通过Gossip协议彼此通信，达到感知对方的过程 节点握手是集群彼此通信的第一步，由客户端发起命令：cluster meet{ip}{port} 我们只需要在集群内任意节点上执行cluster meet命令加入新节点，握手状态会通过消息在集群内传播，这样其他节点会自动发现新节点并发起握手流程。最后执行cluster nodes命令确认6个节点都彼此感知并组成集群 节点建立握手之后集群还不能正常工作，这时集群处于下线状态，所有的数据读写都被禁止 Redis集群把所有的数据映射到16384个槽中 每个key会映射为一个固定的槽，只有当节点分配了槽，才能响应和这些槽关联的键命令 每个负责处理槽的节点应该具有从节点，保证当它出现故障时可以自动进行故障转移,使用clusterreplicate{nodeId}命令让一个节点成为从节点 5. 集群中节点通信的流程 Gossip协议工作原理就是节点彼此不断通信交换信息，一段时间后所有的节点都会知道集群完整的信息，这种方式类似流言传播。 6. 常用的Gossip消息有哪些 meet消息：用于通知新节点加入。消息发送者通知接收者加入到当前集群，meet消息通信正常完成后，接收节点会加入到集群中并进行周期性的ping、pong消息交换 ping消息：集群内交换最频繁的消息，集群内每个节点每秒向多个其他节点发送ping消息，用于检测节点是否在线和交换彼此状态信息。ping消息发送封装了自身节点和部分其他节点的状态数据 pong消息：当接收到ping、meet消息时，作为响应消息回复给发送方确认消息正常通信。pong消息内部封装了自身状态数据。节点也可以向集群内广播自身的pong消息来通知整个集群对自身状态进行更新 fail消息：当节点判定集群内另一个节点下线时，会向集群内广播一个fail消息，其他节点接收到fail消息之后把对应节点更新为下线状态。 7. Gossip协议如何选择通信的节点 集群内每个节点维护定时任务默认每秒执行10次，每秒会随机选取5个节点找出最久没有通信的节点发送ping消息，用于保证Gossip信息交换的随机性。每100毫秒都会扫描本地节点列表，如果发现节点最近一次接受pong消息的时间大于cluster_node_timeout/2，则立刻发送ping消息，防止该节点信息太长时间未更新。每次消息头的大小为2kb 8. 集群伸缩的原理 集群伸缩=槽和数据在节点之间的移动 9. 如何扩容集群 准备新节点 需要提前准备好新节点并运行在集群模式下，新节点建议跟集群内的节点配置保持一致，便于管理统一 加入集群 新节点依然采用cluster meet命令加入到现有集群中 迁移槽和数据 槽是Redis集群管理数据的基本单位，首先需要为新节点制定槽的迁移计划，确定原有节点的哪些槽需要迁移到新节点。迁移计划需要确保每个节点负责相似数量的槽，从而保证各节点的数据均匀 10. 如何收缩集群 首先需要确定下线节点是否有负责的槽，如果是，需要把槽迁移到其他节点，保证节点下线后整个集群槽节点映射的完整性 当下线节点不再负责槽或者本身是从节点时，就可以通知集群内其他节点忘记下线节点，当所有的节点忘记该节点后可以正常关闭 11. 什么是集群的故障转移 Redis集群自身实现了高可用。高可用首先需要解决集群部分失败的场景：当集群内少量节点出现故障时通过自动故障转移保证集群可以正常对外提供服务。 12. 集群如何发现故障 当集群内某个节点出现问题时，需要通过一种健壮的方式保证识别出节点是否发生了故障。Redis集群内节点通过ping/pong消息实现节点通信，消息不但可以传播节点槽信息，还可以传播其他状态如：主从状态、节点故障等。因此故障发现也是通过消息传播机制实现的。 13. 集群如何标记主观下线 集群中每个节点都会定期向其他节点发送ping消息，接收节点回复pong消息作为响应。如果在cluster-node-timeout时间内通信一直失败，则发送节点会认为接收节点存在故障，把接收节点标记为主观下线（pfail）状态 14. 集群如何标记客观下线 当某个节点判断另一个节点主观下线后，相应的节点状态会跟随消息在集群内传播。ping/pong消息的消息体会携带集群1/10的其他节点状态数据，当接受节点发现消息体中含有主观下线的节点状态时，会在本地找到故障节点的ClusterNode结构，保存到下线报告链表中. 通过Gossip消息传播，集群内节点不断收集到故障节点的下线报告。当半数以上持有槽的主节点都标记某个节点是主观下线时。 15. 为什么必须是负责槽的主节点参与故障发现决策 因为集群模式下只有处理槽的主节点才负责读写请求和集群槽等关键信息维护，而从节点只进行主节点数据和状态信息的复制。 16. 为什么半数以上处理槽的主节点 必须半数以上是为了应对网络分区等原因造成的集群分割情况，被分割的小集群因为无法完成从主观下线到客观下线这一关键过程，从而防止小集群完成故障转移之后继续对外提供服务。 17. 如何进行故障转移 故障节点变为客观下线后，如果下线节点是持有槽的主节点则需要在它的从节点中选出一个替换它，从而保证集群的高可用. 资格检查：每个从节点都要检查最后与主节点断线时间，判断是否有资格替换故障的主节点。如果从节点与主节点断线时间超过cluster-node-time*cluster-slave-validity-factor，则当前从节点不具备故障转移资格。参数cluster-slavevalidity-factor用于从节点的有效因子，默认为10。 准备选举时间：当从节点符合故障转移资格后，更新触发故障选举的时间，只有到达该时间后才能执行后续流程 发起选举：当从节点定时任务检测到达故障选举时间（failover_auth_time）到达后，发起选举流程 更新配置纪元：配置纪元是一个只增不减的整数，每个主节点自身维护一个配置纪标示当前主节点的版本，所有主节点的配置纪元都不相等，从节点会复制主节点的配置纪元。 选举投票 11. 缓存设计1. 缓存带来了哪些收益 加速读写 缓存通常是基于内存的，读写性能非常高。 降低后端负载 帮助后端减少访问量和复杂计算（例如很复杂的SQL语句），在很大程度降低了后端的负载。 2. 缓存带来了哪些成本 数据不一致性：缓存层和存储层的数据存在着一定时间窗口的不一致性，时间窗口跟更新策略有关。 代码维护成本：加入缓存后，需要同时处理缓存层和存储层的逻辑，增大了开发者维护代码的成本。 运维成本：以Redis Cluster为例，加入后无形中增加了运维成本。 3. 什么场景应该使用缓存 开销大的复杂计算：以MySQL为例子，一些复杂的操作或者计算（例如大量联表操作、一些分组计算），如果不加缓存，不但无法满足高并发量，同时也会给MySQL带来巨大的负担。 加速请求响应：即使查询单条后端数据足够快（例如select*from table where id=），那么依然可以使用缓存，以Redis为例子，每秒可以完成数万次读写，并且提供的批量操作可以优化整个IO链的响应时间。 4. 什么是缓存穿透 缓存穿透是指查询一个根本不存在的数据，缓存层和存储层都不会命中，通常出于容错的考虑，如果从存储层查不到数据则不写入缓存层。缓存穿透将导致不存在的数据每次请求都要到存储层去查询，失去了缓存保护后端存储的意义。缓存穿透问题可能会使后端存储负载加大，由于很多后端存储不具备高并发性，甚至可能造成后端存储宕掉。 5. 为什么会发生缓存穿透 自身业务代码或者数据出现问题 一些恶意攻击、爬虫等造成大量空命中。下面我们来看一下如何解决缓存穿透问题 6. 如何优化缓存穿透问题 当存储层不命中后，仍然将空对象保留到缓存层中，之后再访问这个数据将会从缓存中获取，这样就保护了后端数据源。 使用布隆过滤器进行拦截 7. 什么是无底洞现象 添加新的节点，性能没有上升反而下降，这种现象称为无底洞现象 8. 无底洞现象是怎么产生的 键值数据库由于通常采用哈希函数将key映射到各个节点上，造成key的分布与业务无关，但是由于数据量和访问量的持续增长，造成需要添加大量节点做水平扩容，导致键值分布到更多的节点上，所以无论是Memcache还是Redis的分布式，批量操作通常需要从不同节点上获取，相比于单机批量操作只涉及一次网络操作，分布式批量操作会涉及多次网络时间。客户端一次批量操作会涉及多次网络操作，也就意味着批量操作会随着节点的增多，耗时会不断增大。 9. 无底洞现象怎么优化 10. 什么是缓存雪崩 由于缓存层承载着大量请求，有效地保护了存储层，但是如果缓存层由于某些原因不能提供服务，于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会级联宕机的情况。 11. 如何优化缓存雪崩 12. 什么是缓存击穿 13. 如何优化缓存击穿 参考：《Redis开发与运维》 （未完待续）","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"redis","slug":"数据库/redis","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/"}],"tags":[{"name":"总结","slug":"总结","permalink":"https://www.severin.xyz/tags/%E6%80%BB%E7%BB%93/"},{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"redis","slug":"redis","permalink":"https://www.severin.xyz/tags/redis/"}]},{"title":"MySQL SQL优化","slug":"MySQL索引优化","date":"2019-12-26T16:00:00.000Z","updated":"2020-02-06T11:17:25.836Z","comments":true,"path":"2019/12/27/MySQL索引优化/","link":"","permalink":"https://www.severin.xyz/2019/12/27/MySQL%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96/","excerpt":"","text":"1. 建立索引的技巧 对于单键索引，尽量选择针对当前query过滤性更好的索引 在选择组合索引的时候，当前Query中过滤性最好的字段在索引字段顺序中，位置越靠前越好。(避免索引过滤性好的索引失效) 在选择组合索引的时候，尽量选择可以能够包含当前query中的where字句中更多字段的索引 尽可能通过分析统计信息和调整query的写法来达到选择合适索引的目的 2. 索引规范 对复合索引于尽量使用全值匹配 遵循最左前缀法则 不要在索引列上进行任何操作，包括自动或者手动类型转换 存储引擎不能使用索引中范围条件右边的列 尽量使用覆盖索引 MySQL在使用不等于和is not null的时候会导致索引失效 like以通配符开头，MySQL索引会失效变成全表扫描 使用or会导致索引失效，可以使用union来代替 3. 关联查询优化 保证被驱动表的join字段已经被索引 left join 时，选择小表作为驱动表，大表作为被驱动表 inner join 时，mysql会自己帮你把小结果集的表选为驱动表 子查询尽量不要放在被驱动表，有可能使用不到索引 4. 子查询优化 有索引的情况下 用 inner join 是最好的 其次是 in ，exists最糟糕 无索引的情况下用 小表驱动大表 因为join 方式需要distinct ，没有索引distinct消耗性能较大。所以 exists性能最佳 in其次join性能最差？ 无索引的情况下大表驱动小表in 和 exists 的性能应该是接近的 都比较糟糕 exists稍微好一点 超不过5% 但是inner join 优于使用了 join buffer 所以快很多如果left join 则最慢","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"数据库/mysql","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"https://www.severin.xyz/tags/mysql/"},{"name":"索引优化","slug":"索引优化","permalink":"https://www.severin.xyz/tags/%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96/"}]},{"title":"InnoDB锁总结","slug":"InnoDB锁总结","date":"2019-12-25T16:00:00.000Z","updated":"2020-01-30T12:38:48.573Z","comments":true,"path":"2019/12/26/InnoDB锁总结/","link":"","permalink":"https://www.severin.xyz/2019/12/26/InnoDB%E9%94%81%E6%80%BB%E7%BB%93/","excerpt":"","text":"1. 什么是锁，为什么要加锁锁是一种用来实现对共享资源安全访问的工具，当多个用户并发地存取数据时，在数据库中就可能会产生多个事务同时操作同一行数据的情况，若对并发操作不加控制就可能会读取和存储不正确的数据，破坏数据的一致性。 2. InnoDB锁的类型 按照锁的粒度来分 表锁 MySQL中锁定粒度最大的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率越高，并发度最低，MyISAM和InnoDB引擎都支持表级锁。 行锁 Mysql中锁定粒度最小的一种锁，只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。 页锁 按照锁算法来分 Recodrd Lock Gap Lock Next-key Lock 按照加锁机制来分 乐观锁 乐观锁会“乐观地”假定大概率不会发生并发更新冲突，访问、处理数据过程中不加锁，只在更新数据时再根据版本号或时间戳判断是否有冲突，有则处理，无则提交事务； 悲观锁 悲观锁会“悲观地”假定大概率会发生并发更新冲突，访问、处理数据前就加排他锁，在整个数据处理过程中锁定数据，事务提交或回滚后才释放锁； 按照兼容性来分 共享锁 共享锁（Share Locks，简记为S锁）又称为读锁。其它事务可以并发地读取数据，可以再加共享锁，但任何事务都不能获取数据上的排它锁，直至已经释放所有共享锁。 排他锁 排它锁（Exclusive lock，简记为X锁）又称为写锁。若事务对数据对象加上了排它锁，则只允许该事务对数据对象进行读取和修改，其它事务不能再对数据对象加任何类型的锁，直到该事务释放对象上的排它锁。在更新操作(INSERT、UPDATE 或 DELETE)过程中始终应用排它锁。 意向锁 为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁。 意向共享锁（IS） 表示事务准备给数据行加入共享锁，事务在给一个数据行加共享锁之前必须先取得该表的IS锁。 意向排它锁（IX） 表示事务准备给数据行加入排它锁，事务在给一个数据行加排它锁之前必须先取得该表的IX锁。 3. 一些锁的细节 意向锁的细节 MySQL中表锁和行锁共存，若不引入意向锁，该如何判断是否锁冲突呢？ 假设事务T要对表T1加X锁，那就必须要判断T1表下每一个数据行是否加了S锁或者X锁。这样做的效率会非常低，需要对整个表进行遍历。在引入意向锁之后情况变得简单了。 假设事务T要对表T1加X锁，在这之前假设已经有事务A对数据行R加了S锁，那么此时表上已经有IS锁了（事务在给一个数据行加S锁之前必须先取得该表的IS锁）。由于X锁和IS锁冲突，所以事务T需要等待锁操作完成。这样就省去了遍历的操作，提高了冲突判断效率。 意向锁是表锁，表示的是一种意向，仅仅表示事务正在读或写某一行记录，在真正加行锁时才会判断是否冲突。意向锁是InnoDB自动加的，不需要用户干预。 IX和IS是表锁，不会与行锁发生冲突，只会与表锁发生冲突。 有哪些锁算法 Record Lock 记录锁，锁定一个行记录。 由于InnoDB特殊的索引机制，数据库操作使用主键索引时，InnoDB会锁住主键索引；使用非主键索引时，InnoDB会先锁住非主键索引，再锁定主键索引 Gap Lock 间隙锁，锁定一个区间，不包括记录本身 Next-Key Lock 记录锁+间隙锁（临键锁），锁定行记录和区间 InnoDB引擎采用Next-Key Lock来解决幻读问题。因为Next-Key Lock是锁住一个范围，所以就不会产生幻读问题。但是需要注意的是，InnoDB只在Repeatable Read隔离级别下使用该机制。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"数据库/mysql","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"https://www.severin.xyz/tags/mysql/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://www.severin.xyz/tags/InnoDB/"},{"name":"数据库锁","slug":"数据库锁","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E9%94%81/"}]},{"title":"MySQL存储引擎总结","slug":"MySQL存储引擎总结","date":"2019-12-25T16:00:00.000Z","updated":"2020-01-30T12:40:11.460Z","comments":true,"path":"2019/12/26/MySQL存储引擎总结/","link":"","permalink":"https://www.severin.xyz/2019/12/26/MySQL%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E6%80%BB%E7%BB%93/","excerpt":"","text":"1. 有哪些存储引擎 InnoDB 行锁设计，支持外键和事务，支持一致性非锁定读，支持多版本并发控制，实现了SQL标准的四种隔离级别，提供插入缓冲，二次写，自适应哈希索引和预读等功能。 MyISAM 表锁设计，不支持事务，支持全文索引，适合OLAP应用 NDB NDB是一个集群存储引擎，索数据全部方内存中 Memory 将表中的数据全部放在内存中，如果数据库重启或者发生崩溃，数据都将小时，非常适合临时数据的临时表，默认使用哈希索引 Archive存储引擎 只支持插入和查询操作，支持行压缩存储，压缩比一般可以达到1：10，使用行锁来实现插入，适合存储日志信息 Federated存储引擎 不存放数据，类似一个网关，指向远程的MYSQL数据库服务器 Maria存储引擎 用来替代MyISAM存储引擎，缓存数据和索引文件，行锁设计，提供MVCC功能，支持事务 2. InnoDB存储引擎与MyISAM存储引擎的对比 InnoDB支持事务；MyISAM不支持事务。 InnoDB支持外键；MyISAM不支持外键。 InnoDB锁的粒度是行锁；MyISAM锁的粒度是表锁。 InnoDB把数据和索引存在一起；MyISAM把表分为三个文件：表结构(.frm)、表内容(MYD)、表索引(MYI)。 InnoDB不保存表的具体行数，需要通过扫描表来获取有多少行；MyISAM保存表的具体行数。 InnoDB删除表中数据时是一行一行的删除；MyISAM删除表时是先drop表，然后重建表。 InnoDB可跨平台拷贝直接使用；MyISAM很难跨平台直接使用。 InnoDB表格很难压缩；MyISAM表格可以被压缩。 InnoDB中必须包含只有该字段的索引；MyISAM表中可以和其他字段一起建立联合索引。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"数据库/mysql","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"https://www.severin.xyz/tags/mysql/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://www.severin.xyz/tags/InnoDB/"},{"name":"存储引擎","slug":"存储引擎","permalink":"https://www.severin.xyz/tags/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/"}]},{"title":"MySQL调优：Explain命令的使用","slug":"MySQL调优 Explain命令的使用","date":"2019-12-25T16:00:00.000Z","updated":"2020-01-30T12:43:20.827Z","comments":true,"path":"2019/12/26/MySQL调优 Explain命令的使用/","link":"","permalink":"https://www.severin.xyz/2019/12/26/MySQL%E8%B0%83%E4%BC%98%20Explain%E5%91%BD%E4%BB%A4%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"1. Explain是什么有什么用使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的。分析你的查询语句或是表结构的性能瓶颈。 2. Explain执行计划包含哪些字段,每个字段表示什么 id select查询的序列号,包含一组数字，表示查询中执行select子句或操作表的顺序 id相同的时候 ，执行顺序是由上至下 id不同的时候，id值越大的越先执行 id相同和不同同时存在的时候，id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行 select_type 查询的类型，主要是用于区别普通查询、联合查询、子查询等的复杂查询。 类型 说明 SIMPLE 简单的 select 查询,查询中不包含子查询或者UNION PRIMARY 查询中若包含任何复杂的子部分，最外层查询则被标记为Primary DERIVED 在FROM列表中包含的子查询被标记为DERIVED(衍生)MySQL会递归执行这些子查询, 把结果放在临时表里。 SUBQUERY 在SELECT或WHERE列表中包含了子查询 DEPENDENT SUBQUERY 在SELECT或WHERE列表中包含了子查询,子查询基于外层 UNCACHEABLE SUBQUREY 无法被缓存的子查询 UNION 若第二个SELECT出现在UNION之后，则被标记为UNION；若UNION包含在FROM子句的子查询中,外层SELECT将被标记为：DERIVED UNION RESULT 从UNION表获取结果的SELECT table 显示这一行的数据是关于哪张表的 type type显示的是访问类型，是较为重要的一个指标，结果值从最好到最坏依次是： system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range(尽量保证) &gt; index &gt; ALL 类型 说明 system 表只有一行记录（等于系统表），这是const类型的特列，平时不会出现，这个也可以忽略不计 const 表示通过索引一次就找到了,const用于比较primary key或者unique索引。因为只匹配一行数据，所以很快,如将主键置于where列表中，MySQL就能将该查询转换为一个常量 eq_ref 唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。常见于主键或唯一索引扫描 ref 非唯一性索引扫描，返回匹配某个单独值的所有行,本质上也是一种索引访问，它返回所有匹配某个单独值的行，然而，它可能会找到多个符合条件的行，所以他应该属于查找和扫描的混合体 range 只检索给定范围的行,使用一个索引来选择行。key 列显示使用了哪个索引&lt;br一般就是在你的where语句中出现了between、&lt;、&gt;、in等的查询。这种范围扫描索引扫描比全表扫描要好，因为它只需要开始于索引的某一点，而结束语另一点，不用扫描全部索引。 index Full Index Scan，index与ALL区别为index类型只遍历索引树。这通常比ALL快，因为索引文件通常比数据文件小。（也就是说虽然all和Index都是读全表，但index是从索引中读取的，而all是从硬盘中读的） all Full Table Scan，将遍历全表以找到匹配的行 至少要到达range级别 possible_keys 显示可能应用在这张表中的索引，一个或多个。查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询实际使用。（可能会用到） key 实际使用的索引。如果为NULL，则没有使用索引 key_len 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。 ref 显示索引的哪一列被使用了，如果可能的话，是一个常数。哪些列或常量被用于查找索引列上的值 rows rows列显示MySQL认为它执行查询时必须检查的行数。 extra 包含不适合在其他列中显示但十分重要的额外信息，文件排序/使用where信息/临时表等信息。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"数据库/mysql","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"https://www.severin.xyz/tags/mysql/"}]},{"title":"InnoDB存储引擎总结","slug":"InnoDB存储引擎总结","date":"2019-12-24T16:00:00.000Z","updated":"2020-01-30T12:34:02.736Z","comments":true,"path":"2019/12/25/InnoDB存储引擎总结/","link":"","permalink":"https://www.severin.xyz/2019/12/25/InnoDB%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E6%80%BB%E7%BB%93/","excerpt":"","text":"1. MySQL体系结构和存储引擎1. 什么是数据库，什么是数据库实例 数据库是操作系统文件的集合。数据库实例时操作数据库文件的进程。 2. MySQL的线程架构 MySQL时单进程多线程架构的数据库 3. MySQL由哪几部分组成 连接池组件，管理服务和工具组件，SQL接口组件，查询分析器组件，优化器组件，缓冲组件，插件式存储引擎，物理文件组成。 4. MySQL与其他数据库的区别 拥有插件式的表存储引擎（存储引擎是基于表的不是基于数据库的） 5. 有哪些存储引擎，各自的特点是什么 InnoDB 行锁设计，支持外键和事务，支持一致性非锁定读，支持多版本并发控制，实现了SQL标准的四种隔离级别，提供插入缓冲，二次写，自适应哈希索引和预读等功能。 MyISAM 表锁设计，不支持事务，支持全文索引，适合OLAP应用 NDB NDB是一个集群存储引擎，索数据全部方内存中 Memory 将表中的数据全部放在内存中，如果数据库重启或者发生崩溃，数据都将小时，非常适合临时数据的临时表，默认使用哈希索引 Archive存储引擎 只支持插入和查询操作，支持行压缩存储，压缩比一般可以达到1：10，使用行锁来实现插入，适合存储日志信息 Federated存储引擎 不存放数据，类似一个网关，指向远程的MYSQL数据库服务器 Maria存储引擎 用来替代MyISAM存储引擎，缓存数据和索引文件，行锁设计，提供MVCC功能，支持事务 6. InnoDB存储引擎与MyISAM存储引擎的对比 InnoDB支持事务；MyISAM不支持事务。 InnoDB支持外键；MyISAM不支持外键。 InnoDB锁的粒度是行锁；MyISAM锁的粒度是表锁。 InnoDB把数据和索引存在一起；MyISAM把表分为三个文件：表结构(.frm)、表内容(MYD)、表索引(MYI)。 InnoDB不保存表的具体行数，需要通过扫描表来获取有多少行；MyISAM保存表的具体行数。 InnoDB删除表中数据时是一行一行的删除；MyISAM删除表时是先drop表，然后重建表。 InnoDB可跨平台拷贝直接使用；MyISAM很难跨平台直接使用。 InnoDB表格很难压缩；MyISAM表格可以被压缩。 InnoDB中必须包含只有该字段的索引；MyISAM表中可以和其他字段一起建立联合索引。 7. 如何查看数据库支持哪些存储引擎 show engine 8. 连接MySQL的方式有哪些 TCP/IP 命名管道和共享内存（windows） Unix域套接字（xxx.socket） 2. InnoDB存储引擎1. 有哪些InnoDB版本 版本 功能 老版本InnoDB 支持事务，行锁设计，MVCC InnoDB 1.0.x 增加了compress和dynamic页格式 InnoDB 1.1.x 增加了Linux AIO，多回滚段 InnoDB 1.2.x 增加了全文索引支持，在线索引添加 2. InnoDB存储引擎的体系结构 由内存池和后台线程组成 3. 由哪些后台线程，作用是什么 Master Thread 负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性，包括脏页的刷新、合并插入缓冲、undo页的回收等。 IO Thread 处理异步IO请求的回调，有四种IO Threa，包括write、read、insert buffer和log IO Thread Purge Thread 事务提交后，undo页就不需要了，Purge Thread用来回收已分配的undo页。 Page Cleaner Thread 进行脏页的刷新（这个功能是从Master Thread中抽出来的） 4. 有哪些内存池，都用来存放什么 缓冲池 缓存数据页、索引页、插入缓冲、自适应哈希索引、锁信息和数据字典信息等。 重做日志缓冲池 缓存重做日志，首先将重做日志信息放入重做日志缓冲池，然后按一定的频率将其刷新到重做日志文件。 额外的内存缓冲池 对数据结构本身的内存分配使用额外的内存缓冲池管理 5. 缓冲池如何管理的 由LRU列表、Free列表和Flush列表管理。 LRU List 使用LRU（最近最少使用）算法管理页，使用频繁的页放在列表的前端，使用不频繁的页放在列表尾端，当内存不足时，首先释放LRU尾端的页。存储引擎中的LRU算法做了优化，插入新的页时不是放到前端，而是放在列表长度的5/8处，原因是，如何直接读取到的页放LRU的首部，对于某些操作需要访问表中的许多页，但这些页仅在本次操作中需要，并不是活跃的热点数据，这样会将热点数据页从LRU中刷出。 Free List 用来管理空闲的页，分配新的页时必须在这个列表中查找空闲页，如果没有就淘汰LRU列表中的页。 Flush List 管理脏页列表，注意脏页既存在于Flush列表又存在于LRU列表。 6. 什么是checkpoint checkpoint就是将脏页刷新到磁盘的一种机制。InnoDB存储引擎提交事务的时候，需要先写重做日志，然后把修改缓存中的页，这样即使数据库宕机也可以通过重做日志恢复，但是问题是重做日志的大小是有限制的，当重做日志文件不足时就可能导致数据丢失，另外如果只通过重做日志来恢复，则需要把所有的修改重做一遍时间太长了，因此引入了checkpoint技术，他可以在某些情况下将脏页刷新到磁盘，这样就不用担心重做日志文件大小不足的问题，并且恢复时checkpoint之前的重做日志不需要恢复，加快来了恢复的时间。 7. checkpoint解决的问题 缩短数据库恢复的时间 缓冲池不够用时，将脏页刷新到磁盘 重做日志不可用时，将脏页刷新到磁盘 8. checkpoint的分类 Sharo Checkpoint 数据库关闭时将所有的脏页都刷新回磁盘，这是默认的工作方式。 Fuzzy Checkpoint Master Thread Checkpoint 隔一定时间间隔从缓冲池的脏页列表中刷新一定比列的页到磁盘（异步执行） FLUSH_LRU_LIST Checkpoint InnoDB要保证LRU列表中有差不多100个空闲页，如果不足会溢出LRU列表尾端的页，如果这些页中有脏页，会进行Checkpoint Aysnc/Sync Flush Checkpoint 重做日志不可用时，这时需要强制将一些页刷新回磁盘，而此时脏页是从脏页列表中选取的。 Dirty Page too much Checkpoint 脏页数量太多会导致InnoDB存储引擎强制进行Checkpoint 9. Master线程的工作方式 10. InnoDB有哪些关键特性 插入缓冲 两次写 自适应哈希索引 异步IO 刷新邻接页 11. 什么是插入缓冲 对于自增长主键聚集索引的插入是顺序的不需要磁盘的随机读取，效率非常高，但是每张表只有一个聚集索引，其他的都是非聚集索引，对于非聚集索引的插入需要随机读取磁盘（离散的访问非聚集索引页），效率非常低，因此InnoDB存储引擎引入了插入缓冲来解决非聚集索引的插入性能问题：对于非聚集索引的插入或更新操作，不是每一次直接插入到索引页中，而是先判断插入的非聚集索引页是否在缓冲池中，若在，则直接插入，若不在则先放入一个插入缓冲对象中，然后再以一定的频率和情况进行插入缓冲和富非聚集多月叶子结点的合并操作，这样多个插入就合并到一个操作中，这就大大提高了对非聚集索引插入的性能问题。插入缓冲只适应与非唯一的非聚集索引，非唯一的原因是如果是唯一的，那么就要检查每一个页判断是否已存在，这样查找也是离散读取，插入缓冲就没有任何意义了。除了插入缓冲对于更新和删除也有类似的操作（他们的思想是在内存中进行操作，这是一种欺骗，但是之后会写入磁盘保证一致性）。 11. 插入缓冲是如何实现的 插入缓冲是B+树 12. 什么是两次写 InnoDB的页默认大小为16k，但是写入磁盘是按照磁盘的块为单位写，也就是说每次将数据页写会磁盘不是一个原子过程，可能写到一半的时候突然数据库宕机，这个问题叫做部分写失效，这个时候是无法通过redo日志来恢复的，因为redo日志是一个逻辑日日志，只记录了某个偏移量写什么记录，而这个页本身就已经损坏了，所有没有无法恢复。两次写是为了解决这个问题：在内存中有一个2m大小的两次写缓存，在进行脏页刷新的时候，首先会将脏页分两次复制到两次写缓存中，然后两次写缓存会刷新到磁盘中的共享表中，写入磁盘后再进行脏页的刷新，这样就算发生部分写失效，也可以找到该页的副本进行恢复。 13. 什么是自适应哈希索引 哈希是一种快速查询的方法，时间复杂度为O(1)；InnoDB存储引擎会监控各索引页的查询，如果发现建立哈希所有可以提高效率就坏建立哈希索引，这种哈希索引称为自适应哈希索引，他是不需要用户干预的。 14. 什么是异步IO 异步IO是指可以发出一个IO后不需要等待直接发起另一个IO，当所有IO请求发送出去，等待所有IO完成，这样可以提高IO效率。 15. 什么是刷新邻接页 当刷新一个脏页的时候，会检查这个脏页所在的区的其他页，如果也是脏页那么也会被刷新，这样的好处是可以通过AIO把多个IO合并成一个IO 3. 文件1. MySQL中有哪些文件 有参数文件、日志文件、套接字文件、pid文件、表结构定义文件。 2. MySQL数据库的参数类型有哪些 有静态参数和动态参数，动态参数可以在数据库实例启动的情况下修改生效，静态参数必须关闭数据库实例修改后重启。 3. 有哪些日志文件 错误日志 记录数据库在启动运行关闭过程中发生的错误信息。 二进制日志 记录了对数据库执行更改的所有操作，用于恢复，复制和审计的功能 慢查询日志 可以在数据库启动时设置一个慢查询阈值，所有查询时间大于这个阈值的sql语句都会被记录下来，通过慢查询日志可以定位到查询效率低的sql语句，方便排查问题。 查询日志 记录了所有的查询sql语句包括未执行的 4. InnoDB存储引擎有哪些独有的文件 表空间文件和重做日志文件。 4. 表 5. 索引和算法1. 什么是索引 索引是一种用于快速查询和排好序的数据结构 2. 有哪些索引 从数据结构的角度 B+树索引 hash索引 全文索引 从物理存储角度 聚集索引 非聚集索引 从逻辑角度 主键索引 单列索引 符合索引 唯一索引或非唯一索引 空间索引 3. 什么是聚集索引，什么是非聚集索引 聚集索引就是按照表的主键构造一棵B+树，叶节点存放了整张表的行记录数据，聚集索引的特性是数据是索引的一部分。非聚集索引指定了表中记录的逻辑顺序，但记录的物理顺序和索引的顺序不一致。聚集索引和非聚集索引都采用了B+树的结构，但非聚集索引的叶子层并不与实际的数据页相重叠，而采用叶子层包含一个指向表中的记录在数据页中的指针的方式。 4. 聚集索引的优点和缺点 聚集索引的优点是查询速度快，并且一点某个记录找到了其相邻的记录就在这条记录的前后，适合范围查找。缺点是对表进行修改的速度比较慢，这是为了保持表中的记录的物理顺序与索引的顺序一致，而把记录插入到数据页的相应位置，必须在数据页中进行数据重排， 降低了执行速度。 5. 什么时候建聚集索引 此列包含有限数目的不同值； 查询的结果返回一个区间的值； 查询的结果返回某值相同的大量结果集。 6. 什么时候需要建立索引 主键自动建立唯一索引 频繁作为查询条件的字段应该创建索引 查询中与其他表管理的字段，外键关系建立索引 查询中排序的字段建立索引 查询中统计或者分组字段 7. 说明时候不需要建立索引 表记录太少 经常增删改的表 where条件用不到的字段 数据区分度比较小的字段、 8. 什么是B+树 B+树是一个多路的平衡搜索树，B+树的结点由有序的元素和指针组成，指针的个数称为B+树的阶，B+树的特点是。 根节点至少有一个元素，非根节点元素的范围为m/2&lt;=k&lt;=m-1 B+树有两种类型的节点，内部节点和叶子节点，内部节点不存储数据，只存储所有，数据都存储在叶子节点。 内部节点的key是按顺序存放的，元素的左子树的key都小于它，右子树的key都大于它，叶子结点中的记录也按照key的大小排列。 每个叶子结点都存有相邻叶子结点的指针，叶子结点本身依关键字的大小自小而大顺序链接。 父节点存有右孩子的第一个元素的索引。 9. 什么是B树 B树也称B-树,它是一颗多路平衡查找树。二叉树我想大家都不陌生，其实，B树和后面讲到的B+树也是从最简单的二叉树变换而来的 每个节点最多有m-1个关键字（可以存有的键值对）。 根节点最少可以只有1个关键字。 非根节点至少有m/2个关键字。 每个节点中的关键字都按照从小到大的顺序排列，每个关键字的左子树中的所有关键字都小于它，而右子树中的所有关键字都大于它。 所有叶子节点都位于同一层，或者说根节点到每个叶子节点的长度都相同。 每个节点都存有索引和数据，也就是对应的key和value。 10. 对比以下B+树比B树的优点（为什么MySQL采用B+树作为索引而不是B树） 单一节点存储的元素更多，使得查询的IO次数更少，所以也就使得它更适合做为数据库MySQL的底层数据结构了。 所有的查询都要查找到叶子节点，查询性能是稳定的，而B树，每个节点都可以查找到数据，所以不稳定。 所有的叶子节点形成了一个有序链表，更加便于查找。 6. 锁1. 什么是锁，为什么要加锁 锁是一种用来实现对共享资源安全访问的工具，当多个用户并发地存取数据时，在数据库中就可能会产生多个事务同时操作同一行数据的情况，若对并发操作不加控制就可能会读取和存储不正确的数据，破坏数据的一致性。 2. InnoDB锁的类型 按照锁的粒度来分 表锁 MySQL中锁定粒度最大的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率越高，并发度最低，MyISAM和InnoDB引擎都支持表级锁。 行锁 Mysql中锁定粒度最小的一种锁，只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。 页锁 按照锁算法来分 Recodrd Lock Gap Lock Next-key Lock 按照加锁机制来分 乐观锁 乐观锁会“乐观地”假定大概率不会发生并发更新冲突，访问、处理数据过程中不加锁，只在更新数据时再根据版本号或时间戳判断是否有冲突，有则处理，无则提交事务； 悲观锁 悲观锁会“悲观地”假定大概率会发生并发更新冲突，访问、处理数据前就加排他锁，在整个数据处理过程中锁定数据，事务提交或回滚后才释放锁； 按照兼容性来分 共享锁 共享锁（Share Locks，简记为S锁）又称为读锁。其它事务可以并发地读取数据，可以再加共享锁，但任何事务都不能获取数据上的排它锁，直至已经释放所有共享锁。 排他锁 排它锁（Exclusive lock，简记为X锁）又称为写锁。若事务对数据对象加上了排它锁，则只允许该事务对数据对象进行读取和修改，其它事务不能再对数据对象加任何类型的锁，直到该事务释放对象上的排它锁。在更新操作(INSERT、UPDATE 或 DELETE)过程中始终应用排它锁。 意向锁 为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁。 意向共享锁（IS） 表示事务准备给数据行加入共享锁，事务在给一个数据行加共享锁之前必须先取得该表的IS锁。 意向排它锁（IX） 表示事务准备给数据行加入排它锁，事务在给一个数据行加排它锁之前必须先取得该表的IX锁。 3. 意向锁是什么，解决了什么问题 MySQL中表锁和行锁共存，若不引入意向锁，该如何判断是否锁冲突呢？ 假设事务T要对表T1加X锁，那就必须要判断T1表下每一个数据行是否加了S锁或者X锁。这样做的效率会非常低，需要对整个表进行遍历。在引入意向锁之后情况变得简单了。 假设事务T要对表T1加X锁，在这之前假设已经有事务A对数据行R加了S锁，那么此时表上已经有IS锁了（事务在给一个数据行加S锁之前必须先取得该表的IS锁）。由于X锁和IS锁冲突，所以事务T需要等待锁操作完成。这样就省去了遍历的操作，提高了冲突判断效率。 意向锁是表锁，表示的是一种意向，仅仅表示事务正在读或写某一行记录，在真正加行锁时才会判断是否冲突。意向锁是InnoDB自动加的，不需要用户干预。 IX和IS是表锁，不会与行锁发生冲突，只会与表锁发生冲突。 4. 有哪些锁算法 Record Lock 记录锁，锁定一个行记录。 由于InnoDB特殊的索引机制，数据库操作使用主键索引时，InnoDB会锁住主键索引；使用非主键索引时，InnoDB会先锁住非主键索引，再锁定主键索引 Gap Lock 间隙锁，锁定一个区间，不包括记录本身 Next-Key Lock 记录锁+间隙锁（临键锁），锁定行记录和区间 InnoDB引擎采用Next-Key Lock来解决幻读问题。因为Next-Key Lock是锁住一个范围，所以就不会产生幻读问题。但是需要注意的是，InnoDB只在Repeatable Read隔离级别下使用该机制。 7. 事务1. 事务有哪些特性 A（原子性） 原子性指的是一组操作不可分割要么全部执行成功要么全部执行失败，这有全部执行成功整个事务才算成功。 C（一致性） 事务将数据库从一种状态转变为下一种状态，事务开始和结束后，数据库的完整性约束没有被破坏。 I（隔离性） 事务与事务之间是相互分离的，一个事务提交前对其他事务是不可见的。 D（持久性） 事务一旦提交，其结果是永久性的，即使发生宕机数据库也能恢复。 2. 什么是事务 事务就是访问或者更新数据库时的一个执行单元，这个执行单元中的所有操作，要么全部完成要么全部要失败 3. 事务有哪些分类 扁平事务 所有操作都是在一个层次，操作是原子的，要么全部执行要么都回滚，不能回滚部分。 带保存点是扁平事务 允许事务回滚到当事务的一个较早的状态。（保存点） 链事务 在提交一个事务时，释放不需要的数据对象，将必要的处理上下文隐式的传给下一个要开始的事务。需要注意，提交事务操作和下一个事务操作将合并为一个原子操作，就是下一个事务可以看到上一个事务的结果。 链事务，就是指回滚时，只能恢复到最近一个保存点；而带有保存点的扁平事务则可以回滚到任意正确的保存点。 链事务在执行commit后就会释放当前事务所持有的所有锁，而带有保存点的扁平事务不会影响所持有的锁。 嵌套事务 事务与事务之间是有层次结构的 分布式事务 分布式环境下的事务 4. 事务的隔离级别 未提交读 已提交读 可重复读 可串行化 5. 事务是怎么实现的 事务具有四种属性ACID，其中隔离性是通过锁来实现的，其他的三种属性是用过undo log和redo log来实现的。 6. redo日志和undo日志的区别 redo日志用来保证事务的原子性和持久性，undo日志用来保证事务的一致性 redo日志和undo日志都可以视为一种恢复操作，redo日志恢复提交修改的页操作，undo日志用来回滚行记录到某个特定的版本 redo日志是物理日志，记录的是对页的物理修改，undo日志是逻辑日志，根据每行记录进行记录 7. 事务的ACID是怎么实现的 事务的原子性是通过undo日志实现的、持久性是通过redo日志或二进制日志实现的、隔离性是通过锁机制实现的，一致性是通过保证原子性、持久性和隔离性，数据库本身和应用层面进行保证的。 https://www.cnblogs.com/kismetv/p/10331633.html 7. 详细介绍重做日志 什么是重做日志，重做日志有什么用 在事务提交前必须将该事务的所有日志写到重做日志文件中，重做日志文件由两部分组成，分别是redo日志和undo日志。 如何保证重做日志缓冲写入重做日志文件 在每次重做日志缓冲写入重做日志文件时都会调用fsync操作，将数据同步到磁盘，因此重做日志文件写入的效率与fsync操作调用的频率有关，允许用户控制重做日志刷新到磁盘的策略，主要由三种策略，事务提交时必须调用一次fsync；事务提交时不进行出写入重做日志操作，这个操作交给master thread完成，它会没1秒钟进行一次fsync操作；不进行fsync操作，由操作系统来决定什么时候同步。fsync执行的越频繁效率越低，但是越不容易丢失数据， redo日志的作用 redo日志是来保证数据库的持久性的，当事务提交后会将修改写入缓存，如果发生了宕机可以通过redo日志来恢复。 undo日志的作用 事务在对数据库进行修改时，由于某种原因失败了，或者用户请求回滚，可以利用undo信息将数据回滚到修改之前的样子。 undo log属于逻辑日志，它记录的是sql执行相关的信息。当发生回滚时，InnoDB会根据undo log的内容做与之前相反的工作：对于每个insert，回滚时会执行delete；对于每个delete，回滚时会执行insert；对于每个update，回滚时会执行一个相反的update，把数据改回去。 为什么redo日志的写入会比刷新脏页的速度快 刷脏是随机IO，因为每次修改的数据位置随机，但写redo log是追加操作，属于顺序IO。 刷脏是以数据页（Page）为单位的，MySQL默认页大小是16KB，一个Page上一个小修改都要整页写入；而redo log中只包含真正需要写入的部分，无效IO大大减少。 redo日志和二进制日志的区别 作用不同：redo log是用于crash recovery的，保证MySQL宕机也不会影响持久性；binlog是用于point-in-time recovery的，保证服务器可以基于时间点恢复数据，此外binlog还用于主从复制。 层次不同：redo log是InnoDB存储引擎实现的，而binlog是MySQL的服务器层(可以参考文章前面对MySQL逻辑架构的介绍)实现的，同时支持InnoDB和其他存储引擎。 内容不同：redo log是物理日志，内容基于磁盘的Page；binlog的内容是二进制的，根据binlog_format参数的不同，可能基于sql语句、基于数据本身或者二者的混合。 写入时机不同：binlog在事务提交时写入；redo log的写入时机相对多元 8. 备份与恢复 参考：《MySQL技术内幕：InnoDB存储引擎》 （未完待续）","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"数据库/mysql","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"https://www.severin.xyz/tags/mysql/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://www.severin.xyz/tags/InnoDB/"}]},{"title":"MySQL索引总结","slug":"MySQL索引总结","date":"2019-12-24T16:00:00.000Z","updated":"2020-01-30T12:41:53.537Z","comments":true,"path":"2019/12/25/MySQL索引总结/","link":"","permalink":"https://www.severin.xyz/2019/12/25/MySQL%E7%B4%A2%E5%BC%95%E6%80%BB%E7%BB%93/","excerpt":"","text":"MySQL索引包含两部分内容，一部分是MySQL索引本身，一部分是B树和B+树。 对于MySQL索引本身需要了解基本的概念和原理，知道有哪些索引？怎么建立索引？什么时候应该建索引，索引建立在什么字段上。 1. MySQL索引 什么是索引 索引是一种用于快速查询和排好序的数据结构 索引的分类（分类一定是基于某种标准来分的） 从数据结构的角度 B+树索引 hash索引 全文索引 从物理存储角度 聚集索引 非聚集索引 从逻辑角度 主键索引 单列索引 符合索引 唯一索引或非唯一索引 空间索引 什么是聚集索引，什么是非聚集索引 什么时候需要建立索引 主键自动建立唯一索引 频繁作为查询条件的字段应该创建索引 查询中与其他表管理的字段，外键关系建立索引 查询中排序的字段建立索引 查询中统计或者分组字段 说明时候不需要建立索引 表记录太少 经常增删改的表 where条件用不到的字段 数据区分度比较小的字段、 2. B树和B+树的概念 什么是B+树 B+树是一个多路的平衡搜索树，B+树的结点由有序的元素和指针组成，指针的个数称为B+树的阶，根节点至少有一个元素，非根节点元素的范围为m/2&lt;=k&lt;=m-1,B+树有两种类型的节点，内部节点和叶子节点，内部节点不存储数据，只存储所有，数据都存储在叶子节点。内部节点的key是按顺序存放的，元素的左子树的key都小于它，右子树的key都大于它，叶子结点中的记录也按照key的大小排列。每个叶子结点都存有相邻叶子结点的指针，叶子结点本身依关键字的大小自小而大顺序链接。父节点存有右孩子的第一个元素的索引。 什么是B树 B树也称B-树,它是一颗多路平衡查找树。二叉树我想大家都不陌生，其实，B树和后面讲到的B+树也是从最简单的二叉树变换而来的 每个节点最多有m-1个关键字（可以存有的键值对）。 根节点最少可以只有1个关键字。 非根节点至少有m/2个关键字。 每个节点中的关键字都按照从小到大的顺序排列，每个关键字的左子树中的所有关键字都小于它，而右子树中的所有关键字都大于它。 所有叶子节点都位于同一层，或者说根节点到每个叶子节点的长度都相同。 每个节点都存有索引和数据，也就是对应的key和value。 3. 对比以下B+树比B树的优点（为什么用B+树作为索引而不是B树） 单一节点存储的元素更多，使得查询的IO次数更少，所以也就使得它更适合做为数据库MySQL的底层数据结构了。 所有的查询都要查找到叶子节点，查询性能是稳定的，而B树，每个节点都可以查找到数据，所以不稳定。 所有的叶子节点形成了一个有序链表，更加便于查找。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"数据库/mysql","permalink":"https://www.severin.xyz/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://www.severin.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"mysql","slug":"mysql","permalink":"https://www.severin.xyz/tags/mysql/"},{"name":"MySQL索引","slug":"MySQL索引","permalink":"https://www.severin.xyz/tags/MySQL%E7%B4%A2%E5%BC%95/"}]},{"title":"TCP数据流与窗口管理","slug":"TCP数据流与窗口管理","date":"2019-12-21T16:00:00.000Z","updated":"2020-01-31T03:12:53.162Z","comments":true,"path":"2019/12/22/TCP数据流与窗口管理/","link":"","permalink":"https://www.severin.xyz/2019/12/22/TCP%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%8E%E7%AA%97%E5%8F%A3%E7%AE%A1%E7%90%86/","excerpt":"","text":"1. 交互式通信 什么是交互式通信 交互式通信是指双方通信时传输一些较小的信息，这些信息会封装成较小的报文段，但是又希望延迟尽量低。这里的问题在于，TCP每次传输的信息量很少（也就是说大部分都是TCP首部）降低了网络的利用率，但是如果让TCP发送的数据多一点(比如说几个小数据累积在一起后发送)又会带来较大的时延，如何平衡这两者关系是交互式通信要研究的问题。 交互式通信是如何做的 通常交互式通信，用户在一端输入信息希望马上看到另一方的回显信息，采取的做法是首先发送方向接收方发送一个报文，接收方收到报文后要对其进行确认，但是不同的是接收方会把回显信息与ACK信息一起返回，发生放收到后再进行确认，通过这样的优化，可以少发一个报文并且发送方也可以少等一个报文发送的时间。 2. 延迟确认TCP不会对每一个数据包都返回ACK确认,而是采取累积确认的方式，这样就可以大量减少网络中的ACK数据包，但是TCP不能够延迟任意时长，否则发送方会超时，不同的操作系统实现的延时时间不一样。延时确认不能用在对时延要求高的应用上。 3. Nagle算法 Nagle算法解决了什么问题 Nagle算法是一种拥塞控制算法。每次发送少量的数据时，TCP的头部和IP的首部会有很大的开销，这会造成相当高的网络传输代价（这种代价对局域网没有影响，对广域网可能造成拥塞，如糊涂窗口综合征），Nagle算法就是用来解决这种小包可能导致网络拥塞的问题。 Nagle算法的原理 Nagle算法要求，当一个TCP连接中有在传数据（已经发生但还未经确认），小的报文段（长度小于SMSS，SMSS是指发送方能够发送的最大数据段的长度）就不能被发送，直到所有的在传数据都收到ACK，TCP会将小数据收集整合到一个报文段中发送。大的报文段不受Nagle算法的影响，实际上Nagle算法对于小的报文段来说就是一个停止等待协议。 Nagle算法为什么有效 因为要等到所有在传数据收到ACK才发小数据，那么ACK返回的越快，小数据包传输也就越快，如果是时延比较高的广域网中，ACK返回就会越慢，小数据被发送出去的速度也会变慢，这样小数据包就不会加重网络的阻塞。也就是说RTT可以控制发包速率。 如果延时ACK与Nagle算法结合会发生什么 延时ACK与Nagle算法直接结合使用效果会很差，延迟确认会使得接受方推迟发送ACK报文，而使用了Nagle算法的发送方又要等到接收到ACK报文才发送，导致网络处于空闲状态。 Nagle算法不适用于什么场景 要求时延尽量小的应用不适合使用Nagle算法如网络游戏，远程控制等。 4. 流量控制与窗口管理TCP使用滑动窗口来实现流量控制，”流量控制“控制的是发送方的发送速率，它与拥塞控制不同的是，流量控制考虑的是接收方的接受能力（如果接收方接受能力差，接收缓存满了，发送方发过来的数据包就会被丢弃然后重传，显然这是不必要的） 滑动窗口的机制 发送方和接收方都会维护一个窗口，称为发送窗口和接收窗口，它们都是以字节为单位的。 发送窗口 假设发送窗口向右移动，则发送窗口左边的数据是已经发送且确认了的可以从缓存区中清除，发送窗口右边的数据是还不能发送的，发送窗口内部维护了一个指针，指针的左边是已发送但还未确认的数据，指针右边是可发送但还未发送的数据。 接收窗口 接收窗口左边界的数据是已接受并确认的数据，右窗口是不能接收的数据，窗口内部是接受后还未确认的数据 如何利用滑动窗口实现流量控制 TCP发送端向接收端使用滑动窗口机制发送数据，发送速率取决于发送方的发送窗口大小，所以为了实现流量控制，接收方会携带窗口大小信息来控制发送方发送窗口的大小，以达到发送端流量控制的目的。 零窗口问题 流量控制过程中的一种极端情况是，接收端告知发送端窗口为0，这样发送端就无法发送数据给接收端了，当接收端窗口大小恢复为非零值，会给发送端传输一个窗口更新报文告知其可以继续发送数据，但是如果发送方发送的一个包含窗口更新的ACK丢失了，通信双方就会一直处于等待状态。 TCP持续计时器 持续计时器解决了零窗口问题，发送端会维护一个持续计时器间歇性的查询接收端，看其窗口是否已经增长。计时器会触发发送端发送一个窗口探测报文，接收端会回一个带有窗口信息的ACK报文。采用指数退避的方式来设置持续计时器的时间。 糊涂窗口综合症 糊涂窗口综合症是指当发送端应用进程产生数据很慢、或接收端应用进程处理接收缓冲区数据很慢，或二者兼而有之；就会使应用进程间传送的报文段很小，特别是有效载荷很小； 极端情况下，有效载荷可能只有1个字节；传输开销有40字节(20字节的IP头+20字节的TCP头) 这种现象。 发送方和接收方都可能会引发糊涂窗口综合征 发送方 发送方产生的数据比较慢，每次产生的数据都很小。对于发送方不应该发送小的报文段，可以使用Nagle算法来控制何时发送 接收方 接收方的原因是进程处理缓存中的数据不及时，然后通知了一个较小的窗口给发送方。对于接收方应该避免通告小的窗口值，可以使用延迟确认的方式等缓存中的数据被情空了再告知窗口值。 5. 紧急机制带外数据 传输层协议使用带外数据（out-of-band，OOB）来发送一些重要的数据，如果通信一方有重要的数据需要通知对方时，协议能够将这些数据快速地发送到对方。为了发送这些数据，协议一般不使用与普通数据相同的通道，而是使用另外的通道。 TCP协议没有真正意义上的带外数据。为了发送重要协议，TCP提供了一种称为紧急模式（urgent mode）的机制。TCP协议在数据段中设置URG位，表示进入紧急模式。接收方可以对紧急模式采取特殊的处理。 紧急模式 TCP头部有一个位字段URG标志紧急数据，当URG位为1时，TCP头部节点的紧急指针位会记录一个偏移量，指向紧急数据的最后一位，在读取到紧急指针所指向的位置之前，TCP的接受进程都处于紧急状态，当读取到紧急数据后一位时，恢复到正常状态。 当URG置1时，发送方应用进程就告诉发送方的TCP有紧急数据要传送。于是发送方TCP就把紧急数据插入到本报文段数据的最前面。 在紧急指针字段的具体实现上，由于过去的文档有错误或不太明确的地方，因而导致对有关RFC文档产生了不同的理解。 参考：《TCP/IP详解 卷1：协议》Kevin R. Fall W.Richard Stevens","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"传输层","slug":"计算机网络/传输层","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E4%BC%A0%E8%BE%93%E5%B1%82/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"tcp","slug":"tcp","permalink":"https://www.severin.xyz/tags/tcp/"}]},{"title":"HTTP协议总结","slug":"计算机网络应用层协议HTTP","date":"2019-12-21T16:00:00.000Z","updated":"2020-02-03T06:29:20.498Z","comments":true,"path":"2019/12/22/计算机网络应用层协议HTTP/","link":"","permalink":"https://www.severin.xyz/2019/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E5%B1%82%E5%8D%8F%E8%AE%AEHTTP/","excerpt":"","text":"通常计算机网络的知识中，每一层都会要求深入了解一个协议，而应用层协议是我们日常接触到的最多的协议，HTTP则是应用层协议中最常用的协议，所以一定一定要对HTTP协议有非常深刻的认识。除了要理解HTTP协议本身（这里通常指的是HTTP1.1），我们还要对不同版本的HTTP进行研究，如HTTP1.0/HTTP1.1/HTTP2.0,要了解每一个版本的协议的特点，解决了上一个版本的哪一个问题。 1. 概述 HTTP协议是什么（下一个定义） HTTP被设计于20世纪90年代初期，是一种可扩展的协议。它是应用层的协议，通过TCP或者是TLS加密的TCP连接来发送，理论上任何可靠的传输协议都可以使用。因为其良好的扩展性，时至今日，它不仅被用来传输超文本文档，还用来传输图片、视频或者向服务器发送如HTML表单这样的信息。HTTP还可以根据网页需求，仅获取部分Web文档内容更新网页。 HTTP的组件有哪些 客户端（用户代理） 就是任何能够为用户发起行为的工具，大多数时指的是浏览器，但是只要能够发起http请求的都成为用户代理，如程序员写的爬虫等 代理 在浏览器和服务器之间，有许多计算机和其他设备转发了HTTP消息。由于Web栈层次结构的原因，它们大多都出现在传输层、网络层和物理层上，对于HTTP应用层而言就是透明的，虽然它们可能会对应用层性能有重要影响。还有一部分是表现在应用层上的，被称为代理（Proxies）。代理（Proxies）既可以表现得透明，又可以不透明（“改变请求”会通过它们）。 常用的代理的作用： 缓存 过滤 负载均衡 认证 日志记录 服务端 提供客户端请求的文档或资源的运行在服务器上的程序 HTTP的特点 HTTP是简单的，HTTP1.1和HTTP1.0都是基于ASCII码的文本协议，非常易读 HTTP是可扩展的，通过添加headers,只要服务端和客户端就新 headers 达成语义一致，新功能就可以被轻松加入进来. HTTP是无状态的（重点，什么是无状态）：在同一个连接中，两个执行成功的请求之间是没有关系的，服务器不能确定两个来自同一个连接的http请求是不是同一个用户 HTTP与连接 HTTP并不需要其底层的传输层协议是面向连接的，只需要它是可靠的，或不丢失消息的（这句话是说HTTP不要求底层协议面向连接），但是通常HTTP协议基于TCP协议（这里说的是通常还是基于面向连接的协议） HTTP1.0 一次HTTP请求响应就要建立和释放一次TCP连接 HTTP1.1引入了流水线（很难实现）和持久连接（TCP连接在一个给定的时间内不会被释放）的机制。 HTTP2.0 HTTP的工作过程 打开一个TCP连接 发送一个HTTP报文（请求报文） 读取服务端返回的报文信息（响应报文） 关闭连接或者为后续请求重用连接 其中第一步和最后一步是对底层传输层协议的控制，不同的HTTP版本是不一样的 HTTP报文的结构（问你对HTTP报文是否了解） 请求报文 响应报文 2. HTTP缓存 缓存的作用 缓解服务器端压力，提升性能(获取资源的耗时更短了)。对于网站来说，缓存是达到高性能的重要组成部分。 什么是web缓存 缓存是一种保存资源副本并在下次请求时直接使用该副本的技术。当 web 缓存发现请求的资源已经被存储，它会拦截请求，返回该资源的拷贝，而不会去源服务器重新下载 HTTP缓存的分类 私有缓存 私有缓存只能用于单独用户，如用户浏览器中的缓存 共享缓存 共享缓存存储的响应能够被多个用户使用，如缓存代理服务器。 缓存什么样的资源 ​ HTTP缓存只能缓存GET方法请求的资源，其他方法响应的资源无能为力。 缓存如何控制 这个问题指的是怎么对资源进行缓存？ Cache-Controller请求头 禁止进行缓存 缓存中不得存储任何关于客户端请求和服务端响应的内容。每次由客户端发起的请求都会下载完整的响应内容。(就是说不使用缓存) Cache-Control: no-store 强制确认缓存 每次有请求发出时，缓存会将此请求发到服务器（译者注：该请求应该会带有与本地缓存相关的验证字段），服务器端会验证请求中所描述的缓存是否过期，若未过期（注：实际就是返回304），则缓存才使用本地缓存副本 Cache-Control: no-cache 私有缓存和公共缓存 “public” 指令表示该响应可以被任何中间人（译者注：比如中间代理、CDN等）缓存。若指定了”public”，则一些通常不被中间人缓存的页面（译者注：因为默认是private）（比如 带有HTTP验证信息（帐号密码）的页面 或 某些特定状态码的页面），将会被其缓存。 而 “private” 则表示该响应是专用于某单个用户的，中间人不能缓存此响应，该响应只能应用于浏览器私有缓存中。（设置的是缓存类型） Cache-Control: private Cache-Control: public 缓存过期机制 max-age表示资源能够被缓存的最大时间，也就是说资源多久之后会过期。（设置缓存过期时间） Cache-Control: max-age=31536000 Pragma头 Pragma 是HTTP/1.0标准中定义的一个header属性，请求中包含Pragma的效果跟在头信息中定义Cache-Control: no-cache相同，但是HTTP的响应头没有明确定义这个属性，所以它不能拿来完全替代HTTP/1.1中定义的Cache-control头。通常定义Pragma以向后兼容基于HTTP/1.0的客户端。（只用理解他是http1.0的东西，通常我们不用了解http1.0的具体细节） 新鲜度 缓存驱逐 理论上来讲，当一个资源被缓存存储后，该资源应该可以被永久存储在缓存中。由于缓存只有有限的空间用于存储资源副本，所以缓存会定期地将一些副本删除，这个过程叫做缓存驱逐。 什么是新鲜度 当服务器上面的资源进行了更新，那么缓存中的对应资源也应该被更新，由于HTTP是C/S模式的协议，服务器更新一个资源时，不可能直接通知客户端更新缓存，所以双方必须为该资源约定一个过期时间，在该过期时间之前，该资源（缓存副本）就是新鲜的。（简单的说就是这个资源还没过期就是新鲜的） 如何清除掉陈旧的资源 驱逐算法用于将陈旧的资源（缓存副本）替换为新鲜的，注意，一个陈旧的资源（缓存副本）是不会直接被清除或忽略的，当客户端发起一个请求时，缓存检索到已有一个对应的陈旧资源（缓存副本），则缓存会先将此请求附加一个If-None-Match头，然后发给目标服务器，以此来检查该资源副本是否是依然还是算新鲜的，若服务器返回了 304 (Not Modified)（该响应不会有带有实体信息），则表示此资源副本是新鲜的。 对于含有特定头信息的请求，会去计算缓存寿命。比如Cache-control: max-age=N的头，相应的缓存的寿命就是N。通常情况下，对于不含这个属性的请求则会去查看是否包含Expires属性，通过比较Expires的值和头里面Date属性的值来判断是否缓存还有效。如果max-age和expires属性都没有，找找头里的Last-Modified信息。如果有，缓存的寿命就等于头里面Date的值减去Last-Modified的值除以10 加速资源 更多地利用缓存资源，可以提高网站的性能和响应速度。为了优化缓存，过期时间设置得尽量长是一种很好的策略。对于定期或者频繁更新的资源，这么做是比较稳妥的，但是对于那些长期不更新的资源会有点问题。 如何解决长期不更新资源的更新问题 不频繁更新的文件会使用特定的命名方式：在URL后面（通常是文件名后面）会加上版本号。加上版本号后的资源就被视作一个完全新的独立的资源，同时拥有一年甚至更长的缓存过期时长。 问题 需要手动的修改每一个引用该资源的地方 缓存如何验证 什么时候开始缓存验证 用户点击刷新按钮 如果缓存的响应头信息里含有”Cache-control: must-revalidate”的定义，在浏览的过程中也会触发缓存验证 在浏览器偏好设置里设置Advanced-&gt;Cache为强制验证缓存也能达到相同的效果 ETags 客户端请求一个页面（A）。 服务器返回页面A，并在给A加上一个ETag。 客户端展现该页面，并将页面连同ETag一起缓存。 客户再次请求页面A，并将上次请求时服务器返回的ETag一起传递给服务器。 服务器检查该ETag，并判断出该页面自上次客户端请求之后还未被修改，直接返回响应304（未修改——Not Modified）和一个空的响应体。（能区分相同URL不同的对象） 带Vary头的响应 不同客户端对内容格式的支持程度不同（比如有些支持数据压缩，有些不支持），所以即便请求URL 和请求方法都相同，服务器返回的数据也会不同（称为内容协商）。Vary 字段记录了缓存服务器返回特定数据参考了哪些请求字段。缓存服务器拿到源服务器的响应报文，会根据 Vary 里的字段列表，缓存不同版本的数据。当客户端再次访问时，缓存服务器会分析请求字段，返回正确的版本。（这段话的意思是，相同的资源有不同的版本，如何正确返回） 3. HTTP Cookies Cookies是什么 HTTP Cookie（也叫Web Cookie或浏览器Cookie）是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie使基于无状态的HTTP协议记录稳定的状态信息成为了可能。（简单的说Cookies是存在客户端的会话管理技术） Cookies有什么用 会话状态管理 个性化设置 浏览器行为跟踪 如何设置Cookies 服务端通过如下请求头设置客户端的Cookues Set-Cookie: &lt;cookie名&gt;=&lt;cookie值&gt; Cookies的有效时间 会话期Cookies 会话期Cookie是最简单的Cookie：浏览器关闭之后它会被自动删除，不需要指定过期时间或者有效期。有些浏览器提供了会话恢复功能，这种情况下即使关闭了浏览器，会话期Cookie也会被保留下来，就好像浏览器从来没有关闭一样。（思考怎么办？） 持久性Cookies 和关闭浏览器便失效的会话期Cookie不同，持久性Cookie可以指定一个特定的过期时间（Expires）或有效期（Max-Age） 当Cookie的过期时间被设定时，设定的日期和时间只与客户端相关，而不是服务端。 Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Cookies的标记 Secure 标记为 Secure 的Cookie只应通过被HTTPS协议加密过的请求发送给服务端（但是敏感信息不能放在Cookies中，有的浏览器会禁止Secure这个标记） HttpOnly 如果包含服务端 Session 信息的 Cookie 不想被客户端 JavaScript 脚本调用，那么就应该为其设置 HttpOnly 标记（防止跨站脚本攻击XSS） Cookie的作用域 Domain 和 Path 标识定义了Cookie的作用域：即Cookie应该发送给哪些URL。 Domain Domain 标识指定了哪些主机可以接受Cookie。如果不指定，默认为当前文档的主机 Path Path 标识指定了主机下的哪些路径可以接受Cookie SameSite Cookie SameSite Cookie允许服务器要求某个cookie在跨站请求时不会被发送，从而可以阻止跨站请求伪造攻击（CSRF） None 浏览器会在同站请求、跨站请求下继续发送cookies，不区分大小写。 Strict 浏览器将只发送相同站点请求的cookie(即当前网页URL与请求目标URL完全一致)。如果请求来自与当前location的URL不同的URL，则不包括标记为Strict属性的cookie。 Lax 在新版本浏览器中，为默认选项，Same-site cookies 将会为一些跨站子请求保留，如图片加载或者frames的调用，但只有当用户从外部站点导航到URL时才会发送。如link链接 4. 跨域问题 同源策略 同源策略是浏览器的一个安全策略，所谓同源是指，域名，协议，端口相同。 同源策略是浏览器的行为，是为了保护本地数据不被JavaScript代码获取回来的数据污染，因此拦截的是客户端发出的请求回来的数据接收，即请求发送了，服务器响应了，但是无法被浏览器接收。 什么是跨域访问问题 为了用户浏览的安全，浏览器使用了同源策略，禁止浏览器在一个域中访问另一个域的资源，但是有的时候我们必须要访问不同域的问题，这样就造成了跨域问题。 如何解决跨域访问问题(之后重点讲) 跨域资源共享（CORS） jsonp nginx反向代理 CORS 跨域资源共享是一种机制，它使用额外的HTTP头来告诉浏览器 让运行在一个 origin上的Web应用被准许访问来自不同源服务器上的指定的资源。跨域资源共享机制允许 Web 应用服务器进行跨域访问控制，从而使跨域数据传输得以安全进行 请求分为两种，不同种类的请求处理不同 对于简单请求，浏览器直接发出CORS请求。具体来说，就是在头信息之中，增加一个Origin字段，Origin字段用来说明，本次请求来自哪个源（协议 + 域名 + 端口）。服务器根据这个值，决定是否同意这次请求，如果Origin指定的源，不在许可范围内，服务器会返回一个正常的HTTP回应。浏览器发现，这个回应的头信息没有包含Access-Control-Allow-Origin字段（详见下文），就知道出错了，从而抛出一个错误。 非简单请求是那种对服务器有特殊要求的请求，比如请求方法是PUT或DELETE，或者Content-Type字段的类型是application/json。非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为”预检”请求。一旦服务器通过了”预检”请求，以后每次浏览器正常的CORS请求，就都跟简单请求一样，会有一个Origin头信息字段。服务器的回应，也都会有一个Access-Control-Allow-Origin头信息字段 JSONP JSONP只支持GET请求，CORS支持所有类型的HTTP请求。JSONP的优势在于支持老式浏览器，以及可以向不支持CORS的网站请求数据。 5. HTTP协议的演变这一节讲的是HTTP的发展史从HTTP0.9到HTTP1.0再到HTTP1.1再到HTTP2.0,需要了解每个版本协议的特点。 HTTP0.9（单行协议，最早的版本，所以功能很少，不能以现在的眼光去看待这个协议） HTTP/0.9 极其简单：请求由单行指令构成，以唯一可用方法GET开头，其后跟目标资源的路径. GET /mypage.html 响应也极其简单的：只包含响应文档本身。 &lt;HTML&gt; 这是一个非常简单的HTML页面 &lt;/HTML&gt; HTTP1.0（构建扩展性，这个时候的HTTP协议已经有了现代协议的雏形了，但是没有完成标准化） 比上一代的优点 协议版本信息现在会随着每个请求发送（HTTP/1.0被追加到了GET行）。 状态码会在响应开始时发送，使浏览器能了解请求执行成功或失败，并相应调整行为（如更新或使用本地缓存）。 引入了HTTP头的概念，无论是对于请求还是响应，允许传输元数据，使协议变得非常灵活，更具扩展性。 在新HTTP头的帮助下，具备了传输除纯文本HTML文件以外其他类型文档的能 HTTP1.1（标准化协议，20多年后的今天依然还有很多人使用） 比上一代的优点 连接可以复用，节省了多次打开TCP连接加载网页文档资源的时间。 增加流水线操作，允许在第一个应答被完全发送之前就发送第二个请求，以降低通信延迟。（难以实现） 支持响应分块。 引入额外的缓存控制机制。 引入内容协商机制，包括语言，编码，类型等，并允许客户端和服务器之间约定以最合适的内容进行交换。 使用Host，能够使不同域名配置在同一个IP地址的服务器上。 HTTP2.0 这些年来，网页愈渐变得的复杂，甚至演变成了独有的应用，可见媒体的播放量，增进交互的脚本大小也增加了许多：更多的数据通过HTTP请求被传输。HTTP/1.1链接需要请求以正确的顺序发送，理论上可以用一些并行的链接（尤其是5到8个），带来的成本和复杂性堪忧。比如，HTTP流水线就成为了Web开发的负担。（这是说虽然http1.1流水线解决了某些问题，但是引入了复杂性，解决的不够优雅，自然HTTP2.0会重新解决这个问题） 特点 HTTP/2是二进制协议而不是文本协议。不再可读，也不可无障碍的手动创建，改善的优化技术现在可被实施。（改变了协议的格式） 这是一个复用协议。并行的请求能在同一个链接中处理，移除了HTTP/1.x中顺序和阻塞的约束。（连接复用，并行的请求可以在同一个连接中完成） 压缩了headers。因为headers在一系列请求中常常是相似的，其移除了重复和传输重复数据的成本。（压缩了头部，将报文段的长度减少了） 其允许服务器在客户端缓存中填充数据，通过一个叫服务器推送的机制来提前请求。（引入服务端推送机制） HTTPS HTTP的版本解决的是HTTP协议的效率问题，每一代都有效率上的提升，但是没有解决安全问题，HTTPs就是来解决安全的。 HTTP协议遇到的安全问题 篡改 监听 伪造 简单的说HTTPS就是在HTTP协议之下TCP之上引入了TLS协议 6. HTTP消息这一节讲的是HTTP报文的格式，这是学习任何一个协议都要注意的问题 HTTP请求报文 请求行 HTTP请求是由客户端发出的消息，用来使服务器执行动作。起始行 (start-line) 包含三个元素 请求方法 请求目标（URL） HTTP版本 请求头 分为三种类型 通用头部 请求头部 实体头部 请求体 有些请求将数据发送到服务器以便更新数据：常见的的情况是 POST 请求（包含 HTML 表单数据） HTTP响应报文 响应行 HTTP 响应的起始行被称作 状态行 (status line)，包含以下信息： 协议版本 状态码 状态文本信息 HTTP/1.1 404 Not Found 响应头 响应体 响应的最后一部分是 body。不是所有的响应都有 body：具有状态码 (如 201 或 204) 的响应，通常不会有 body HTTP2帧格式 HTTP/1.x 报文有一些性能上的缺点： Header 不像 body，它不会被压缩。 两个报文之间的 header 通常非常相似，但它们仍然在连接中重复传输。 无法复用。当在同一个服务器打开几个连接时：TCP 热连接比冷连接更加有效 HTTP/2 引入了一个额外的步骤：它将 HTTP/1.x 消息分成帧并嵌入到流 (stream) 中。数据帧和报头帧分离，这将允许报头压缩。将多个流组合，这是一个被称为 多路复用 (multiplexing) 的过程，它允许更有效的底层 TCP 连接。 HTTP 帧现在对 Web 开发人员是透明的。在 HTTP/2 中，这是一个在 HTTP/1.1 和底层传输协议之间附加的步骤。Web 开发人员不需要在其使用的 API 中做任何更改来利用 HTTP 帧；当浏览器和服务器都可用时，HTTP/2 将被打开并使用。 6. 经典的HTTP会话 建立连接 发送客户端请求 服务器响应 7.HTTP1.x的连接管理连接管理是一个 HTTP 的关键话题：打开和保持连接在很大程度上影响着网站和 Web 应用程序的性能。在 HTTP/1.x 里有多种模型：短连接, 长连接, 和 HTTP 流水线。 短链接 HTTP 最早期的模型，也是 HTTP/1.0 的默认模型，是短连接。每一个 HTTP 请求都由它自己独立的连接完成；这意味着发起每一个 HTTP 请求之前都会有一次 TCP 握手，而且是连续不断的。 TCP 协议握手本身就是耗费时间的，所以 TCP 可以保持更多的热连接来适应负载。短连接破坏了 TCP 具备的能力，新的冷连接降低了其性能。 这是 HTTP/1.0 的默认模型(如果没有指定 Connection协议头，或者是值被设置为 close)。而在 HTTP/1.1 中，只有当Connection被设置为 close 时才会用到这个模型。 长连接 短连接有两个比较大的问题：创建新连接耗费的时间尤为明显，另外 TCP 连接的性能只有在该连接被使用一段时间后(热连接)才能得到改善（这也是影响性能的一个因素不能忘记）。为了缓解这些问题，长连接 的概念便被设计出来了，甚至在 HTTP/1.1 之前。或者这被称之为一个 keep-alive 连接。 一个长连接会保持一段时间，重复用于发送一系列请求，节省了新建 TCP 连接握手的时间，还可以利用 TCP 的性能增强能力。当然这个连接也不会一直保留着：连接在空闲一段时间后会被关闭(服务器可以使用 Keep-Alive 协议头来指定一个最小的连接保持时间。（这里说明了HTTP长连接怎样提高效率的） 长连接也还是有缺点的；就算是在空闲状态，它还是会消耗服务器资源，而且在重负载时，还有可能遭受 DoS attacks攻击。这种场景下，可以使用非长连接，即尽快关闭那些空闲的连接，也能对性能有所提升。（这段话说明长连接潜在的问题） 在 HTTP/1.1 里，默认就是长连接的，协议头都不用再去声明它。 HTTP流水线 默认情况下，HTTP请求是按顺序发出的。下一个请求只有在当前请求收到应答过后才会被发出。由于会受到网络延迟和带宽的限制，在下一个请求被发送到服务器之前，可能需要等待很长时间。 流水线是在同一条长连接上发出连续的请求，而不用等待应答返回。这样可以避免连接延迟 目前没有现代浏览器默认启用这个特性，流水线很复杂，难以实现 域名分片 这是一种过时技术，如果用这个机制不如升级到HTTP2.0 这种机制的理论是：浏览器为每个域名的连接都是有限的，如果要加快访问速度，可以想到的就是为每个域名增加连接数量，但是这无法实现（无法改变浏览器的行为），一种妥协的方法是，将同一个域名的资源拆分到不同的域中，这样连接数量就可以提高，从而提高性能，但是这种解决方法不够优雅。 例子：如果服务器端想要更快速的响应网站或应用程序的应答，它可以迫使客户端建立更多的连接。例如，不要在同一个域名下获取所有资源，假设有个域名是 www.example.com，我们可以把它拆分成好几个域名：www1.example.com、www2.example.com、www3.example.com。所有这些域名都指向同一台服务器，浏览器会同时为每个域名建立 6 条连接(在我们这个例子中，连接数会达到 18 条)。这一技术被称作域名分片。 8. HTTP首部 太多了所以不用记住，只要熟悉常用的就要可以，或者是为了实现某种机制需要的请求头，如设置cookie，缓存控制。 HTTP首部 8. HTTP请求方法差不多是面试中问道HTTP必问的问题，需要属性每种方法并且知道他们的区别，还有restful风格api的对应关系，以及每种方法的幂等性，什么是幂等性等问题（京东一面又问到） GET POST PUT PETCH HEAD DELETE CONNECT OPTIONS TRACE 9. HTTP状态码 HTTP 响应状态代码指示特定 HTTP请求是否已成功完成。响应分为五类：信息响应(100–199)，成功响应(200–299)，重定向(300–399)，客户端错误(400–499)和服务器错误 (500–599)。 记住常用的几个状态码，以及某些状态码的区别如同样是重定向301和304的区别 301与304的相同点和不同点 301和304都表示地址重定向，也就是当访问一个url会被指向另一个url，但是301重定向是永久的移动，304是临时重定向。 参考： HTTP协议文档 HTTP状态码 掘金文章参考文章","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"应用层","slug":"计算机网络/应用层","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E5%BA%94%E7%94%A8%E5%B1%82/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"HTTP协议","slug":"HTTP协议","permalink":"https://www.severin.xyz/tags/HTTP%E5%8D%8F%E8%AE%AE/"}]},{"title":"HTTPs协议抓包探究","slug":"Https","date":"2019-12-21T16:00:00.000Z","updated":"2020-02-27T08:08:35.563Z","comments":true,"path":"2019/12/22/Https/","link":"","permalink":"https://www.severin.xyz/2019/12/22/Https/","excerpt":"","text":"2. TLS握手过程抓包分析 客户端向服务端发送Client Hello Client Hello中携带了当前客户端支持的TLS协议的版本号、客户端支持的加密套件、客户端支持的压缩算法以及一个随机数Random_C 服务端向客户端发送Sever Hello 服务端协商返回确定的信息，如确定使用哪种加密套件（Cipher Suites）或压缩方法等。，以及一个随机生成的序列Random_S 服务端向客户端发送Certificate 服务器向客户端发送自己的证书 客户端收到证书后会校验证书的合法性，合法后进行之后的操作 服务端向客户端发送Server key change，Server Hello Done 服务器端通过Server key change返回给客户端相关D-H算法参数，这些参数后期客户端可以算出会话密钥，如果使用的是RSA算法，那么这一步是不需要的。传递完参数之后，发送Server Hello Done告诉客户端服务器端的握手结束了 客户端向服务端发送Client Key Exchange, Change Cipher Spec, Encrypted Handshake Message 客户端根据第四步传过来的公钥，生成一个叫预备-主密钥的pre-master key。 Client Key Exchange将这个预备-主密钥传给服务器端。服务器端结合自己的私钥解出这个预备-主密钥的信息，得到第三个随机数，所以，到目前为止，客户端和服务器都拥有 Random_C + Random_S + Pre_master key Change Cipher Spec这一步是告诉服务器端后期的通信都会使用我们协商出来的密钥进行通信。 Encrypted Handshake Message是客户端将前面的握手消息生成摘要再用协商好的秘钥加密（对称加密），这是客户端发出的第一条加密消息。服务端接收后会用秘钥解密，能解出来说明前面协商出来的秘钥是一致的。 服务端向客户端发送Change Cipher Spec, Encrypted Handshake Message 如果服务器端通过D-H算法能够解密摘要，那么服务器端应该告诉客户端说我们之间协商的会话密钥是一致的。 3. DH算法DH算法是一种可以在不信任通道进行秘钥交换的协议 4. 非对称加密、对称加密算法对比 非对称加密秘钥有两个，加密使用公钥，解密使用私钥。对称加密算法使用同一个秘钥进行加密和解密 对称加密不能在不受信任的通道中使用，非对称可以 非对称加密算法计算量大开销大，对称加密计算量少开销小 5. 如何验证身份 申请者通过非对称加密算法（RSA）生成一对公钥和密钥，然后把需要的申请信息（国家，域名等）连同公钥发送给 证书认证机构（CA） CA构确认无误后通过消息摘要算法（MD5，SHA) 生成整个申请信息的摘要签名M， 然后 把 签名M和使用的摘要算法 用 CA自己的私钥 进行加密 证书包含了 公钥 证书拥有者身份信息 数字证书认证机构（发行者）信息 发行者对这份文件的数字签名及使用的算法 有效期 通过 CA的公钥 去解密得到证书的签名摘要的。 再次用 相同的摘要算法（证书里面有保存所使用的算法）对整个证书做签名，如果得到的签名和证书上的签名是一致的，说明这个证书确实是该信任的CA颁发的。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"应用层","slug":"计算机网络/应用层","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E5%BA%94%E7%94%A8%E5%B1%82/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"HTTP协议","slug":"HTTP协议","permalink":"https://www.severin.xyz/tags/HTTP%E5%8D%8F%E8%AE%AE/"}]},{"title":"三大查找算法","slug":"三大查找算法","date":"2019-12-20T16:00:00.000Z","updated":"2020-01-30T06:04:15.997Z","comments":true,"path":"2019/12/21/三大查找算法/","link":"","permalink":"https://www.severin.xyz/2019/12/21/%E4%B8%89%E5%A4%A7%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/","excerpt":"","text":"1. 二分查找 算法思路 已知数组是有序的，假设是升序的，取中间位置的数字与目标数字进行比较，如果相等那么就找到了，如果中间位置大，目标数字就在右边，下一步只用在数组左半部分查找，假如中间的数字小，则目标数字一个在数组的右边，下一步只用在数组右半部查找。 时间复杂度 O(logn) 代码 public class BinarySearch { public static void main(String[] args) { int[] arr={1,2,3,4,5,6,7,8,9}; int search = search(arr, 4); System.out.println(search); } private static int search(int[] arr,int target){ int start=0; int end=arr.length-1; while (start&lt;=end){ int mid=start+(end-start)/2; if (arr[mid]==target){ return mid; }else if (arr[mid]&gt;target){ end=mid-1; }else { start=mid+1; } } return -1; } } 扩展 如果要求找到与某个数字最接近的数字也可以通过二分查找找到。 2. 分块查找也称索引顺序查找 3. 哈希查找 算法思想 哈希查找是通过计算数据元素的存储地址进行查找的一种方法。在插入元素的时候通过hash函数根据键的值计算出插入的下标位置，元素存放在该下标位置的数组中，如果冲突了可以采用拉链法或者开放地址法解决，查找的时候只需根据键计算出插入位置，然后就能查找到元素。 时间复杂度 O(1) 代码 哈希查找算法的实现在于哈希类的实现，哈希的实现有两个关键：一是hash函数如何选择？而是哈希冲突如何解决？这两个问题很可能会问道，建议百度了解，已经可能会提到优化，比如说java1.8的优化。 public class HashSearch { public static void main(String[] args) { Hash hash=new Hash(); hash.put(1,1); hash.put(2,2); hash.put(3,3); System.out.println(hash.get(1)); } } /** * 实现一个简单的哈希类,采用拉链法解决hash冲突 */ class Hash{ private Node[] tables=new Node[10]; public void put(int key,int value){ Node last = getNode(key); if (last!=null){ last.value=value; return; } int index = hash(key); if (tables[index]==null){ tables[index]=new Node(key,value); }else{ Node node = new Node(key, value); node.next=tables[index]; tables[index]=node; } } private Node getNode(int key){ int index = hash(key); if (tables[index]==null){ return null; } Node p=tables[index]; while (p!=null){ if (p.key==key){ return p; } p=p.next; } return null; } public Integer get(int key){ Node node = getNode(key); if (node==null){ return null; } return node.value; } private int hash(int key){ return key%10; } private static class Node{ int key; int value; Node next; public Node(int key, int value) { this.key = key; this.value = value; } } }","categories":[{"name":"经典算法","slug":"经典算法","permalink":"https://www.severin.xyz/categories/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/"},{"name":"查找","slug":"经典算法/查找","permalink":"https://www.severin.xyz/categories/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E6%9F%A5%E6%89%BE/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://www.severin.xyz/tags/%E7%AE%97%E6%B3%95/"},{"name":"查找","slug":"查找","permalink":"https://www.severin.xyz/tags/%E6%9F%A5%E6%89%BE/"}]},{"title":"十大排序算法分析","slug":"十大排序算法分析","date":"2019-12-19T16:00:00.000Z","updated":"2020-01-29T11:08:10.550Z","comments":true,"path":"2019/12/20/十大排序算法分析/","link":"","permalink":"https://www.severin.xyz/2019/12/20/%E5%8D%81%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/","excerpt":"","text":"排序算法是面试中常问的算法，大厂中排序算法问的深度很深，对排序算法的理解有多个层次 知道常用算法的写法，了解各种算法的时间复杂度，空间复杂度和稳定性 了解每种算法的性能瓶颈 对于每种算法知道如何优化 知道每种算法的应用场景 1. 选择排序 算法思想 将一组数据分为两部分，前面是已排序部分，后面是未排序部分，初始状态可认为位置 0 为已排序部分 (数组下标从0开始)，其余为未排序部分，每一次都从未排序部分选择一个最小元素放在已排序部分的末尾，然后已排序部分增加一个元素，未排序部分减少一个元素，直到数据全部有序。 时间复杂度 选择排序无论数据初始是何种状态，均需要在未排序元素中选择最小或最大元素与未排序序列中的首尾元素交换，因此它的最好、最坏、平均时间复杂度均为 O(n^2)。 空间复杂度 空间复杂度为O(1) 稳定性 直接选择排序是不稳定的。因为每次遍历比较完后会使用本次遍历选择的最小元素和无序区的第一个元素交换位置，所以如果无序区第一个元素后面有相同元素的，则可能会改变相同元素的相对顺序（稳定性：能保证两个相等的数,经过排序之后,其在序列的前后位置顺序不变） 优化思路 每次查找时不仅找出最小值，还找出最大值，分别插到前面和后面，可以减少一半的查询时间。 如果数组元素重复率高，可以考虑使用辅助空间在每一次循环的时候，将本次选择的数及相同元素的索引记录下来，一起处理。 代码 public class SelectionSort { public static void main(String[] args) { int[] arr={1,4,3,2,3,2,1,2,6}; sort(arr); for (int i : arr) { System.out.print(i+&quot; &quot;); } } public static void sort(int[] arr){ for (int i=0;i&lt;arr.length-1;i++){ int minPos=i; for (int j = minPos+1; j &lt; arr.length; j++) { if (arr[j]&lt;arr[minPos]){ minPos=j; } } swap(arr,i,minPos); } } private static void swap(int[] arr,int i,int j){ int temp=arr[i]; arr[i]=arr[j]; arr[j]=temp; } } 2. 冒泡排序 算法思想 通过比较相邻的两个元素，将大的元素或者小的元素交换到后面，这样越大或者越小的元素都会交换到数组的后端。 时间复杂度 时间复杂度是O(n^2) 空间复杂度 空间复杂度为O(1) 稳定性 稳定 优化思路 用一个计数器记录交换的次数，当某一轮交换次数为0则表示数组已经有序，那么就不用继续进行了。 记录最后一次交换的位置，该位置之后没有进行交换说明是有序的了，下一轮只用遍历该位置即可。 代码 public class BubbleSort { public static void main(String[] args) { int[] arr={1,4,23,2,1,1,23,2,1,8}; sort(arr); for (int i=0;i&lt;arr.length;i++){ System.out.print(arr[i]+&quot; &quot;); } } public static void sort(int[] arr){ for (int i=0;i&lt;arr.length-1;i++){ for (int j=0;j&lt;arr.length-i-1;j++){ if (arr[j]&gt;arr[j+1]){ swap(arr,j,j+1); } } } } private static void swap(int[] arr,int i,int j){ int temp=arr[i]; arr[i]=arr[j]; arr[j]=temp; } } 3. (直接)插入排序 算法思想 每趟将一个元素，按照其关键字的大小插入到它前面已经排序的子序列中，依此重复，直到插入全部元素。 时间复杂度 时间复杂度为O(n^2) 空间复杂度 空间复杂度为O(1) 稳定性 稳定 优化思路 希尔排序 二分查找插入排序 二分查找插入排序的原理：是直接插入排序的一个变种，区别是：在有序区中查找新元素插入位置时，为了减少元素比较次数提高效率，采用二分查找算法进行插入位置的确定。 代码 public class InsertionSort { public static void main(String[] args) { int[] arr={1,5,4,3,2,6,7,8,9}; sort(arr); for (int i : arr) { System.out.print(i+&quot; &quot;); } } public static void sort(int[] arr){ for (int i=0;i&lt;arr.length;i++){ int temp=arr[i]; int j=i-1; for (;j&gt;=0&amp;&amp;arr[j]&gt;temp;j--){ arr[j+1]=arr[j]; } arr[j+1]=temp; } } } 4. 希尔排序 算法思路 希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。 时间复杂度 ​ 时间复杂度取决于增量序列的选择O(n^1.3) 空间复杂度 稳定性 不稳定 代码 public class ShellSort { public static void main(String[] args) { int[] arr={1,3,2,4,6,5,7,9,8,0}; sort(arr); for (int i : arr) { System.out.print(i+&quot; &quot;); } } public static void sort(int [] arr){ // 增量序列 int[] ds={1,3,7}; for (int i=ds.length-1;i&gt;=0;i--){ insertSort(arr,1); } } /** * 按增量分组进行直接插入排序 * @param arr 数组 * @param d 增量 */ private static void insertSort(int[] arr,int d){ for (int i=0;i&lt;d;i++){ for (int j=i;j&lt;arr.length;j+=d){ int k=j-d; int temp=arr[j]; for (;k&gt;=i&amp;&amp;arr[k]&gt;temp;k-=d){ arr[k+d]=arr[k]; } arr[k+d]=temp; } } } } 5. 堆排序 算法思路 堆是这样一种数据结构，首先堆是一个完全二叉树，其父节点一定大于其所有的子节点。堆排序就是利用堆这种数据结构进行排序，首先构造一个堆，堆顶元素就是最大或最小的元素，把他与堆的最后一个元素交换，这样堆顶元素就调整到了排序后的顺序，而堆的元素个数减少了一个且结构发生了变化，只需要重新调整堆就可以了。循环这个步骤数组就变成有序的了。 时间复杂度 nlogn -&gt; n表示第几轮 logn为调整堆的时间复杂度 空间复杂度 O(1) 稳定性 不稳定 代码 public class HeapSort { public static void main(String[] args) { int[] arr={1,4,3,2,5,7,6,8,9,0}; sort(arr); for (int i : arr) { System.out.print(i+&quot; &quot;); } } public static void sort(int[] arr){ // 首先要对整个数组进行heapify操作 for (int i=arr.length/2;i&gt;=0;i--){ heapify(arr,arr.length,i); } for (int i=1;i&lt;=arr.length;i++){ swap(arr,0,arr.length-i); heapify(arr,arr.length-i,0); } } private static void swap(int[] arr,int i,int j){ int temp=arr[i]; arr[i]=arr[j]; arr[j]=temp; } private static void heapify(int[] arr,int len,int i){ int left=i*2+1; int right=i*2+2; int max=i; if (left&lt;len&amp;&amp;arr[left]&gt;arr[max]){ max=left; } if (right&lt;len&amp;&amp;arr[right]&gt;arr[max]){ max=right; } if (max!=i){ swap(arr,i,max); heapify(arr,len,max); } } } 6. 归并排序 算法思路 基于二路归并算法，将数组分成两个部分，对每一部分递归采取归并排序，这样数组两个部分就排好序了，对于两个已排好序的数组，只需要进行二路归并就可以得到一个有序的数组。 时间复杂度 O(nlogn) 空间复杂度 O(n) 稳定性 稳定 优化思路 原地归并 因为用归并将一个大数组排序时，需要进行多次归并，而且每次归并会都创建一个新数组来存储排序结果会带来问题。由于原地归并排序不需要额外的空间，所以空间复杂度为O(1)。 当递归到规模足够小时，利用插入排序 代码 public class MergeSort { private static int[] tempArr; public static void main(String[] args) { int[] arr={1,2,5,4,3,6,9,7,8,0}; sort(arr); for (int i : arr) { System.out.print(i+&quot; &quot;); } } public static void sort(int[] arr){ tempArr=new int[arr.length]; sort(arr,0,arr.length-1); } private static void sort(int[] arr,int start,int end){ if (start==end){ return; } int mid=start+(end-start)/2; sort(arr,start,mid); sort(arr,mid+1,end); merge(arr,start,mid,end); } private static void merge(int[] arr,int start,int mid,int end){ System.arraycopy(arr,0,tempArr,0,arr.length); int i=start; int j=mid+1; int k=start; while (i&lt;=mid&amp;&amp;j&lt;=end){ if (tempArr[i]&lt;tempArr[j]){ arr[k++]=tempArr[i++]; }else{ arr[k++]=tempArr[j++]; } } while (i&lt;=mid){ arr[k++]=tempArr[i++]; } while (j&lt;=end){ arr[k++]=tempArr[j++]; } } } 7. 快速排序 算法思路 快速排序使用分治法策略来把一个序列分为较小和较大的2个子序列，然后递归地排序两个子序列。具体步骤是选择一个元素作为基准元素，将比它小的元素放置在它的左边，将比它大的元素排它的后面，这样基准元素的位置就确定了，然后分别对基准元素左边和右边的序列进行相同操作，每次都能将一个元素排放到正确的位置。 时间复杂度 O(nlogn) 递归的过程O(n) partition过程O(n)-&gt;O(nlogn) 空间复杂度 空间复杂度为logn 优化思路 代码 public class QuickSort { public static void main(String[] args) { int[] arr={1,4,2,3,6,5,7,9,8,0}; sort(arr); for (int i : arr) { System.out.print(i+&quot; &quot;); } } public static void sort(int[] arr){ quickSort(arr,0,arr.length-1); } private static void quickSort(int[] arr,int start,int end){ if (start&gt;=end){ return; } int pos=partition(arr,start,end); quickSort(arr,start,pos-1); quickSort(arr,pos+1,end); } private static int partition(int[] arr,int start,int end){ int left=start-1; int k=start; while (k&lt;=end){ if(arr[k]&gt;arr[start]){ k++; }else{ swap(arr,++left,k++); } } swap(arr,start,left); return left; } private static void swap(int[] arr,int i,int j){ int temp=arr[i]; arr[i]=arr[j]; arr[j]=temp; } } 8. 桶排序 算法思路 这是一种算法思想，基于非比较的排序算法，时间复杂度比较低但是一般需要额外的空间，将数组中的元素分配到不同的桶，桶与桶之间是有顺序的，桶的内部元素无序，对每个不为空的桶进行排序（可以使用别的排序算法），然后将不为空的桶中的元素进行输出就可以完成排序。 代码 public class BucketSort { public static void main(String[] args) { int[] arr={1,3,2,5,4,6,9,8,7,0}; sort(arr); for (int i : arr) { System.out.printf(i+&quot; &quot;); } } public static void sort(int[] arr){ int max=arr[0]; int min=arr[0]; for (int value : arr) { min = Math.min(value, min); max = Math.max(value, max); } ArrayList&lt;Integer&gt;[] buckets=new ArrayList[max/10-min/10+1]; for (int i=0;i&lt;buckets.length;i++){ buckets[i]=new ArrayList&lt;Integer&gt;(); } for (int value : arr) { buckets[(value-min)/10].add(value); } for (int i=0;i&lt;buckets.length;i++){ if (buckets[i].size()!=0){ Collections.sort(buckets[i]); } } int k=0; for (int i=0;i&lt;buckets.length;i++){ if (buckets[i].size()!=0){ for (int j=0;j&lt;buckets[i].size();j++){ arr[k++]=buckets[i].get(j); } } } } } 9. 基数排序 算法思想 基数排序是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。是桶排序思想的一种。是一种多关键字排序。 代码 public class RadixSort { public static void main(String[] args) { int[] arr={123,43,231,24,56,432,124}; sort(arr); for (int i : arr) { System.out.print(i+&quot; &quot;); } } public static void sort(int[] arr){ int[] result=new int[arr.length]; int[] count=new int[10]; int maxLen=maxLen(arr); // 分别对十位，个位，百位......进行排序 for (int i=0;i&lt;=maxLen;i++){ int temp= (int) Math.pow(10,i); for (int value : arr) { count[value / temp % 10]++; } for (int j=1;j&lt;count.length;j++){ count[j]+=count[j-1]; } for (int j=arr.length-1;j&gt;=0;j--){ result[--count[arr[j]/temp%10]]=arr[j]; } System.arraycopy(result,0,arr,0,arr.length); Arrays.fill(count,0); } } private static int maxLen(int[] arr){ int max=arr[0]; for (int i : arr) { max=Math.max(i,max); } int len=0; while (max!=0){ max/=10; len++; } return len; } } 10. 计数排序 算法思路 准备一个桶，桶的长度为n，且待排序数组的范围都在0到n-1之间，这样数组中的数i就可以放入桶的第i项，桶只要记住每个数字出现的次数，然后扫描桶就可以得到排序后的序列，适合数组范围不大的元素。 代码 public class CountSort { public static void main(String[] args) { int[] arr={1,3,2,4,5,6,8,7,9,0}; sort(arr); for (int i : arr) { System.out.print(i+&quot; &quot;); } } private static void sort(int[] arr){ int[] count=new int[10];// 计数数组，数组的数的范围落在0-count.length-1 for (int value : arr) { count[value]++; } int k=0; for (int i=0;i&lt;count.length;i++){ while (count[i]&gt;0){ count[i]--; arr[k++]=i; } } } } 11. 常用排序一览表 12. 常见面试问题总结 告诉你某种算法，比如很明确的问你某种算法然后不断追问，如快速排序（字节跳动一面） 说一下快速排序的思想（先考你知不知道思想） 快速排序的时间复杂度/空间复杂度/稳定(考一下你会不会分析算法或者说这个算法是不是你背下来的实际上你并不知道或不理解) 快速排序不适合什么样的数据（考你某种算法的缺点，进一步看你理不理解这个算法） 如果要排上面的算法，怎么优化（基于某个问题，要求你进行优化，这类问题最深应该就问到这一步了，后面也问不下去了） 不明确告诉你某种算法，问你排某一类特征的数据应该用什么算法排序(字节跳动一面) 这类问题难在要逆向思考，你得对所有算法都得足够的熟悉 问题：排一组比较有序的数组用什么算法，为什么可以用这种算法 13.一些其他排序算法 置换选择排序 枚举排序 14. 总结 一共有四种不稳定的排序算法，选择排序，堆排序，希尔排序，快速排序，其他的都是稳定的排序。 数据的特征大概有：数据基本有序，数据范围不大，数据逆序 分析数据的时间复杂度必须知道他的循环嵌套关系，以及每层循环的时间复杂度，优化也是这么思考的，看每一层循环这么优化 空间复杂度比较好分析，但是注意递归的情况，比如快速排序的空间时间复杂度不是O(1)而是logn(上面的表示错误的) 要理解算法的稳定性有什么影响，不稳定为什么不好？","categories":[{"name":"经典算法","slug":"经典算法","permalink":"https://www.severin.xyz/categories/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/"},{"name":"排序","slug":"经典算法/排序","permalink":"https://www.severin.xyz/categories/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://www.severin.xyz/tags/%E7%AE%97%E6%B3%95/"},{"name":"排序","slug":"排序","permalink":"https://www.severin.xyz/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"java虚拟机问题总结","slug":"java虚拟机问题总结","date":"2019-12-18T16:00:00.000Z","updated":"2020-02-29T12:01:15.649Z","comments":true,"path":"2019/12/19/java虚拟机问题总结/","link":"","permalink":"https://www.severin.xyz/2019/12/19/java%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/","excerpt":"","text":"java虚拟机问题总结一. 自动内存管理机制1. java虚拟机运行时数据区有哪些，各自的功能 程序计数器 程序计数器是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 java堆 此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存，由于栈上分配、标量替换技术的存在，对象不一定都在堆中分配。 方法区 用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 虚拟机栈 每个方法在执行的同时都会创建一个栈帧用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 本地方法栈 本地方法栈与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行Java方法服务，而本地方法栈则为虚拟机使用到的Native方法服务。 2. 永久代和元空间的概念在JDK1.8之前方法区被称为“永久代”，原因是当时将堆划分出一块永久代来实现方法区，这样垃圾收集器就可以像管理java堆来管理方法区，而到了jdk1.8，则是使用本地空间实现的元空间来代替永久代。 3. 什么是运行时常量池，存放什么运行时常量池是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 4.什么是直接内存，有什么用直接内存并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域。 在JDK 1.4中新加入了NIO类，引入了一种基于通道与缓冲区的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。 本机直接内存的分配不会受到Java堆大小的限制，但是，既然是内存，肯定还是会受到本机总内存限制。 5. 对象是如何创建的 虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。 在类加载检查通过后，接下来虚拟机将为新生对象分配内存 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值 接下来，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息存放在对象的对象头之中。根据虚拟机当前的运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 执行new指令之后会接着执行＜init＞方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 6.对象在内存中是怎样分配的一个对象在内存中由三部分组成：对象头，实例数据，对齐填充。 HotSpot虚拟机的对象头包括两部分信息组成，第一部分用于存储对象自身的运行时数据，这部分数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，官方称它为“Mark Word”。对象头的另外一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据。 实例数据部分是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。无论是从父类继承下来的，还是在子类中定义的，都需要记录起来。 对象的大小必须是8字节的整数倍。而对象头部分正好是8字节的倍数（1倍或者2倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 7. 对象分配的方式 指针碰撞（非线程安全） 假设Java堆中内存是绝对规整的，所有被使用过的内存都被放在一边，空闲的内存被放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间方向挪动一段与对象大小相等的距离，这种分配方式称为“指针碰撞”。 CAS同步处理（线程安全） 对分配内存空间的动作进行同步处理，虚拟机采用CAS配上失败重试的方式保证更新操作的原子性。 TALB（线程安全） 把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local AllocationBuffer，TLAB），哪个线程要分配内存，就在哪个线程的本地缓冲区中分配，只有本地缓冲区用完了，分配新的缓存区时才需要同步锁定。 8. 如何定位到一个对象，这些方法有什么优点和缺点有两种方式，一种是直接指针访问，一种是对象句柄访问。如果使用句柄访问的话，那么Java堆中将会划分出一块内存来作为句柄池，对象引用中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息。直接指针访问，对象引用存放了对象的真实地址。直接指针引用的优点是定位快，但是缺点是，当对象移动时需要修改对象引用的内容，句柄访问的缺点是需要两次定位才能找到对象，但是优点是对象移动，不需要改变对象引用的内容。使用句柄访问的方式更加常见。 9. JVM中可能发生的OOM的情况什么是OOM，OOM的全称是OutOfMemory，当申请的内存太大，java虚拟机无法满足我们的时候就会抛出OOM异常。 java运行时数据区只有程序计数器不会发生OOM，其他区域都有可能发生OOM 方法区OOM java堆OOM 虚拟方法栈OOM 本地方法栈OOM 本机直接内存OOM 10. 如何确定哪些内存需要回收主要两种方法，一种是引用计数算法，一种是可达性分析算法。 引用计数算法：对象都拥有一个计数器，当有一个地方引用该对象则计数器加1,引用失效就减1，当计数器的值为0的时候，这个对象就需要被回收 可达性分析算法：通过一系列的称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。 11. 哪些对象可以作为GC ROOT基本上就是java代码可以写出来的对象引用 虚拟机栈中的对象（本地变量） 方法区静态属性引用的对象(静态变量) 方法区中常量引用的对象（常量） 本地方法栈中JNI(native方法)引用的对象 12. 引用有哪些类型 强引用 强引用就是指在程序代码之中普遍存在的，类似“Object obj=new Object（）”这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象 软引用 软引用是用来描述一些还有用但并非必需的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。如果这次回收还没有足够的内存，才会抛出内存溢出异常 弱引用 弱引用也是用来描述非必需对象的，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。 虚引用 虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知 13. finalize方法的作用一个对象被标记为不可达的时候，并不意味着一定会回收这个对象，还需要经过一次筛选，筛选的条件是这个对象的finalize方法是否有必要执行。。当对象没有覆盖finalize（）方法，或者finalize（）方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行“。当一个对象被判断有必要执行，会把这个对象放置在一个做作F-QUEUE的队列中，并在稍后由一个由虚拟机自动建立的、低优先级的Finalizer线程去执行它。如果在finalize方法中重新把这个对象赋到对象引用链中，这个对象就不会被回收。 14. 方法区是否会被回收，回收的内容是什么，什么时候需要回收方法区java虚拟机规范中没有定义必须回收方法区，并且回收方法区的收益非常低。主要回收两部分内容：废弃常量和无用的类。一个无用的类是指：该类所有的实例都已经被回收，也就是Java堆中不存在该类的任何实例。加载该类的ClassLoader已经被回收。该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 在大量使用反射、动态代理、CGLib等ByteCode框架、动态生成JSP以及OSGi这类频繁自定义ClassLoader的场景都需要虚拟机具备类卸载的功能，以保证永久代不会溢出。 15. 有哪些垃圾收集算法 标记清除算法 最基础的收集算法是“标记-清除”（Mark-Sweep）算法，如同它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。 标记清除算法可能会造成大量的内存碎片 标记整理算法 标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 复制算法 将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 这种方法的缺点就是浪费了空间。 s实际上：将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。当回收时，将Eden和Survivor中还存活着的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。HotSpot虚拟机默认Eden和Survivor的大小比例是8:1，也就是每次新生代中可用内存空间为整个新生代容量的90%（80%+10%），只有10%的内存会被“浪费”。 分代收集算法 这种算法并没有什么新的思想，只是根据对象存活周期的不同将内存划分为几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记—清理”或者“标记—整理”算法来进行回收。 16. Partial GC、Minor GC、Major GC、Mixed GC、Full GC 名词 含义 Partial GC 不是完整收集整个Java堆的垃圾收集，分为：Minor GC，Major GC和Mixed GC Minor GC 指目标只是新生代的垃圾收集，Young GC Major GC 指目标只是老年代的垃圾收集，Old GC，这个概念有点资料指整堆收集 Mixed GC 指目标是收集整个新生代以及部分老年代的垃圾收集。目前G1收集器会有这种行为 Full GC 收集整个Java堆和方法区的垃圾收集。 17. Remember Set通常将java堆划分为不同的区域，每次只对一个区域进行回收。假如要现在进行一次只局限于新生代区域内的收集（Minor GC），但新生代中的对象是完全有可能被老年代所引用的，为了找出该区域中的存活对象，不得不在固定的GC Roots之外，再额外遍历整个老年代中所有对象来确保可达性分析结果的正确性，反过来也是一样。遍历整个老年代所有对象的方案虽然理论上可行，但无疑会为内存回收带来很大的性能负担。为了解决这个问题提出了跨代引用假说，即跨代引用相对于同代引用来说仅占极少数。基于这条假说，我们就不应再为了少量的跨代引用去扫描整个老年代，也不必浪费空间专门记录每一个对象是否存在及存在哪些跨代引用，只需在新生代上建立一个全局的数据结构（该结构被称为“记忆集”，RememberedSet），这个结构把老年代划分成若干小块，标识出老年代的哪一块内存会存在跨代引用。此后当发生Minor GC时，只有包含了跨代引用的小块内存里的对象才会被加入到GC Roots进行扫描。虽然这种方法需要在对象改变引用关系（如将自己或者某个属性赋值）时维护记录数据的正确性，会增加一些运行时的开销，但比起收集时扫描整个老年代来说仍然是划算的。 18. 什么是OopMap采用可达性分析算法来识别需要回收的对象，通常第一步是进行GC-Roots的枚举（也就是在整个内存中把Gc-roots找出来），但是在整片内存中查找符合要求的对象引用需要扫描内存，这个步骤是非常耗时间的，并且枚举根节点需要在一个不变的内存快照中进行，所以会先阻塞所有用户线程的执行，因此如果花大量的时间进行枚举根节点，程序会进行长时间的停顿，这一点是无法让用户接受的。为了解决这个问题，我们使用了OopMap这种数据结构，一旦类加载动作完成的时候，HotSpot就会把对象内什么偏移量上是什么类型的数据计算出来，在即时编译过程中，也会在特定的位置记录下栈里和寄存器里哪些位置是引用。这样收集器在扫描时就可以直接从得知这些信息了，并不需要真正一个不漏地从方法区等GC Roots开始查找。 OopMap是一种数据结构，利用它可以解决枚举根节点长时间停顿的问题。 19. 什么是安全点虽然OopMap解决了枚举根节点的时间问题，但是程序运行时，引用关系是变化的，因此每一条指令都可能生成新的OopMap，这样的话内存中需要存放许多OopMap，占用大量的额外空间，安全点就是来解决引入OopMap带来的空间消耗问题的。 安全点的思想是不在每一条指令生成OopMap，而只在特定的指令生成OopMap，这些指令的位置就是安全点，程序执行时，只有在安全点才会停顿下来开始GC。 安全点的选定会带来一些问题，如果安全点选的太少，会让两次GC的等待时间太长，太频繁会导致工作线程经常停顿，增大运行时的负荷。因此安全点的选取必须遵循一个特定的原则，真实情况下，所以，安全点的选定基本上是以程序“是否具有让程序长时间执行的特征”为标准进行选定，比如方法调用，循环跳转，异常跳转等。 但是还有一个问题，如何保证GC时所有的线程都停顿在安全点位置，有两种方式，一种是抢先式中断，一种是主动式中断。 抢先式中断式先把所有线程中断，对于没有在安全点中断的线程恢复其执行，直到它运行到安全点。 当GC需要中断线程的时候，不直接对线程操作，会在安全点位置设置标志，各个线程执行时主动去轮询这个标志，发现中断标志为真时就自己中断挂起。 20. 什么是安全区域安全区域解决了安全点没有解决的问题，比如安全点机制下，如果某个线程处于阻塞状态或者放弃了CPU，那么该线程无法响应JVM的中断请求。 安全区域是指在一段代码片段中，引用关系不会发生改变，在这个区域中的任意位置开始GC都是安全的。 当线程执行到安全区域中的代码时，首先标识自己已经进入了安全区域，那样，当在这段时间里JVM要发起GC时，就不用管标识自己为安全区域状态的线程了。在线程要离开安全区域时，它要检查系统是否已经完成了根节点枚举（或者是整个GC过程），如果完成了，那线程就继续执行，否则它就必须等待直到收到可以安全离开安全区域的信号为止。 21. 你知道哪些垃圾收集器 Serial、Serial Old Serial收集器是一个单线程的收集器，当它进行垃圾收集时，必须暂停其他所有的工作线程，然后启动一个收集线程进行垃圾收集，直到它收集结束。Serial Old是老年版本 Serial作用于新生代使用复制算法，Serial Old作用于老年代使用标记整理算法。 Serial如此简单，并且会引发STW，它的应用场景是什么 简单而高效（与其他收集器的单线程相比），对于内存资源受限的环境，它是所有收集器里额外内存消耗最小的；对于单核处理器或处理器核心数较少的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。在用户桌面的应用场景以及近年来流行的部分微服务应用中，分配给虚拟机管理的内存一般来说并不会特别大，收集几十兆甚至一两百兆的新生代，垃圾收集的停顿时间完全可以控制在十几、几十毫秒，最多一百多毫秒以内，只要不是频繁发生收集，这点停顿时间对许多用户来说是完全可以接受的。 ParNew ParNew收集器其实就是Serial收集器的多线程版本，新生代使用复制算法，老年代使用标记整理算法。除了Serial收集器外，目前只有它能与CMS收集器配合工作。 Parallel Scavenge Parallel Scavenge收集器是一个新生代收集器，使用复制算法，是并行的多线程收集器。Parallel Scavenge收集器的目标是达到一个可控制的吞吐量。所谓吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值。 Parallel Scavenge收集器提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间的-XX：MaxGCPauseMillis参数以及直接设置吞吐量大小的-XX：GCTimeRatio参数 Parallel Old Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。 CMS CMS收集器是一种以获取最短回收停顿时间为目标的收集器，它的优点是并发收集、低停顿； 它的缺点是对处理器资源很敏感，在并发阶段，它虽然不会导致用户线程停顿，但却会因为占用了一部分线程而导致应用程序变慢，降低总吞吐量。CMS收集器无法处理“浮动垃圾”，有可能出现“Con-current Mode Failure”失败进而导致另一次完全“StopThe World”的Full GC的产生。基于标记清除算法，收集结束时会有大量空间碎片产生。 什么是浮动垃圾 在CMS的并发标记和并发清理阶段，用户线程是还在继续运行的，程序在运行自然就还会伴随有新的垃圾对象不断产生，但这一部分垃圾对象是出现在标记过程结束以后，CMS无法在当次收集中处理掉它们，只好留待下一次垃圾收集时再清理掉。这一部分垃圾就称为“浮动垃圾”。 G1 Garbage First（简称G1）收集器是垃圾收集器技术发展历史上的里程碑式的成果，它开创了收集器面向局部收集的设计思路和基于Region的内存布局形式。 G1不再坚持固定大小以及固定数量的分代区域划分，而是把连续的Java堆划分为多个大小相等的独立区域，每一个Region都可以根据需要，扮演新生代的Eden空间、Survivor空间，或者老年代空间。收集器能够对扮演不同角色的Region采用不同的策略去处理，这样无论是新创建的对象还是已经存活了一段时间、熬过多次收集的旧对象都能获取很好的收集效果。 更具体的处理思路是让G1收集器去跟踪各个Region里面的垃圾堆积的“价值”大小，价值即回收所获得的空间大小以及回收所需时间的经验值，然后在后台维护一个优先级列表，每次根据用户设定允许的收集停顿时间，优先处理回收价值收益最大的那些Region，这也就是“Garbage First”名字的由来。 22. CMS收集器的工作过程 初始标记 标记GC-ROOTS可以直接引用的对象，会停顿工作线程，但是速度很快 并发标记 进行GC ROOTS track的过程，找到引用链其他对象，花费的时间相对长，但是可以与工作线程并发执行 重新标记 重新标记由于并发标记过程中引用关系发生变化的那部分对象，会停顿工作线程 并发清除 并发清除垃圾 23. G1收集器的工作过程 初始标记 并发标记 再次标记 筛选回收 24. CMS与G1的对比与CMS的“标记-清除”算法不同，G1从整体来看是基于“标记-整理”算法实现的收集器，但从局部（两个Region之间）上看又是基于“标记-复制”算法实现，无论如何，这两种算法都意味着G1运作期间不会产生内存空间碎片，垃圾收集完成之后能提供规整的可用内存。 内存占用来说，虽然G1和CMS都使用卡表来处理跨代指针，但G1的卡表实现更为复杂，而且堆中每个Region，无论扮演的是新生代还是老年代角色，都必须有一份卡表，这导致G1的记忆集（和其他内存消耗）可能会占整个堆容量的20%乃至更多的内存空间； 目前在小内存应用上CMS的表现大概率仍然要会优于G1，而在大内存应用上G1则大多能发挥其优势，这个优劣势的Java堆容量平衡点通常在6GB至8GB之间 25. 有哪些低延迟垃圾收集器Shenandoah、ZGC 26. 对象分配要遵循哪些策略 对象优先在Eden分配 大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC。 大对象直接进入老年代 长期存活的对象将进入老年代 既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这点，虚拟机给每个对象定义了一个对象年龄（Age）计数器。如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设为1。对象在Survivor区中每“熬过”一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁），就将会被晋升到老年代中。 动态对象年龄判定 27. 如何判断对象的年龄为了能更好地适应不同程序的内存状况，虚拟机并不是永远地要求对象的年龄必须达到了MaxTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到MaxTenuringThreshold中要求的年龄。 28. 什么是内存分配担保在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管这次Minor GC是有风险的；如果小于，或者HandlePromotionFailure设置不允许冒险，那这时也要改为进行一次Full GC。 二. 虚拟机执行子系统1. 什么是java的平台无关性和语言无关性平台无关性是指java代码可以不经过任何修改在不同的操作系统中运行，平台无关性是通过java虚拟机将物理硬件的区别和实际操作系统的系统调用细节屏蔽实现的，对于java来说看到的java虚拟机是一样的。语言无关性是指java虚拟机运行的是Class文件，至于这个Class文件是怎么产生的，由什么语言产生的都无所谓，只要符合虚拟机规范都可以运行。 2. 说一下Class文件的结构《Java虚拟机规范》的规定，Class文件格式采用一种类似于C语言结构体的伪结构来存储数据，这种伪结构中只有两种数据类型：“无符号数”和“表”。 无符号数属于基本的数据类型，以u1、u2、u4、u8来分别代表1个字节、2个字节、4个字节和8个字节的无符号数，无符号数可以用来描述数字、索引引用、数量值或者按照UTF-8编码构成字符串值 表是由多个无符号数或者其他表作为数据项构成的复合数据类型，为了便于区分，所有表的命名都习惯性地以“_info”结尾 魔数 魔数的作用是确定这个文件是否是一个能被虚拟机接受的Class文件，魔数为CAFEBABE Class的版本号 版本号由次版本号和主板本号组成，虚拟机必须拒绝执行超过其版本号的Class文件。 常量池 紧接着主次版本号之后的是常量池入口，常量池中主要存放两大类常量：字面量和符号引用。 访问标志 这个标志用于识别一些类或者接口层次的访问信息，包括：这个Class是类还是接口；是否定义为public类型；是否定义为abstract类型；如果是类的话，是否被声明为final等。 类索引、父类索引和接口索引集合 类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名 字段表集合 字段表用于描述接口或者类中声明的变量。字段包括类级变量以及实例级变量，但不包括在方法内部声明的局部变量。 方法表集合 属性表集合 3. 类加载时机 遇到new、getStatic、setStatic、invokeStatic指令时 反射调用时 子类加载时，如果父类没有被加载则会先加载父类 main方法所在的类会在虚拟机启动时加载 当使用JDK 1.7的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，并且这个方法句柄 所对应的类没有进行过初始化，则需要先触发其初始化。 4. 类加载的过程 加载 类加载的时机（有且仅有五个） 遇到new、getStatic、setStatic、invokeStatic指令时 反射调用时 子类加载时，如果父类没有被加载则会先加载父类 main方法所在的类会在虚拟机启动时加载 当使用JDK 1.7的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，并且这个方法句柄 所对应的类没有进行过初始化，则需要先触发其初始化。 加载需要完成的事情 通过类的全限定名获取这个类的二进制字节流 将这个二进制流代表的静态存储结构转化成运行时数据结构 在内存中生成一个java.lang.Class对象（虽然是对象，但是在hotspot中放到方法区），作为方法区这个类的各种数据的访问入口 可以从什么地方获取二进制字节流 各种zip包，比如jar包，war包，ear包等 网络中，典型应用是Applet 运行时计算生成，如动态代理 其他文件生成，如jsp 数据库中获取 数组类的加载 数组类不是由类加载器去加载的，而是虚拟机创建的 数组类的元素类型需要类加载器加载 如果数组的组件类型（Component Type，指的是数组去掉一个维度的类型）是引用类 型，那就递归采用本节中定义的加载过程去加载这个组件类型，数组C将在加载该组件类型 的类加载器的类名称空间上被标识 如果数组的组件类型不是引用类型（例如int[]数组），Java虚拟机将会把数组C标记为与 引导类加载器关联。 数组类的可见性与它的组件类型的可见性一致，如果组件类型不是引用类型，那数组类 的可见性将默认为public。 验证 验证的目的 确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 验证阶段的检验动作 文件格式验证 保证输入的字节流能正确地解析并存储于方法区之内，格式上符合描述一个Java类型信息的要求。 包括魔数检验，版本号检验等…… 元数据验证 对字节码描述的信息进行语义分析，以保证其描述的信息符合Java语言规范的要求。 字节码验证 对类的方法体进行校验分析，保证被校验类的方法在运行时不会做出危害虚拟机安全的事件。 符号引用验证 发生在虚拟机将符号引用转化为直接引用的时候，这个转化动作将在连接的第三阶段——解析阶段中发生。符号引用验证可以看做是对类自身以外（常量池中的各种符号引用）的信息进行匹配性校验。 包括： 符号引用中通过字符串描述的全限定名是否能找到对应的类。 在指定类中是否存在符合方法的字段描述符以及简单名称所描述的方法和字段。 符号引用中的类、字段、方法的访问性（private、protected、public、default）是否可被 当前类访问。 准备 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。 解析 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程 初始化 初始化阶段是执行类构造器&lt;clinit&gt;()方法的过程 5. 有哪些类加载器 6. 什么是双亲委派模型 类加载器之间的这种层次关系，称为类加载器的双亲委派模型。 双亲委派模型要求除了顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器。 双亲委派模型的工作过程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载。 9. 双亲委派模型的好处使用双亲委派模型来组织类加载器之间的关系，有一个显而易见的好处就是Java类随着它的类加载器一起具备了一种带有优先级的层次关系。例如类java.lang.Object，它存放在rt.jar之中，无论哪一个类加载器要加载这个类，最终都是委派给处于模型最顶端的启动类加载器进行加载，因此Object类在程序的各种类加载器环境中都是同一个类。相反，如果没有使用双亲委派模型，由各个类加载器自行去加载的话，如果用户自己编写了一个称为java.lang.Object的类，并放在程序的ClassPath中，那系统中将会出现多个不同的Object类，Java类型体系中最基础的行为也就无法保证，应用程序也将会变得一片混乱。 10. 如何实现自定义类加载器只需要继承ClassLoader，并覆盖findClass方法。在调用loadClass方法时，会先根据委派模型在父加载器中加载，如果加载失败，则会调用自己的findClass方法来完成加载。 11. 有哪些破坏双亲委派模型的例子 第一次被破坏是双亲委派出现之前 双亲委派模型的第二次“被破坏”是由这个模型自身的缺陷所导致的，双亲委派很好地解决了各个类加载器的基础类的统一问题（越基础的类由越上层的加载器进行加载），基础类之所以称为“基础”，是因为它们总是作为被用户代码调用的API，但世事往往没有绝对的完美，如果基础类又要调用回用户的代码，那该怎么办 双亲委派模型的第三次“被破坏”是由于用户对程序动态性的追求而导致的，这里所说的“动态性”指的是当前一些非常“热门”的名词：代码热替换 12. 栈帧存放了什么局部变量表、操作数栈、动态链接和方法返回地址 13. 局部变量表存放什么局部变量表是一组变量值存储空间，用于存放方法参数和方法内部定义的局部变量。 局部变量表的容量以变量槽（Variable Slot，下称Slot）为最小单位 14. 操作数栈的运行过程当一个方法刚刚开始执行的时候，这个方法的操作数栈是空的，在方法的执行过程中，会有各种字节码指令往操作数栈中写入和提取内容，也就是出栈/入栈操作。例如，在做算术运算的时候是通过操作数栈来进行的，又或者在调用其他方法的时候是通过操作数栈来进行参数传递的。 15. 动态链接是什么每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接","categories":[{"name":"java进阶","slug":"java进阶","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/"},{"name":"java虚拟机","slug":"java进阶/java虚拟机","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/java%E8%99%9A%E6%8B%9F%E6%9C%BA/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://www.severin.xyz/tags/jvm/"},{"name":"java","slug":"java","permalink":"https://www.severin.xyz/tags/java/"},{"name":"总结","slug":"总结","permalink":"https://www.severin.xyz/tags/%E6%80%BB%E7%BB%93/"}]},{"title":"探究java虚拟机：垃圾收集器总结","slug":"探究java虚拟机：垃圾收集器总结","date":"2019-12-18T16:00:00.000Z","updated":"2020-01-30T05:59:43.714Z","comments":true,"path":"2019/12/19/探究java虚拟机：垃圾收集器总结/","link":"","permalink":"https://www.severin.xyz/2019/12/19/%E6%8E%A2%E7%A9%B6java%E8%99%9A%E6%8B%9F%E6%9C%BA%EF%BC%9A%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E6%80%BB%E7%BB%93/","excerpt":"","text":"一. 经典的垃圾收集器特点总结 1. Serial收集器Serial收集器的工作方式是停止所有正在运行的用户线程，然后启动一个线程进行垃圾收集。它工作在新生代，使用复制算法。特点是简单高效，没有额外的内存开销，并且没有线程切换开销。适合客户端场景和微服务等管理内存小的场景。 2. ParNew收集器ParNew收集器实质上是Serial收集器的多线程并行版本，除了同时使用多条线程进行垃圾收集之外，其他和Serial收集器一样。它的特点是除了Serial收集器外，目前只有它能与CMS收集器配合工作。但是也是HotSpot虚拟机第一款退出历史舞台的垃圾收集器。 ParNew收集器在单核心处理器的环境不比Serial收集器性能好，甚至由于存在线程交互的开销，该收集器在某些时候不能超越Serial收集器。 随着可以被使用的处理器核心数量的增加，ParNew对于垃圾收集时系统资源的高效利用还是很有好处的。它默认开启的收集线程数与处理器核心数量相同。 3. Parallel Scavenge收集器Parallel Scavenge收集器作用于新生代，使用复制算法，采用多线程进行垃圾收集，与ParNew收集器类似，但是不同的地方是，Parallel Scavenge收集器的目标是达到一个可控制的吞吐量（吞吐量是指CPU处理用户代码的时间与总CPU时间的之比）。 Parallel Scavenge收集器提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间的-XX：MaxGCPauseMillis参数以及直接设置吞吐量大小的-XX：GCTimeRatio参数。 4. Serial Old收集器Serial收集器的老年版本，使用标记整理算法，作用于老年代，其他特性与Serial收集器一样。 5. Parallel Old收集器Parallel Scavenge收集器的老年版本，基于标记整理算法实现。它的意义是使得新生代和老年代都可以达到一个可控制的吞吐量的目标。 6. CMS收集器作用于老年代，使用标记清除算法，是第一个真正可以与用户工作线程并发执行的垃圾收集器，目标是获取最短回收停顿时间。它的优点是并发收集和低停顿。缺点是对CPU资源敏感，无法清除浮动垃圾，会产生大量的内存碎片。 CMS默认启动的回收线程数是（处理器核心数量+3）/4，也就是说，如果处理器核心数在四个或以上，并发回收时垃圾收集线程只占用不超过25%的处理器运算资源，并且会随着处理器核心数量的增加而下降。 工作流程如下： 初始标记 仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，会发生STW。 并发标记 从GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行。 重新标记 为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间通常会比初始标记阶段稍长一些，但也远比并发标记阶段的时间短。 并发清除 清理删除掉标记阶段判断的已经死亡的对象，由于采用标记清除算法不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的。 7. G1收集器G1收集器开创了收集器面向局部收集的设计思路和基于Region的内存布局形式，它不再按照新生代和老年代的方式划分堆，而是将堆划分成相等大小的区域（Region），G1收集器会跟踪每一个区域的回收价值，即回收所获得的空间大小以及回收所需时间的经验值，然后在后台维护一个优先级列表，默认的回收策略是优先回收价值大的区域。Region中还有一类特殊的Humongous区域，专门用来存储大对象。G1认为只要大小超过了一个Region容量一半的对象即可判定为大对象。 工作过程如下： 初始标记 仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS指针的值，让下一阶段用户线程并发运行时，能正确地在可用的Region中分配新对象。需要停顿工作线程。 并发标记 从GC Root开始对堆中对象进行可达性分析，递归扫描整个堆里的对象图，找出要回收的对象，这阶段耗时较长，但可与用户程序并发执行。当对象图扫描完成以后，还要重新处理SATB记录下的在并发时有引用变动的对象。 重新标记对用户线程做另一个短暂的暂停，用于处理并发阶段结束后仍遗留下来的最后那少量的SATB记录。 筛选回收 负责更新Region的统计数据，对各个Region的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，可以自由选择任意多个Region构成回收集，然后把决定回收的那一部分Region的存活对象复制到空的Region中，再清理掉整个旧Region的全部空间。这里的操作涉及存活对象的移动，是必须暂停用户线程，由多条收集器线程并行完成的。 二. 低延迟垃圾收集器衡量一款垃圾收集器的标准有：内存占用、吞吐量、延迟。垃圾收集器很难同时实现三个标准，但是随着计算机硬件的发展，内存占用和吞吐量逐渐变得不是问题，内存价格的下降使得不在过分关注内存占用问题，CPU性能的提高使得吞吐量提高，但是延迟问题无法通过计算机硬件的进步解决，比如说回收1TB的内存花费的时间自然要比回收1GB的内存的时间长。Shenandoah和ZGC都是低延迟的垃圾收集器，它们都可以在任意可管理的（譬如现在ZGC只能管理4TB以内的堆）堆容量下，实现垃圾收集的停顿都不超过十毫秒的目标。 1. Shenandoah收集器Shenandoah收集器不是由Oracle开发的“官方“垃圾收集器，它是由RedHat公司开发的新型收集器项目，现在已经贡献给OpenJDK 12，并且成为OpenJDK 12的新特性。它的目标是实现一种能在任何堆内存大小下都可以把垃圾收集的停顿时间限制在十毫秒以内的垃圾收集器。 Shenandoah更像是G1的下一代继承者，它们两者有着相似的堆内存布局，在初始标记、并发标记等许多阶段的处理思路上都高度一致，甚至还直接共享了一部分实现代码。 Shenandoah相比起G1的改进主要有三点： 支持并发的整理算法 不会有专门的新生代Region或者老年代Region的存在，没有实现分代 Shenandoah摒弃了在G1中耗费大量内存和计算资源去维护的记忆集，改用名为“连接矩阵”的全局数据结构来记录跨Region的引用关系，降低了处理跨代指针时的记忆集维护消耗，也降低了伪共享问题的发生概率 工作过程： 初始标记 与G1一样，首先标记与GC Roots直接关联的对象，这个阶段仍是“Stop The World”的，但停顿时间与堆大小无关，只与GC Roots的数量相关。 并发标记 与G1一样，遍历对象图，标记出全部可达的对象，这个阶段是与用户线程一起并发的，时间长短取决于堆中存活对象的数量以及对象图的结构复杂程度。 最终标记 与G1一样，处理剩余的SATB扫描，并在这个阶段统计出回收价值最高的Region，将这些Region构成一组回收集（Collection Set）。最终标记阶段也会有一小段短暂的停顿。 并发清理 这个阶段用于清理那些整个区域内连一个存活对象都没有找到的Region。 并发回收 把回收集里面的存活对象先复制一份到其他未被使用的Region之中，并发运行，此时工作线程的引用会发生变化，会通过读屏障和被称为“Brooks Pointers”的转发指针来解决。 初始引用更新 并发回收阶段复制对象结束后，还需要把堆中所有指向旧对象的引用修正到复制后的新地址，这个操作称为引用更新。引用更新的初始化阶段实际上并未做什么具体的处理，设立这个阶段只是为了建立一个线程集合点，确保所有并发回收阶段中进行的收集器线程都已完成分配给它们的对象移动任务而已。初始引用更新时间很短，会产生一个非常短暂的停顿。 并发引用更新 真正开始进行引用更新操作，这个阶段是与用户线程一起并发的，时间长短取决于内存中涉及的引用数量的多少。并发引用更新与并发标记不同，它不再需要沿着对象图来搜索，只需要按照内存物理地址的顺序，线性地搜索出引用类型，把旧值改为新值即可。 最终引用更新 修正存在于GC Roots中的引用 并发清理 经过并发回收和引用更新之后，整个回收集中所有的Region已再无存活对象，这些Region都变成ImmediateGarbage Regions了，最后再调用一次并发清理过程来回收这些Region的内存空间，供以后新对象分配使用。 转发指针 转发指针是实现对象移动与用户程序并发的一种解决方案。 此前，要做类似的并发操作，通常是在被移动对象原有的内存上设置保护陷阱，一旦用户程序访问到归属于旧对象的内存空间就会产生自陷中段，进入预设好的异常处理器中，再由其中的代码逻辑把访问转发到复制后的新对象上。虽然确实能够实现对象移动与用户线程并发，但是如果没有操作系统层面的直接支持，这种方案将导致用户态频繁切换到核心态，代价是非常大的，不能频繁使用。 新方案不需要用到内存保护陷阱，而是在原有对象布局结构的最前面统一增加一个新的引用字段，在正常不处于并发移动的情况下，该引用指向对象自己。使用时只需要修改一处指针的值，即旧对象上转发指针的引用位置，使其指向新对象，便可将所有对该对象的访问转发到新的副本上。 转发指针与某些早期Java虚拟机使用过的句柄定位有一些相似之处，两者都是一种间接性的对象访问方式，差别是句柄通常会统一存储在专门的句柄池中，而转发指针是分散存放在每一个对象头前面。 2. ZGC收集器ZGC是jdk 11的新特性，由Oracle公司研发，目标是在尽可能对吞吐量影响不太大的前提下，实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在十毫秒以内的低延迟。 ZGC收集器是一款基于Region内存布局的，（暂时）不设分代的，使用了读屏障、染色指针和内存多重映射等技术来实现可并发的标记-整理算法的，以低延迟为首要目标的一款垃圾收集器 ZGC的内存布局与Shenandoah和G1一样，也采用基于Region的堆内存布局，但不同的是，ZGC的Region具有动态性——动态创建和销毁，以及动态的区域容量大小。在x64硬件平台下，ZGC的Region可以具有大、中、小三类容量 小型Region：容量固定为2MB，用于放置小于256KB的小对象。 中型Region：容量固定为32MB，用于放置大于等于256KB但小于4MB的对象。 大型Region：容量不固定，可以动态变化，但必须为2MB的整数倍，用于放置4MB或以上的大对象。每个大型Region中只会存放一个大对象 染色指针 染色指针是一种直接将少量额外的信息存储在指针上的技术，因为操作系统寻址的时候并不需要用到指针上的所有位，那么剩下的位就可以存一些信息。染色指针直接将引用的信息记录在引用中，这样一个对象是否存活只与其引用有关，与对象本身无关，这样即使对象移动了，也能知道一个引用是否需要重置。 工作过程 并发标记 与G1、Shenandoah一样，并发标记是遍历对象图做可达性分析的阶段，但是ZGC的标记是在指针上而不是在对象上进行的，标记阶段会更新染色指针中的Marked 0、Marked 1标志位。 并发预备重分配 统计得出本次收集过程要清理哪些Region，将这些Region组成重分配集。 ZGC的重分配集只是决定了里面的存活对象会被重新复制到其他的Region中，里面的Region会被释放，而并不能说回收行为就只是针对这个集合里面的Region进行，因为标记过程是针对全堆的 并发重分配 把重分配集中的存活对象复制到新的Region上，并为重分配集中的每个Region维护一个转发表，记录从旧对象到新对象的转向关系。 因为染色指针技术，ZGC收集器能仅从引用上就明确得知一个对象是否处于重分配集之中，如果用户线程此时并发访问了位于重分配集中的对象，这次访问将会被预置的内存屏障所截获，然后立即根据Region上的转发表记录将访问转发到新复制的对象上，并同时修正更新该引用的值，使其直接指向新对象，ZGC将这种行为称为指针的“自愈”能力。 染色指针带来的好处： 只有第一次访问旧对象会陷入转发，也就是额外的开销只有一次 一旦重分配集中某个Region的存活对象都复制完毕后，这个Region就可以立即释放用于新对象的分配，旧指针一旦被使用，它们都是可以自愈的 并发重映射 重映射所做的就是修正整个堆中指向重分配集中旧对象的所有引用，ZGC很巧妙地把并发重映射阶段要做的工作，合并到了下一次垃圾收集循环中的并发标记阶段里去完成，反正它们都是要遍历所有对象的，这样合并就节省了一次遍历对象图的开销。 参考：《深入理解java虚拟机：JVM高级特性与最佳实践（第三版）》周志明","categories":[{"name":"java进阶","slug":"java进阶","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/"},{"name":"java虚拟机","slug":"java进阶/java虚拟机","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/java%E8%99%9A%E6%8B%9F%E6%9C%BA/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://www.severin.xyz/tags/jvm/"},{"name":"java","slug":"java","permalink":"https://www.severin.xyz/tags/java/"},{"name":"总结","slug":"总结","permalink":"https://www.severin.xyz/tags/%E6%80%BB%E7%BB%93/"}]},{"title":"Spring原理总结","slug":"Spring原理总结","date":"2019-12-18T16:00:00.000Z","updated":"2020-01-30T12:21:58.723Z","comments":true,"path":"2019/12/19/Spring原理总结/","link":"","permalink":"https://www.severin.xyz/2019/12/19/Spring%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/","excerpt":"","text":"本文不是源码分析，源码分析的过程很复杂，本文的目的是使用精简的语言，基于源码分析的基础，将框架原理进行总结。 1. Spring Boot自动装配的原理首先在使用Spring Boot的时候main方法所在的类会加上一个叫做@SpringBootApplication的注解，这个注解之上主要有三个注解 @SpringBootConfiguration @EnableAutoConfiguratio @ComponentScan 其中@SpringBootConfiguratio注解被@Configuration注解修饰，作用是将main方法所在的那个类声明成一个配置类，然后@ComponentScan的作用使容器扫描与main方法同一级的包及其子包，然后最重要的一个注解@EnableAutoConfiguration，这个注解与自动装配密切相关。 注解@EnableAutoConfiguration上面有一个注解@Import，这个注解的作用是将一些类注入到容器中，@Import注解有三个用法 直接将某一个指定的类注入到容器中 借助于ImportBeanDefinitionRegistrar接口将类注入到容器中 借助于ImportSelector类将类注入到容器中 Spring Boot使用的是第三种方式，借助一个叫AutoConfigurationImportSelector的类，返回类的全限定名数组，这些数组中的元素所代表的的类都会被注入到容器中，具体是哪些类呢？首先会去加载所有Spring预先定义的配置条件信息，这些信息位于org.springframework.boot.autoconfigure包下的META-INF/spring-autoconfigure-metadata.properties文件中，这些类都是被注解@Configuration修饰的配置类，这些配置类还存在一些和条件装配相关的注解，配置类中约定好了配置方式，这样用户就不需要手动去配置，如果需要修改模型信息，可以修改yml文件的内容，这也就是所谓的约定大于配置。 2. Spring IOC的原理首先解释一下什么是IOC，IOC的意思是控制反转，控制反转是一种思想，它指的是将类管理自身成员变量的权利交给第三方容器，也就是说在没有使用IOC容器的时候，一个对象所依赖的成员变量是需要自己管理、实例化的，但是有了IOC之后，程序员只要通过配置信息描述对象与对象之间的关系，然后交给ioc容器，容器会自动帮我们配置好类与类之间的关系。 然后在Spring中实现控制反转的方式叫做DI，也就是依赖注入。首先我们使用Spring创建IOC容器是通过ApplicationContex这个类创建的，而这个类有一个顶层的类，叫做BeanFactory，BeanFactory这个类不是由用户直接使用的，而是Spring内部的一个很重要的类，它是实现了ioc的基本功能。ApplicationContex也有几个子类，如ClassPathXmlApplicationContex、FileSystemXmlApplicationContex、AnnotationConfigApplicationContex，这些子类的区别在于配置信息的位置和类型不同，比如ClassPathXmlApplicationContex的配置信息是XML文件，并且会在ClassPath下找，而FileSystemXmlApplicationContex的配置文件是xml文件，但是需要提供一个全路径名的xml文件，AnnotationConfigApplicationContex它的配置信息是java类和一些注解。 以最简单的ClassPathXmlApplicationContex为例说明IoC容器的初始化过程，首先在构造对象时会调用构造方法，构造方法中有一个叫做refresh的方法，它的作用是销毁旧的容器并创建新的容器，也就是初始化的过程。 首先refresh方法会先加一个同步代码块然后执行后续的步骤，第一步是准备工作，记录下容器的启动时间、标记“已启动”状态、处理配置文件中的占位符、校验配置文件。然后调用obtainFreshBeanFactory()方法返回一个BeanFactory，这个方法内部具体的过程是，首先关闭旧的BeanFactory然后new一个类型是DefaultListableBeanFactory的BeanFactory。生成之后会调用loadBeanDefinition来加载bean到BeanFactory。 这个BeanDefinition代表了一个bean的所有信息，如bean的名称，bean的id，bean的类型，是否为单例，是否懒加载，所有的依赖等信息。因此实现ioc容器的一个最重要的步骤是如何把配置信息转化成BeanDefinition对象。 具体如何和加载BeanDefinition的呢，它会去实例化一个XmlBeanDefinitionReader对象，通过它来加载配置信息，首先会根据配置文件的信息比如说文件地址把这个文件读到内存中，因为这个文件是xml格式的，所以会把他转化为一个DOM树，方便后面的操作，后面就是对这棵DOM树进行解析，将其中的标签解析成BeanDefinition并且把它们注册到BeanFactory中，具体就是把BeanDefinition放入一个Map中。这样我们的BeanFactory就得到了所有的BeanDefinition，但是此时还没有进行初始化。 Spring 把我们在 xml 配置的 bean 都注册以后，会设置类加载器，然后还会”手动”注册一些特殊的 bean，这些bean有特殊的作用，比如： 最后一步是把那些没有声明为懒加载的bean实例化，并放在一个单例池中。 之后我们使用这个容器一般是通过getBean方法来获取一个Bean的实例，这个方法的参数是bean的Name或者是Bean的Class，首先回去单例池中找，如果找到了就返回，否则会去检查当前这个bean所对应的BeanDefinition是否存在，如果存在就回去尝试加载这个BeanDefinition的类，然后实例化，最后进行依赖注入，得到bean实例后返回。 3. Spring AOP的原理首先解释一下什么是AOP，AOP的全称是面向切面编程，在开发的过程中，有很多的代码是与业务无关的，比如说日志的打印等，但是这些代码可能散落在源代码的各个地方，如果以硬编码的形式实现则维护难度比较大，而使用AOP可以预编译或者运行时动态代理的方式对对象的方法进行增强。 Spring的AOP主要使用了两种技术，一是JDK的Proxy类，二是Cglib。Spring AOP作用于IOC容器中的bean，具体的实现是这样的，在从Spring ioc容器中获取bean的过程中，Spring容器提供了一个调用点给用户，具体来说是在创建出bean的实例后，会调用BeanPostProcessor来处理bean，AOP就是在这个过程中对bean的实例进行了动态代理，然后返回的也是代理类。 4. Spring MVC的实现原理首先，用户从客户端过来的请求会被一个叫做DispatcherServlet的前端控制器拦截，这个DispatcherServlet类继承自Servlet，拦截到请求后，会通过HandlerMapping去查找处理这个请求的uri的handler，因为handler有多种类型，所以会去找到handleAdapter去处理，处理完以后会返回一个ModelAndView，然后前端控制器会把这个ModelAndView交给视图解析器进行解析和渲染，然后把响应发送到客户端。 用户发送请求至前端控制器DispatcherServlet DispatcherServlet收到请求后，调用HandlerMapping处理器映射器，请求获取Handle 处理器映射器根据请求url找到具体的处理器，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet； DispatcherServlet 调用 HandlerAdapter处理器适配器 HandlerAdapter 经过适配调用 具体处理器(Handler，也叫后端控制器)； Handler执行完成返回ModelAndView； HandlerAdapter将Handler执行结果ModelAndView返回给DispatcherServlet； DispatcherServlet将ModelAndView传给ViewResolver视图解析器进行解析； ViewResolver解析后返回具体View； DispatcherServlet对View进行渲染视图（即将模型数据填充至视图中） 、 DispatcherServlet响应用户。","categories":[{"name":"java进阶","slug":"java进阶","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/"},{"name":"java虚拟机","slug":"java进阶/java虚拟机","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/java%E8%99%9A%E6%8B%9F%E6%9C%BA/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://www.severin.xyz/tags/jvm/"},{"name":"java","slug":"java","permalink":"https://www.severin.xyz/tags/java/"},{"name":"总结","slug":"总结","permalink":"https://www.severin.xyz/tags/%E6%80%BB%E7%BB%93/"}]},{"title":"java并发总结","slug":"java并发总结","date":"2019-01-05T16:00:00.000Z","updated":"2020-02-29T07:38:45.160Z","comments":true,"path":"2019/01/06/java并发总结/","link":"","permalink":"https://www.severin.xyz/2019/01/06/java%E5%B9%B6%E5%8F%91%E6%80%BB%E7%BB%93/","excerpt":"","text":"1. 并发编程基础 为什么要使用并发编程 硬件发展初期，CPU的性能遵循摩尔定律，但是后来摩尔定律失效了，单核CPU的性能很难继续提高，于是计算机CPU向多核方向发展，使用并发编程可以提高CPU的使用效率。另外，对于某些应用，使用并发编程可以提高运行的效率，比如一些并发计算框架。还有就是并发编程方便业务的拆分，比如java虚拟机的实现就有多条线程，每条线程负责不同的功能，如gc线程，final线程等。 并发编程的缺点 线程上下文切换导致程序的效率降低 可能会带来线程安全性问题，如死锁 一些易混淆的概念 同步与异步 同步是指方法调用后，需要等到方法返回才能执行后面的代码，异步是方法调用后不用关心结果，可以直接执行后续代码，当被调用的方法完成后会通知调用者。 并行与并发 并行是在一个时间点上多个线程可以同时运行，需要硬件的支持。并发是在一段时间内，线程通过交替运行，看上去同时运行。 临界区 如果一个资源只能同时被一个线程访问，那么这个资源就称临界资源，访问临界资源的代码称为临界区。 2. java内存模型 3. 并发关键字 synchronized关键字 synchronized如何使用，锁住的是什么 synchronized可以修饰方法 当修饰静态方法时，锁住的是这个类对应Class对象 当修饰的是实例方法时，锁住的是当前这个方法所在的实例对象 synchronized可以修饰代码块 同步代码块中写的是什么对象，锁的就是哪个对象 synchronized的可重入性 获得同步锁的线程再次获得该锁，不需要释放，可以直接获取，释放时，只有释放多次才能真正释放 synchronized的实现原理 添加同步代码块的代码字节码中会在进入同步代码块的位置添加一个叫做monitorenter的指令，在退出同步代码块或者发生异常的位置添加指令monitorexit。每一个对象都对应一个monitor对象监视器，执行monitorenter的线程会去尝试获取对象监视器，这个获取的过程是互斥的，也就是说只有一个线程可以获取得到，获取到的线程可以继续进行，没有获取到的线程视synchronized锁状态而定，如果是重量级锁，线程会加入同步队列中处于阻塞状态。当执行monitorexit指令时，会释放对象监视器，并通知同步队列中的线程出队尝试获取对象同步器。 synchronized的happen-before规则 对同一个监视器的解锁happen-before对该监视器的加锁 synchronized的内存语义 获取锁的后会去主内存中读取数据，释放锁后会将工作内存的数据刷新到主内存中。 java对象头与锁的状态 java对象头的markworld部分记录了锁的状态，一共有四种状态，分别是： 无锁状态 偏向锁状态、 轻量级锁状态 重量级锁状态 锁可以升级但是不能降级 synchronized优化 CAS 什么是CAS CAS是一种乐观策略，乐观策略假设线程访问共享资源不会存在冲突，既然不会发生冲突就不需要加锁解锁。CAS的大致过程是这样的，有三个参数，内存中的实际值，预期的值和新值，当执行CAS操作时，先会判断预期的值与实际的值是否相等，如果相等证明没有别的线程在访问，然后赋上新值，如果失败，通常会进行自旋尝试。 CAS的优点 不加锁响应速度快。 CAS的缺点 ABA问题 添加版本号，jdk提供了AtomicStampedReference 自旋时间太长，浪费CPU 只能保证一个共享变量的原子操作 解决方案是利用对象整合多个共享变量，即一个类中的成员变量就是这几个共享变量。然后将这个对象做CAS操作就可以保证其原子性。atomic中提供了AtomicReference来保证引用对象之间的原子性。 自旋锁 自旋锁的获取 当一个线程获取锁的时候，会在对象头中记录偏向的线程id，以后在该线程获取锁的时候不需要重新获取锁，只需要简单从测试一下对象头的markworld中记录的偏向的线程id是否为当前线程，如果是则获取锁成功，如果获取锁失败，会判断对象头的偏向锁是否开启，如果未开启则去竞争锁，则尝试使用CAS将对象头的偏向锁指向当前线程。 自旋锁的撤销 偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。首先会等到现在运行到一个全局的安全点，然后暂停偏向锁偏向的线程并判断该线程是否存活，如果不存活，则将对象头设置为无锁状态。否则偏向其他线程或者恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。 轻量级锁 轻量级锁的获取 在执行同步代码块之前，JVM会在当前线程的栈帧中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后尝试使用CAS将对象头修改为指向锁记录的指针，如果成功则获取到了轻量级锁，否则当前线程会进行自旋来获取锁。 轻量级锁的释放 轻量级锁释放时会通过CAS将Displaced Mark Word的值赋给对象头，如果成功，则说明没有竞争，否则会将锁膨胀成重量级锁。 重量级锁 获取重量级锁失败会进入阻塞状态。 不同锁的优缺点对比 锁 优点 缺点 适用场景 偏向锁 获取锁的开销小 如果线程间存在竞争，偏向锁的撤销会带来额外的开销 适用于只有一个线程访问同步代码块的场景 轻量级锁 线程不会阻塞，提高了线程的响应速度 自旋会消耗CPU 追求响应速度，同步代码块执行时间短，自旋时间短 重量级锁 线程竞争不会自旋，不消耗CPU 线程阻塞，响应时间变慢 追求吞吐量，同步块执行时间长 volatitle关键字 volatitle的作用 保证可见性 禁止指令重排序 不保证原子性 volatitle的原理 在生成汇编代码时会在volatile修饰的共享变量进行写操作的时候会多出Lock前缀的指令，Lock前缀指令会将当前处理器缓存行的数据写回到主存中，由于缓存一致性原理，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行读操作的时候，会重新从系统内存中把数据读到处理器缓存里。 volatitle的happen-before规则 对volatitle变量的写happen-before与对volatitle变量的读 volatitle的内存语义 当volatile变量写后，线程中本地内存中共享变量就会置为失效的状态，因此线程B再需要读取从主内存中去读取该变量的最新值。 volatitle的内存语义的实现（禁止指令重排序的实现） 通过添加内存屏障实现禁止处理器和编译器进行重排序。 内存屏障有哪些 volatitle禁止指令重排的规则 volatitle是如何添加内存屏障的 在每个volatile写操作的前面插入一个StoreStore屏障； 禁止上面的普通写和下面的volatile写重排序； 在每个volatile写操作的后面插入一个StoreLoad屏障； 防止上面的volatile写与下面可能有的volatile读/写重排序 在每个volatile读操作的后面插入一个LoadLoad屏障； 禁止下面所有的普通读操作和上面的volatile读重排序 在每个volatile读操作的后面插入一个LoadStore屏障。 禁止下面所有的普通写操作和上面的volatile读重排序 final关键字 fnal的作用 被final修饰的变量，如果是基本数据类型，值不能改变，如果是引用数据类型引用不能改变 被final修饰的方法不能被重写 被final修饰的类不能被继承 final域的初始化 实例final域：直接初始化，代码块，构造方法 静态final域：直接初始化，静态代码块 其他：直接初始化 final域重排序规则 如果final域是基本数据类型 对final域的初始化不能重排序到构造方法之外 对final域所在对象的引用的读不能重排序到对final域对象的读之后 如果final域是引用数据类型 禁止在构造函数对一个final修饰的对象的成员域的写入与随后将这个被构造的对象的引用赋值给引用变量 重排序 final重排序的实现原理 写final域会要求编译器在final域写之后，构造函数返回前插入一个StoreStore屏障。读final域的重排序规则会要求编译器在读final域的操作前插入一个LoadLoad屏障。 4. Lock体系 AQS AQS的全称是抽象队列同步器，是java并发包中的一个抽象类，是实现锁和其他同步类的基础框架，AQS的内部维护了一个int型的变量表示同步状态，并提供了一些方法用来原子的修改或获取同步状态，还使用一个双向链表实现的队列用来管理线程。AQS的设计使用了模板方法设计模式，它将一些方法开放给子类进行重写，而同步器给同步组件所提供模板方法又会重新调用被子类所重写的方法。同步器是面向锁的实现者，它简化了锁的实现方式，屏蔽了同步状态的管理，线程的排队，等待和唤醒等底层操作。AQS推荐在实现同步器的时候使用一个内部类继承AQS并通过模板方法实现同步组件语义。 同步队列 AQS使用同步队列来管理线程，AQS中有一个内部类Node，表示队列的一个节点，这个类包含所代表的的线程，节点状态，前驱节点和后继结点。AQS的同步队列是通过双向链表实现的，并使用头尾指针进行管理。当一个线程获取锁失败，就会进入同步队列，如果同步队列中的线程获取锁成功，则会进行出队操作。 独占锁 独占锁的获取 获取同步状态成功则直接返回，失败则通过CAS算法实现入队操作。位于同步队列的线程会去不断的尝试获取同步状态，具体的规则是 如果当前节点的前驱节点是头节点，并且能够获得同步状态的话，当前线程能够获得锁该方法执行结束退出； 获取锁失败的话，先将节点状态设置成SIGNAL，然后调用LookSupport.park方法使得当前线程阻塞。 独占锁的释放 每一次锁释放后就会唤醒队列中该节点的后继节点所引用的线程，从而进一步可以佐证获得锁的过程是一个FIFO（先进先出）的过程 总结 在获取同步状态时，AQS维护一个同步队列，获取同步状态失败的线程会加入到队列中进行自旋；移除队列的条件是前驱节点是头结点并且成功获得了同步状态。在释放同步状态时，同步器会调用unparkSuccessor()方法唤醒所有的后继节点。 共享锁 ​ 共享锁的获取和释放与独占锁类似，不同点在于判断同步状态时，state的值大于0则表示获取到了。 ReentrantLock 重入锁的实现原理 在获取锁状态的时候如果该锁未被任何线程占有，则该锁能够被当前线程获取，如果被占有会检查占有线程是不是当前线程，如果是的话，会使得状态加1。 在释放锁的时候，会将同步状态减1，只有同步状态为0，锁才释放成功。 公平锁与非公平锁的实现原理 公平锁的实现：当线程会判断同步队列中有前驱节点，如果有前驱节点说明有线程比当前线程更早的请求资源，根据公平性，当前线程请求资源失败。如果当前节点没有前驱节点的话，再才有做后面的逻辑判断的必要性。 非公平性锁的实现原理：不用去判断是否有前驱节点。 ReentreanReadWriteLock 写锁的获取 首先获取同步状态，同步状态是一个int型的整数。写锁的次数用低16位表示，使用二进制掩码计算获得。如果读锁以被获取或者当前线程不是已经获取写锁的线程，则当前获取失败，否则更新写锁的次数，获取锁成功。 当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。 写锁的释放 写锁次数减去1，如果写状态为0.则释放写锁，否则不更新同步状态。 读锁的获取 判断当前写锁释放被其他线程获取，如果是则获取读锁失败，否则获取成功，同步状态的高16位表示读锁获取次数，更新相关状态即可。 读锁的释放 读锁释放 将同步状态减去读状态即可 锁降级 遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁 Condition 使用Condition实现的等待/通知机制与wait和notify实现的区别 Condition能够支持不响应中断，而通过使用Object方式不支持； Condition能够支持多个等待队列（new 多个Condition对象），而Object方式只能支持一个； Condition能够支持超时时间的设置，而Object不支持 等待队列 创建一个condition对象是通过lock.newCondition(),而这个方法实际上是会new出一个ConditionObject对象，该类是AQS的一个内部类，内部维护了一个 等待队列，所有调用condition.await方法的线程会加入到等待队列中，并且线程状态转换为等待状态，等待队列是一个单向队列。 await实现原理 首先检测当前线程是否获取了同步状态，如果没有就抛出异常。 释放当前线程所占用的lock，在释放的过程中会唤醒同步队列中的下一个节点 调用LockSuport.park方法使得当前线程进入到等待状态 线程被唤醒的条件有两个，第一个是由其他线程调用signal和signalAll方法唤醒，第二个是被中断 被唤醒的线程会自旋等待获取到同步状态（即获取到lock） signal实现原理 先检测当前线程是否已经获取lock 将等待队列的头结点从等待队列中移除 将该节点移入到同步队列中去 signalAll实现原理 将等待队列中的每一个节点都移入到同步队列中，即“通知”当前调用condition.await()方法的每一个线程 LockSupport LockSupprot是线程的阻塞原语，用来阻塞线程和唤醒线程。每个使用LockSupport的线程都会与一个许可关联，如果该许可可用，并且可在线程中使用，则调用park()将会立即返回，否则可能阻塞。如果许可尚不可用，则可以调用 unpark 使其可用。但是注意许可不可重入，也就是说只能调用一次park()方法，否则会一直阻塞。内部使用Unsafe类实现的。 5. 并发容器 ConcurrentHashMap CopyOnWriteArrayList CopyOnWrite容器即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以CopyOnWrite容器也是一种读写分离的思想，读和写不同的容器。 读的时候不需要加锁，写的时候先加锁，然后复制一份容器，添加元素后把他赋给新的引用。CopyOnWrite容器适合读多写少的应用，缺点是： 内存占用问题 因为CopyOnWrite的写时复制机制，所以在进行写操作的时候，内存里会同时驻扎两个对象的内存，旧的对象和新写入的对象（注意:在复制的时候只是复制容器里的引用，只是在写的时候会创建新对象添加到新容器里，而旧容器的对象还在使用，所以有两份对象内存）。如果这些对象占用的内存比较大，比如说200M左右，那么再写入100M数据进去，内存就会占用300M，那么这个时候很有可能造成频繁的Yong GC和Full GC。之前我们系统中使用了一个服务由于每晚使用CopyOnWrite机制更新大对象，造成了每晚15秒的Full GC，应用响应时间也随之变长。 数据一致性问题 CopyOnWrite容器只能保证数据的最终一致性，不能保证数据的实时一致性。所以如果你希望写入的的数据，马上能读到，请不要使用CopyOnWrite容器。 ConcurrentLinkedQueue ThreadLocal ThreadLocal是什么 ThreadLocal表示线程的“本地变量”，即每个线程都拥有该变量副本，这样就可以避免共享资源的竞争。 ThreadLocal的实现原理 ThreadLocalMap ThreadLocalMap是ThreadLocal的内部类，是一个键值对，每个Thread都持有一个ThreadLocalMap，ThreadLoca持有的副本就存放在ThreadLocaMap中。 ThreadLocalMap使用开放定址法解决冲突问题，即如果冲突了就找下一个位置。 ThreadLocalMap的Entry的key是弱引用，value是强引用，可能造成内存泄漏问题。 set方法 首先会获取当前线程，并从当前Thread中获取ThreadLocalMap是否为null，如果不为null则可以创建一个，否则会将当前ThreadLocal类作为key，value作为值，放入map中。 get方法 获取当前线程锁对应的ThreadLocalMap，如果ThreadLocalMap不为null，则从这个map中进行查找。 为什么要使用弱引用 ThreadLocal存在两个引用，一个是在业务中使用的强引用，一个是在ThreadLocalMap中的Entry中保存的弱引用，如果Entry中不使用弱引用而是使用强引用，当业务中的ThreadLocal为null或者不访问时ThreadLocal变量会因为Map中保存了Entry中存在该引用导致对象不能使用，也不能释放，从而造成内存泄漏问题。使用弱引用虽然没有办法完全解决内存泄漏的问题，但是他带来了两个好处，一是虽然value不能释放，但是ThreadLocal可以释放，另外因为ThreadLocal是弱引用，所以下一次GC时会被释放，从而导致Entry中的key为null，这样的Entry为脏Entry，只要检查是否存在一个Entry引用不为null，但是key为null，就可以判断一个Entry是否为脏Entry，这样就方便进行清除工作。 如何解决内存泄漏问题 进行set操作时： 如果当前table[i]！=null的话说明hash冲突就需要向后环形查找，若在查找过程中遇到脏entry就通过replaceStaleEntry进行处理； 如果当前table[i]==null的话说明新的entry可以直接插入，但是插入后会调用cleanSomeSlots方法检测并清除脏entry cleanSomeSlots: cleanSomeSlots的功能是循环向后扫描脏Entry，从i位置的下一个位置开始，循环次数为log2(n)次，如果扫描到了一个脏Entry，i会重新设置，n的值会设置成数组的长度，也就是扫描的范围会变大 private boolean cleanSomeSlots(int i, int n) { boolean removed = false; Entry[] tab = table; int len = tab.length; do { i = nextIndex(i, len); Entry e = tab[i]; if (e != null &amp;&amp; e.get() == null) { n = len; removed = true; i = expungeStaleEntry(i); } } while ( (n &gt;&gt;&gt;= 1) != 0); return removed; } expungeStaleEntry 遇到了脏Entry会调用该方法进行清除，该方法首先会清理当前脏entry，即将其value引用置为null，并且将table[staleSlot]也置为null。然后还会继续向后扫描如果发现了脏Entry也会清除，直到遇到某个Entry为null。 replaceStaleEntry private void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value, int staleSlot) { Entry[] tab = table; int len = tab.length; Entry e; ////向前找到第一个脏entry int slotToExpunge = staleSlot; for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null) slotToExpunge = i; for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal&lt;?&gt; k = e.get(); if (k == key) { e.value = value; tab[i] = tab[staleSlot]; tab[staleSlot] = e; if (slotToExpunge == staleSlot) slotToExpunge = i; cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; } if (k == null &amp;&amp; slotToExpunge == staleSlot) slotToExpunge = i; } tab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value); if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); } BlockingQueue BlockingQueue 提供了可阻塞的插入和移除的方法。当队列容器已满，生产者线程会被阻塞，直到队列未满；当队列容器为空时，消费者线程会被阻塞，直至队列非空时为止。 常用的阻塞队列 ArrayBlockingQueue 由数组实现的有界阻塞队列，一旦创建，容量不能改变，默认非公平的 LinkedBlockingQueue 用链表实现的有界阻塞队列，通常在创建 LinkedBlockingQueue 对象时，会指定其大小，如果未指定，容量等于 Integer.MAX_VALUE PriorityBlockingQueue 支持优先级的无界阻塞队列 SynchronousQueue 每个插入操作必须等待另一个线程进行相应的删除操作 LinkedBlockingDeque 基于链表数据结构的有界阻塞双端队列，如果在创建对象时为指定大小时，其默认大小为 Integer.MAX_VALUE ArrayBlockingQueue、LinkedBlockingQueue ArrayBlockingQueue 内部使用一把ReentreanLock和两个Condition实现 put方法的实现 先检查队列是否已满，如果当前队列已满，将线程移入到notFull等待队列中。否则直接进行入队操作，然后调用noEmpty的signal方法通知消费者线程，当前队列中有数据可供消费。 take方法的实现 如果队列为空，没有数据，将消费者线程移入等待队列中，否则获取数据并调用noFull的signal方法通知生产者线程，当前队列没有满可以继续生产数据。 LinkedBlockingQueue LinkedBlockingQueue 在插入数据和删除数据时分别是由两个不同的 lock（takeLock和putLock）来控制线程安全的，因此，也由这两个 lock 生成了两个对应的 condition（notEmpty和notFull）来实现可阻塞的插入和删除数据。并且，采用了链表的数据结构来实现队列 put方法的实现 如果队列已满，则阻塞当前线程，将其移入等待队列，否则进行入队操作，插入数据，若队列满足插入数据的条件，则通知被阻塞的生产者线程 take方法的实现 当前队列为空，则阻塞当前线程，将其移入到等待队列中，直至满足条件，否则移除队头元素，获取数据。如果当前满足移除元素的条件，则通知被阻塞的消费者线程 对比 相同点 ArrayBlockingQueue 和 LinkedBlockingQueue 都是通过 condition 通知机制来实现可阻塞式插入和删除元素，并满足线程安全的特性； 不同点 ArrayBlockingQueue 底层是采用的数组进行实现，而 LinkedBlockingQueue 则是采用链表数据结 构； ArrayBlockingQueue 插入和删除数据，只采用了一个 lock，而 LinkedBlockingQueue 则是在插入和删除分别采用了putLock和takeLock，这样可以降低线程由于线程无法获取到 lock 而进入 WAITING 状态的可能性，从而提高了线程并发执行的效率。 6. 线程池为什么要使用线程池 降低资源消耗。通过复用已存在的线程和降低线程关闭的次数来尽可能降低系统性能损耗； 提升系统响应速度。通过复用线程，省去创建线程的过程，因此整体上提升了系统的响应速度； 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源， 工作原理 先判断线程池中核心线程池所有的线程是否都在执行任务。如果不是，则新创建一个线程执行刚提交的任务，否则，核心线程池中所有的线程都在执行任务，则进入第 2 步； 判断当前阻塞队列是否已满，如果未满，则将提交的任务放置在阻塞队列中；否则，则进入第 3 步； 判断线程池中所有的线程是否都在执行任务，如果没有，则创建一个新的线程来执行任务，否则，则交给饱和策略进行处理 饱和策略 AbortPolicy： 直接拒绝所提交的任务，并抛出RejectedExecutionException异常； CallerRunsPolicy：只用调用者所在的线程来执行任务； DiscardPolicy：不处理直接丢弃掉任务； DiscardOldestPolicy：丢弃掉阻塞队列中存放时间最久的任务，执行当前任务 线程池的关闭 关闭线程池，可以通过shutdown和shutdownNow这两个方法。它们的原理都是遍历线程池中所有的线程，然后依次中断线程。shutdown和shutdownNow还是有不一样的地方： shutdownNow首先将线程池的状态设置为STOP,然后尝试停止所有的正在执行和未执行任务的线程，并返回等待执行任务的列表； shutdown只是将线程池的状态设置为SHUTDOWN状态，然后中断所有没有正在执行任务的线程 如何配置合理的参数 要想合理的配置线程池，就必须首先分析任务特性，可以从以下几个角度来进行分析： 任务的性质：CPU 密集型任务，IO 密集型任务和混合型任务。 任务的优先级：高，中和低。 任务的执行时间：长，中和短。 任务的依赖性：是否依赖其他系统资源，如数据库连接。 7. 并发工具 CountDownLatch与CyclicBarrier对比 CountDownLatch 一般用于某个线程 A 等待若干个其他线程执行完任务之后，它才执行；而 CyclicBarrier 一般用于一组线程互相等待至某个状态，然后这一组线程再同时执行；CountDownLatch 强调一个线程等多个线程完成某件事情。CyclicBarrier 是多个线程互等，等大家都完成，再携手共进。 调用 CountDownLatch 的 countDown 方法后，当前线程并不会阻塞，会继续往下执行；而调用 CyclicBarrier 的 await 方法，会阻塞当前线程，直到 CyclicBarrier 指定的线程全部都到达了指定点的时候，才能继续往下执行； CountDownLatch 方法比较少，操作比较简单，而 CyclicBarrier 提供的方法更多，比如能够通过 getNumberWaiting()，isBroken()这些方法获取当前多个线程的状态，并且 CyclicBarrier 的构造方法可以传入 barrierAction，指定当所有线程都到达时执行的业务功能； CountDownLatch 是不能复用的，而 CyclicBarrier 是可以复用的 Semaphore Semaphore 可以理解为信号量，用于控制资源能够被并发访问的线程数量，以保证多个线程能够合理的使用特定资源。Semaphore 就相当于一个许可证，线程需要先通过 acquire 方法获取该许可证，该线程才能继续往下执行，否则只能在该方法出阻塞等待。当执行完业务功能后，需要通过release()方法将许可证归还，以便其他线程能够获得许可证继续执行。 Semaphore 可以用于做流量控制，特别是公共资源有限的应用场景，比如数据库连接。假如有多个线程读取数据后，需要将数据保存在数据库中，而可用的最大数据库连接只有 10 个，这时候就需要使用 Semaphore 来控制能够并发访问到数据库连接资源的线程个数最多只有 10 个。在限制资源使用的应用场景下，Semaphore 是特别合适的。 Exchanger Exchanger 是一个用于线程间协作的工具类，用于两个线程间能够交换。它提供了一个交换的同步点，在这个同步点两个线程能够交换数据。具体交换数据是通过 exchange 方法来实现的，如果一个线程先执行 exchange 方法，那么它会同步等待另一个线程也执行 exchange 方法，这个时候两个线程就都达到了同步点，两个线程就可以交换数据。 8. 生产者消费者模型 wait/notify实现 public class Demo01 { public static void main(String[] args) { Queue&lt;Integer&gt; queue=new LinkedList&lt;&gt;(); ExecutorService executorService = Executors.newFixedThreadPool(10); for (int i=0;i&lt;5;i++){ executorService.submit(new Producer(queue,10)); executorService.submit(new Consumer(queue)); } } private static class Producer implements Runnable{ private Queue&lt;Integer&gt; queue; private Integer maxSize; public Producer(Queue&lt;Integer&gt; queue,int maxSize){ this.queue=queue; this.maxSize=maxSize; } private boolean isFull(){ return queue.size()==maxSize; } @Override public void run() { while (true){ synchronized (queue){ while (isFull()){ try { queue.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } int prodNum=new Random().nextInt(); queue.add(prodNum); System.out.println(&quot;生产者生产:&quot;+prodNum); queue.notifyAll(); } } } } private static class Consumer implements Runnable{ private Queue&lt;Integer&gt; queue; public Consumer(Queue&lt;Integer&gt; queue){ this.queue=queue; } private boolean isEmpty(){ return queue.size()==0; } @Override public void run() { while (true){ synchronized (queue){ while (isEmpty()){ try { queue.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } Integer poll = queue.poll(); System.out.println(&quot;消费者消费:&quot;+poll); queue.notifyAll(); } } } } } condition实现 public class Demo02 { public static void main(String[] args) { Queue&lt;Integer&gt; queue=new LinkedList&lt;&gt;(); Lock lock=new ReentrantLock(); Condition condition=lock.newCondition(); ExecutorService executorService = Executors.newFixedThreadPool(10); for (int i=0;i&lt;10;i++){ executorService.submit(new Producer(queue,10,lock,condition)); executorService.submit(new Consumer(queue,lock,condition)); } } private static class Producer implements Runnable{ private Queue&lt;Integer&gt; queue; private Condition condition; private int maxSize; private Lock lock; Producer(Queue&lt;Integer&gt; queue, int maxSize,Lock lock,Condition condition){ this.queue=queue; this.lock=lock; this.condition=condition; this.maxSize=maxSize; } private boolean isFull(){ return queue.size()==maxSize; } @Override public void run() { while (true){ try { lock.lock(); while (isFull()){ try { condition.await(); } catch (InterruptedException e) { e.printStackTrace(); } } int prodNum=new Random().nextInt(); queue.add(prodNum); System.out.println(&quot;生产者生产数据:&quot;+prodNum); condition.signalAll(); }finally { lock.unlock(); } } } } private static class Consumer implements Runnable{ private Queue&lt;Integer&gt; queue; private Condition condition; private Lock lock; Consumer(Queue&lt;Integer&gt; queue,Lock lock,Condition condition){ this.queue=queue; this.lock=lock; this.condition=condition; } private boolean isEmpty(){ return queue.size()==0; } @Override public void run() { while (true){ try { lock.lock(); while (isEmpty()){ try { condition.await(); } catch (InterruptedException e) { e.printStackTrace(); } } Integer poll = queue.poll(); System.out.println(&quot;消费者消费:&quot;+poll); condition.signalAll(); }finally { lock.unlock(); } } } } } BlockingQueue实现 public class Demo03 { public static void main(String[] args) { BlockingQueue&lt;Integer&gt; blockingQueue=new LinkedBlockingQueue&lt;&gt;(10); ExecutorService executorService = Executors.newFixedThreadPool(10); for (int i=0;i&lt;5;i++){ executorService.submit(new Producer(blockingQueue)); executorService.submit(new Consumer(blockingQueue)); } } private static class Producer implements Runnable{ private BlockingQueue&lt;Integer&gt; queue; Producer(BlockingQueue&lt;Integer&gt; queue){ this.queue=queue; } @Override public void run() { while (true){ int i = new Random().nextInt(); try { queue.put(i); System.out.println(&quot;生产者生产:&quot;+i); } catch (InterruptedException e) { e.printStackTrace(); } } } } private static class Consumer implements Runnable{ private BlockingQueue&lt;Integer&gt; queue; Consumer(BlockingQueue&lt;Integer&gt; queue){ this.queue=queue; } @Override public void run() { while (true){ Integer take = null; try { take = queue.take(); System.out.println(&quot;消费者消费:&quot;+take); } catch (InterruptedException e) { e.printStackTrace(); } } } } } 9. 其他问题","categories":[{"name":"java进阶","slug":"java进阶","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/"},{"name":"java并发","slug":"java进阶/java并发","permalink":"https://www.severin.xyz/categories/java%E8%BF%9B%E9%98%B6/java%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"java","slug":"java","permalink":"https://www.severin.xyz/tags/java/"},{"name":"总结","slug":"总结","permalink":"https://www.severin.xyz/tags/%E6%80%BB%E7%BB%93/"},{"name":"java并发","slug":"java并发","permalink":"https://www.severin.xyz/tags/java%E5%B9%B6%E5%8F%91/"}]},{"title":"网络层：路由选择协议","slug":"网络层路由选择协议","date":"2019-01-03T16:00:00.000Z","updated":"2020-02-05T14:08:24.127Z","comments":true,"path":"2019/01/04/网络层路由选择协议/","link":"","permalink":"https://www.severin.xyz/2019/01/04/%E7%BD%91%E7%BB%9C%E5%B1%82%E8%B7%AF%E7%94%B1%E9%80%89%E6%8B%A9%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"1. IP数据报的转发网络通过路由器连接在一起，每个路由器维护了一个路由表，路由表的每一条路由主要是由目的网络地址和下一条地址组成。因此如果一个数据报到达一个路由器，首先会去查路由表，找到下一跳地址，然后转发，如果找不到就向默认路由转发。如果数据报的目的地址就是本网络，则可以直接交付。 2. 理想的路由算法路由选择协议的核心是路由算法，理想的路由算法具有如下特点： 算法必须是正确和完整的 算法在计算上应该简单 算法能够适应通信量和网络拓扑的变化 算法应该具有稳定性 算法是公平的 算法应该是最佳的 3. 分层次的路由选择协议路由算法分为静态路由选择策略与动态路由选择策略，静态路由选择策略需要手动配置，特点是简单开销小，但是不能及时适应网络状态的变化，适用于小规模网络。动态路由选择策略的特点是可以较好的适应网络状态的变化，但实现起来较为复杂，开销也比较大，适用于大规模网络。 互联网中采用的路由选择协议有以下特点 自适应 动态路由选择，能够较好的适应网络状态的变化 分布式 路由选择之间交换路由信息 分层次 将整个互联网分成许多较小的自治系统AS 4. 内部网关协议RIPRIP是一种分布式局域距离向量的路由选择协议，是互联网的标准协议，最大的优点是简单。 RIP协议要求网络中每一个路由器都要维护从它自己到其他每一个目的网络的距离记录，距离的定义如下： 路由器直接相连的网络距离定义为1，一个路由器到另一个非直接相连的路由器的距离为中间所经过的路由器数量加1。RIP协议中的距离也称跳数，当距离等于16时相当于不可达，RIP只适用于小型网络。 RIP协议认为跳数小的路由是好路由而不考虑其他因素比如网络带宽，所以实现简单。 RIP协议的基本工作原理 启动了RIP的接口将发生RIP请求报文，收到RIP请求报文的路由器，如果启动了RIP，则会发生RIP响应报文 RIP报文封装在UDP用户数据报中，发送和接收端口都是520 封装有RIP报文的UDP用户数据报封装在IP数据报中，目的IP地址为255.255.255 RIP会启动一个更新定时器，超时时间为30s，也就是说每隔30秒发送一次RIP更新，实际上为了避免路由器同时发生RIP更新，超时时间会加上一个随机的偏移量，该偏移量在-5到+5之间选择。 路由器的每个条目都有一个失效定时器，默认超时时间为180s，超时后，会将条目的距离变成PD，该条目不会被转发。 路由器的每个条目都有一个清除定时器，默认超时时间为240s，超时后，会将该条目的清除。 RIP路由条目更新规则 若收到的某条路由条目在路由表中没有，则直接在路由表中添加，原因是发现了新网络 若路由表中已有到达相同目的网络的路由表条目： 若来自相同的下一跳路由，则进行更新，因为这是到达目的网络且下一跳相同的最新路由信息。 若来自不同的下一跳路由，则需要比较距离： 若新路由条目中的距离小于原路由条目中的距离，则进行更新，这是因为新路由更具优势。 若新路由条目中的距离等于原路由条目中的距离，则添加新路由，以便等价负载均衡。 若新路由条目中的距离大于原路由条目中的距离，则不更新，因为新路由劣势。 RIP减少路由环路产生以及加快收敛速度的方法 水平分割 从某接口学习来的路由信息不能再从该接口发送出去，避免两个路由间的路由环路问题。 带有毒化逆转的水平分割 从某接口学习来的路由信息可以从该接口发送出去，但是需要将距离改为16。 触发更新 只要路由条目被更新，则立刻将该路由条目发给邻居，而不必等更新定时器到时 RIP不能完全避免路由环路产生 5. 内部网关协议OSPFOSPF特点 开放最短路径优先OSPF 开发表明OSPF是公开发表的，不受任何一家厂商控制 最短路径有限是因为使用了Dijkstra提出的最短路径算法 OSPF是基于链路状态的，而不像RIP基于距离向量 OSPF采用SPF算法计算路由，理论上不会出现路由环路 OSPF不限制网络规模，更新效率高，收敛速度快 OSP分组使用IP数据报进行封装，协议号89，组播地址224.0.0.5和224.0.0.6 OSPF的默认管理距离为110，可手动修改 链路状态 链路状态时指本路由器都和哪些路由相邻，以及相应链路的代价，代价用来表示费用，距离，时延，带宽等等，这些都由网络管理人员来决定。思科路由器计算代价的方法是计算100M带宽/链路带宽，小于1的用1表示大于1的舍去小数。 邻居关系的建立和维护 OSPF路由器之间通过交换问候(Hello)分组，建立和维护邻居关系。 Hello分组发往组播地址224.0.0.5 发送周期为10秒 40秒未收到来自邻居路由器的Hello分组，则认为该邻居路由器不可达 链路状态通告LSA 每个路由器都会产生链路状态通告LSA，LSA中包含以下内容： 直连网络的链路状态信息 邻居路由器的链路状态信息 链路状态更新LSU LSA被封装在链路状态更新分组LSU 采用洪泛法发送LSU 链路状态数据库LSDB 每个路由器都有一个链路状态数据库LSDB，用于存储LSA 通过各路由器洪泛发送封装有自己LSA的LSU分组，各路由器的LSDB最终将达到一致 有了LSDB，就可以基于LSDB进行SPF计算 OSPF五种分组类型及其作用 周期性发送，建立和维护邻居关系 问候分组Hello LSDB同步 数据库描述分组 向邻站给出自己的链路状态数据库中的所有链路状态项目的摘要信息 链路状态请求分组 向对方请求发送某些链路状态项目的详细信息 链路状态更新分组 用洪泛法对全网更新链路状态 链路状态确认分组 对链路更新分组的确认 多路访问网络中路由邻居关系的建立 选举指定的路由器DR与备用指定路由器BDR 所有的非DR/BDR只与DR/BDR建立连接关系 非DR/BDR之间通过DR/BDR交换信息 目的是减少邻居关系的数量 OSPF划分区域机制 使用OSPF的自治系统可以划分成多个区域，每个区域由区域描述符表示，这样做的目的是使得洪泛法只局限于当前区域，减少洪泛法发送的数据量。每个区域内部的路由器称为区域内路由器，为了使得区域与区域之间连通，每个区域会有一个区域边界路由器。 6. 外部网关协议BGP不同自治系统为什么不能使用内部网关协议 互联网的规模太大，使得自治系统之间路由选择非常困难 自治系统AS之间的路由选择必须考虑有关策略，如安全、经济、政治等。 BGP工作原理 在配置BGP时，每个自治系统选择至少一个路由器作为BGP发言人 一个BGP发言人与其他AS的BGP发言人要交换路由信息就要先建立TCP连接（端口号为179 BGP刚建立时，需要交换整个BGP路由表，但之后只需要交换更新的部分 BGP四种报文 OPEN报文 用来与相邻的BGP发言人建立关系，使通信初始化 UPDATE报文 用来通告某一路由信息，以及列出要撤销的多条路由 KEEPALIVE报文 用来周期性证实邻站的连通性 NOTIFICATION报文 用来发送检测到的差错 7. 路由器的构成路由器是一种具有多个输入端口和多个输出端口的专用计算机，其任务是转发分组。从路由器某个输入端口收到的分组，按照分组的目的地，把该分组从路由器的某个输出端口转发给下一个路由器。路由器的构成框图如下： 路由器可以分为两部分路由选择部分和分组转发部分。 控制选择部分也叫做控制部分，核心是路由选择处理机，其任务是根据所选定的路由协议构造出路由表，同时不断更新和维护路由表。 分组转发部分由三部分组成：输入端口，输出端口，交换结构 交换结构将分组从一个输入端口转移到某个输出端口 输入端口从线路接收分组，并进行物理层处理，数据链路层处理，最后经过网络层处理得到IP数据报放到输入缓存中等待查表和转发 输出端口将待转发的网络层数据报放到缓存中管理，经过网络层处理，数据链路层处理和物理层处理向线路中发送分组","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络层","slug":"计算机网络/网络层","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E5%B1%82/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络层","slug":"网络层","permalink":"https://www.severin.xyz/tags/%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"name":"路由选择协议","slug":"路由选择协议","permalink":"https://www.severin.xyz/tags/%E8%B7%AF%E7%94%B1%E9%80%89%E6%8B%A9%E5%8D%8F%E8%AE%AE/"}]},{"title":"网络层：其他重要协议","slug":"网络层ICMP,IGMP,NAT","date":"2019-01-03T16:00:00.000Z","updated":"2020-02-04T12:57:47.083Z","comments":true,"path":"2019/01/04/网络层ICMP,IGMP,NAT/","link":"","permalink":"https://www.severin.xyz/2019/01/04/%E7%BD%91%E7%BB%9C%E5%B1%82ICMP,IGMP,NAT/","excerpt":"","text":"1. 网际控制报文协议ICMP为了更有效地转发IP数据报和提高交付成功的机会，在网际层使用了网际控制报文协议ICMP。ICMP允许主机或路由器报告差错情况和提供有关异常情况的报告。 ICMP差错报告报文 终点不可达 当路由器或主机不能交付数据报时就向源点发送终点不可达报文 时间超过 当路由器收到TTL为0的数据报时，除丢弃该数据报外，还要向源点发送时间超过报文 参数问题 当路由器或目的主机收到的数据报的首部中有的此段的值不正确就丢弃该数据报，并向源点发送参数问题报文 改变路由（重定向） 路由器把改变路由报文发送给主机，让主机知道下次应将数据包发送给另外的路由器 ICMP询问报文 回送请求和回答报文 ICMP回送请求报文是由主机或路由器向一个特定的目的主机发出的询问。收到此报文的主机必须给源主机或路由器发送ICMP回送回答报文。这种询问报文用来测试目的站是否可达以及了解其有关状态。 时间戳请求和回答报文 时间戳请求与回答可用于时钟同步和时间测量。 PING ICMP的一个重要应用就是分组网间探测PING，用来测试两台主机之间的连通性。PING使用了ICMP回送请求和回送回答报文，它是应用层直接使用网络层ICMP的一个例子，没有经过运输层TCP或UDP。 traceroute 另一个应用是traceroute，用来跟踪一个分组从源点到终点的路径。源主机向目的主机发送一连串IP数据报，数据报中封装的是无法交付的UDP用户数据报，并且TTL值不断递增，每一个数据报都会对应一个时间超过差错报文，根据这些报文就可以知道从源主机到目的主机需要经过哪些路由器。 2. 网际组管理协议IGMPIP多播 与单播相比，多播可大大节约网络资源。多播允许一台主机发送单个数据报多台主机，当一个多播分组转发到达路由器后，路由器会把收到的分组复制成多个副本从不同的发送端口发送出去，当分组到达目的局域网时，由于局域网具有硬件多播功能，因此不需要复制分组，局域网上的多播组成员都能收到这个分组。 IP多播分为两种：局域网上进行硬件多播，互联网范围进行多播。 多播数据报 多播数据报的目的地址不能写主机的IP地址，因为在同一时间有许多主机加入同一个多播组，当然也不可能在其首部写入所有主机的IP地址，而是要使用给一个多播组的标识，这个标识就是IP地址中的D类地址。D类地址就是多播地址，多播地址只能用于目的地址，不能用于源地址，并且对于多播数据报补偿ICMP差错报文，因此如果使用ping命令的参数是一个多播地址，是无法收到响应的。 硬件多播 以太网硬件地址字段中的第一字节的最低位为1即为多播地址。 IGMP IGMP已有三个版本 IGMP的工作过程 第一阶段：当某台主机加入新的多播组时，该主机应向多博组的多播地址发送一个IGMP报文，声明自己要成为w该组的成员。本地的多播路由器收到IGMP报文后，还要利用多播路由选择协议把这种组成员关系转发给互联网上的其他多播路由器。 第二阶段：组成员关系是动态的。本地多播路由器要周期性地探询本地局域网上的主机，以便知道这些主机是否还继续是组的成员。只要有一台主机对某个组响应，那么路由器就认为这个组是活跃的，如果一个组经过几次探询仍然没有一台主机响应，多播路由器就认为本网络上的主机已经离开了这个组，因此也不再把这个组的成员关系转发给其他的多播路由器 IGMP的细节 在主机和多播路由器之间的所有同学都是IP多播 多播路由器在探询组成员关系时，只需要对所有的组发送一个请求信息的询问报文，而不需要对每个组发送一个询问报文。 当同一个网络上连接有几个多播路由器时，它们能够迅速和有效地选择其中的一个来探询主机的成员关系。 在IGMP的询问报文中有一个数值N，它指明一个最长响应时间。当收到询问时，主机在0到N中随机选择发送响应所需经过的时延。 同一个组内的每一台主机都要监听响应，只要有本组其他主机先发送了相应，就可以不再发送响应了。 3. 虚拟专用网VPN本地地址和全球地址 许多机构的主机并不需要暴露在互联网中，也就是说在机构内部计算机可以自行分配其IP地址，这些地址只在本机构内部有效，称为本地地址，而向互联网的管理机构中申请的全球唯一的IP地址称为全球地址。 虚拟专用网VPN 某些机构可能非常庞大，部门分布在世界各地，但是这些部门之间需要交换信息，解决的方法是利用公有的互联网作为本机构各专用网之间的通信载体，这样的专业又称为虚拟专用网VPN。专有网不同网点通信必须经过互联网，并且所有通过互联网传送的数据都必须加密。 假设一个机构有两个相隔较远的场所A和B，想要建立VPN系统，首先每个场所的网络中的路由器R1和R2应该分别有一个全球IP地址。在场所内部A和B内部的通信量不经过互联网，但是场所A的主机要和场所B的主机通信。数据就要经过路由器穿过互联网。路由器会把收到的内部数据进行加密然后重新加上首部称为在互联网上发送的外部数据报，其源地址是路由器R1的全球地址，目的地址是R2的全球地址。路由器R2收到数据后会进行解密恢复成原来的内部数据报，然后交付给目的主机。 4. 网络地址转换NAT网络地址转换要解决的问题是专业网内部一些分配到了本地IP地址的主机如何与互联网中的其他主机通信。 网络地址转换NAT首先要求专用网连接到互联网的路由器上安装NAT软件，装有NAT软件的路由器叫做NAT路由器，它至少有一个有效的外部全球IP地址，所有本地主机在和外界通信时，都要在NAT路由器上将其本地地址转换成全球IP地址。 工作过程 NAT路由器收到从专用网内部的主机发往互联网上的主机的报文时，会把报文的源IP地址转换为路由器的IP地址 NAT路由器收到互联网中主机发来的数据报时，会通过NAT地址转换表，将IP数据报的目的地址(路由器的IP地址)转化为新的IP地址（本地IP地址） 网络地址与端口号NAPT 通过端口来确定转换规则，比如主机192.168.3在端口3000上发送，就将其转化为路由器ip地址加上一个唯一的端口，这样之后路由器在这个端口收到了数据报就知道一个交给哪台主机的哪个端口。 5. IPV6IPV6的变化主要由 更大的地址空间，IPV6地址的长度为128位，理论上在可预计的将来是用不完的 扩展的地址层次结构，由于IPv6地址空间很大，因此可以划分为更多的层次 灵活的首部格式，IPv6的首部与IPv4首部不兼容，定义了许多可选的扩展首部，可提供比IPv4更多的功能 改进的选项，IPv6的首部长度是固定的，选项放在有效载荷中 允许协议继续扩充，IPv4的功能是固定不变的 支持即插即用，自动配置，不需要DHCP 支持资源的预分配，支持要求保证一定带宽和时延的应用 首部改为8字节对齐，IPv4是4字节对齐 IPv6数据报由两个部分组成，基本首部和有效载荷，有效载荷允许零个或多个扩展首部，再后面是数据部分。 IPv4如何过渡到IPv6 双协议栈 使主机运行两个协议IPv4和IPv6 隧道技术 在IPv6数据报要进入IPv4网络时，把IPv6数据报封装成为IPv4数据报。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络层","slug":"计算机网络/网络层","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E5%B1%82/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络层","slug":"网络层","permalink":"https://www.severin.xyz/tags/%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"name":"ICMP","slug":"ICMP","permalink":"https://www.severin.xyz/tags/ICMP/"},{"name":"IGMP","slug":"IGMP","permalink":"https://www.severin.xyz/tags/IGMP/"},{"name":"IPv6","slug":"IPv6","permalink":"https://www.severin.xyz/tags/IPv6/"},{"name":"NAT","slug":"NAT","permalink":"https://www.severin.xyz/tags/NAT/"},{"name":"VPN","slug":"VPN","permalink":"https://www.severin.xyz/tags/VPN/"}]},{"title":"网络层：IP地址分类","slug":"网络层IP地址分类","date":"2019-01-02T16:00:00.000Z","updated":"2020-02-04T02:24:53.795Z","comments":true,"path":"2019/01/03/网络层IP地址分类/","link":"","permalink":"https://www.severin.xyz/2019/01/03/%E7%BD%91%E7%BB%9C%E5%B1%82IP%E5%9C%B0%E5%9D%80%E5%88%86%E7%B1%BB/","excerpt":"","text":"1. IP地址IP地址是互联网中的主机在全球的唯一标识符，IPV4的IP地址为32位，由互联网名字和数字分配机构进行分配。 2. 分类的IP地址将IP地址分成固定的几类，每一类IP地址都由网络号和主机号组成 IP地址=网络号，主机号 IP地址一共分成五类 其中D类地址用于IP多播，E类地址保留。 为什么要把IP地址分成A类，B类，C类 各种网络的差距很大，有的网络中主机数量多，有的主机数量少，将IP地址分成A，B，C三类是为了满足不同用户的需求，某个单位申请IP地址时会获得一个网络号，然后IP地址自行分配。 A类、B类、C类地址最大可指派的网络数和主机数 A类地址的网络号占8位，其中第一位固定为0，可指派的网络数为27-2。减2的原因是网络号全0不能使用，表示本网络，网络号为127(01111111)保留作为本地环回地址。主机号占24位，可指派的主机数为224-2。减去2的目的是全0的主机号表示该IP地址是”本主机“所连接到的单个网络地址，全1表示网络中所有的地址。A类地址占整个IP地址的50%。 B类地址的网络号占16位，前2位已经固定了，可指派的网络数为214-1，网络号不能组成全0或全1(因为固定位为01)，但是128.0.0.0是不能指派的。主机号占16位，可以指派的主机数为216-2。减2的原因是全0和全1的地址不能使用。B类地址占整个IP地址的25%。 C类地址的网络号占24位，前3位已经固定了，可指派的网络数为221-1。减1的原因是192.0.0.0不可分配。可指派的主机数为28-2。全0个全1的地址不能使用。C类地址占整个IP地址的12.5%。 3. 划分子网分类的IP地址存在的问题 两级IP地址的问题在于A类地址和B类地址所支持连接的主机数量相差太大，对于有些单位可能为了发展申请一个支持主机数量多的A类地址，但实际上又使用不完，从而导致IP地址的大量浪费。另外两级IP地址不够灵活，想要开头一个新网络，必须重新申请一个网络号。 划分子网 将物理网络划分为若干个子网，划分子网只能在网络内部发现，在网络外部只能看到一个统一的网络。此时IP地址的组成。 IP地址=网络号，子网号，主机号 划分子网只是将主机号继续划分，而不改变IP地址原来的网络号。 子网掩码 子网掩码用于判断IP地址属于哪个子网络，子网掩码是一个32位的序列，其前半部分是连续的1，后半部分是连续的0。并且网络号的长度和子网的长度之和为多少，子网掩码中连续的1的数量就为多少，计算时，使用子网掩码与IP地址进行与运算，就得到了IP地址所属的子网络。其中A类地址的默认子网掩码为255.0.0.0,B类地址的默认子网掩码为255.255.0.0,C类地址的默认子网掩码为255.255.255.0。 4. 构造超网构造超网又称无分类编址CIDR。 CIDR解决的问题 划分子网在一定程度上缓解了互联网在发展过程中遇到的困难，但是1992年互联网面临三个问题： B类地址在1992年已分配一半 互联网主干网上的路由表中的项目数急剧增长 整个IPV4地址终将耗尽 无分类编址方法可以解决前两个问题 CIDR的特点 CIDR消除了传统的A类，B类，C类地址以及划分子网的概念，IP地址的组成为： IP地址=网络前缀，主机号 CIDR把网络前缀都相同的连续IP地址组成一个CIDR地址块，比如IP地址128.14.35.7/20，前20位就是网络前缀","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络层","slug":"计算机网络/网络层","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E5%B1%82/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络层","slug":"网络层","permalink":"https://www.severin.xyz/tags/%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"name":"IP地址","slug":"IP地址","permalink":"https://www.severin.xyz/tags/IP%E5%9C%B0%E5%9D%80/"}]},{"title":"网络层：地址解析协议","slug":"网络层地址解析协议","date":"2019-01-01T16:00:00.000Z","updated":"2020-02-05T12:33:49.252Z","comments":true,"path":"2019/01/02/网络层地址解析协议/","link":"","permalink":"https://www.severin.xyz/2019/01/02/%E7%BD%91%E7%BB%9C%E5%B1%82%E5%9C%B0%E5%9D%80%E8%A7%A3%E6%9E%90%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"1. 引言IP协议的设计目标是为跨域不同类型物理网络的分组交换提供互操作，这需要网络层软件使用的地址和底层网络硬件之间进行转换。 2. MAC地址和IP地址网络接口硬件通常有一个硬件地址，称为MAC（介质访问控制）地址，由设备制造商定义并且永久的存储在设备中，这个地址长48位，是全球唯一的，其中前24位称为组织标识符分配给厂家，后24位称为扩展标识符，由厂家自行分配。因为MAC地址是全球唯一的并且不可变，所以可以唯一标识一台硬件设备。 网络层地址（IP地址）是由用户或网络管理员分配的，为设备分配的IP地址是可以变的，也就是说IP地址不能确定某台设备的身份。 通常使用IP地址进行两台主机的通信，那么问题在于怎么找到通信对方的IP地址到底对应哪台设备呢？这就需要将IP地址转换成MAC地址。 3. ARP协议地址解析是发现两个地址之间的映射关系的过程。对于使用IPV4的TCP/IP协议族，主要由ARP协议来实现，简单来说ARP解决的问题是已知某个IP地址，怎么知道这个IP地址对应局域网的哪一台主机。 ARP协议 在主机ARP缓存中维护本局域网上的各主机的ip地址到硬件地址的映射表。当主机要向本局域网中的某台主机发送IP数据报时，首先会在缓存中查找，如果找不到，则会向该局域网中所有主机发送ARP请求，ARP请求中包含发送主机的ip地址，硬件地址，以及目的IP地址。局域网中的其他主机收到这个ARP请求后，会检查目的ip地址与自己的ip地址是不是相同，如果相同则会构造ARP响应分组返回给主机，响应分组包含了自己的物理地址。当发送方主机收到ARP响应后，就会把目的IP地址和对应的物理地址存在ARP缓存中，并设置一个过期时间。 假如目的主机不在这个网络上，就会找到路由器的地址，将这个数据报由路由器转发到其他的网络上。 ARP协议是自动进行的，主机的用户对这种解析过程是不知道的。 4. ARP相关的攻击 ARP欺骗 使用代理伪装成主机，对ARP请求进行应答。 5. 总结网络层解决的问题的一台主机与另一台主机通信，每一台主机都有一个全球唯一的IP地址，但是IP地址虽然是唯一的但是只是一个逻辑地址，也就是说一台主机的IP地址是可以改变的。除了IP地址之外，主机网络接口还有一个固定的全球唯一的硬件地址。因此为了使得网络层中的两台主机可以通信，可以先把IP地址转换为硬件地址，ARP协议就解决了这个问题。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络层","slug":"计算机网络/网络层","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E5%B1%82/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络层","slug":"网络层","permalink":"https://www.severin.xyz/tags/%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"name":"ARP协议","slug":"ARP协议","permalink":"https://www.severin.xyz/tags/ARP%E5%8D%8F%E8%AE%AE/"}]},{"title":"网络层：IP数据报格式","slug":"网络层IP数据报格式","date":"2019-01-01T16:00:00.000Z","updated":"2020-02-03T12:15:54.698Z","comments":true,"path":"2019/01/02/网络层IP数据报格式/","link":"","permalink":"https://www.severin.xyz/2019/01/02/%E7%BD%91%E7%BB%9C%E5%B1%82IP%E6%95%B0%E6%8D%AE%E6%8A%A5%E6%A0%BC%E5%BC%8F/","excerpt":"","text":"1. IP数据报格式 一个IP数据报由IP首部和数据部分组成。IP首部的前一部分是固定首部，共20字节，后一部分是可选字段，其长度是可变的。 2. 首部字段 版本 占四位，指IP协议的版本，通信双方IP协议的版本必须一致，目前广泛使用的IP协议的版本是IPV4，不就的将来会广泛使用IPV6。 首部长度 占四位，表示IP数据报首部长度，最大值为15，单位为4字节，所以最大IP数据报为60字节，固定首部长度为20字节，因此选项最大占用20字节。最常用的首部长度是20，表示不使用任何选项。 服务类型 占8位，在使用区分服务时，这个字段才起作用。 总长度 首部和数据之和的字节数，长度为16位，表示一个IP数据报最大长度为65535字节，但是实际上不会传输那么大的数据报。因为IP层之下的数据链路层协议规定了数据帧中的数据字段的最大长度，称为最大传输单元MTU，常见的以太网规定MTU为1500字节，所以如果数据报长度大于1500字节就会被分片。 如果数据报长度太长，传输效率会变高，但是如果数据报长度很小，却有利于路由器转发数据报。 标识 IP软件会维护一个计数器，每产生一个数据报，计数器就加1，并赋值给标识字段。如果IP数据报被分片，这个标识会赋值给每一个数据报片的标识字段。相同的标识字段的值使分片后的各数据片最后可以组合成原来的数据报。 标志 占3位，但是只有前两位有意义 最低位记为MF，MF为1表示后面还有分片，为0表示当前数据报是数据报分片中的最后一个 中间的位记为DF，标识不能分片 片偏移 分组在分片之后，某片在原分组中的相对位置，单位为8字节。 生存时间 TTL，占8位，表示数据报最多能经过几个路由器，每经过一个路由器，该值就减1，TTL减为0后该数据报就被丢弃。 协议 占8位，指明当前数据报携带的数据使用哪种协议，比如常见的TCP取值为6，UDP为17。 首部校验和 占16位，用来检验IP数据报首部，如何计算： 将首部划分为16位的序列，并把校验和置为0，将所有的16位序列反码算术求和后取反码作为校验和，校验时方法类似，只不过不需要将校验和置为0，求得结果为0则收下这个数据报，否则丢弃。 源IP地址、目的IP地址 发送方和接受方的IP地址。 选项 用来增加IP数据报的功能","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络层","slug":"计算机网络/网络层","permalink":"https://www.severin.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E5%B1%82/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://www.severin.xyz/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络层","slug":"网络层","permalink":"https://www.severin.xyz/tags/%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"name":"IP数据报","slug":"IP数据报","permalink":"https://www.severin.xyz/tags/IP%E6%95%B0%E6%8D%AE%E6%8A%A5/"}]}]}